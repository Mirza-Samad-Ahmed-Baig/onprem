"""Agent-friendly wrapper around and LLM instance"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/04_pipelines.agent.model.ipynb.

# %% auto 0
__all__ = ['AgentModel']

# %% ../../../nbs/04_pipelines.agent.model.ipynb 3
from typing import Any, Dict, List, Optional
import json
import re
from smolagents.models import Model, ChatMessage, MessageRole
from smolagents.models import get_tool_call_from_text, remove_stop_sequences
from smolagents import get_clean_message_list, tool_role_conversions
from enum import Enum
from ... import LLM

# %% ../../../nbs/04_pipelines.agent.model.ipynb 4
class AgentModel(Model):
    """
    A smolagents Model implementation that wraps an onprem LLM instance.
    
    This adapter allows onprem LLM instances to be used with smolagents Agents.
    
    Parameters:
        llm (LLM): An instance of onprem.llm.base.LLM
        model_id (str, optional): An identifier for the model
        **kwargs: Additional keyword arguments to pass to the parent Model class
    """
    
    def __init__(
        self,
        llm: LLM,
        model_id: Optional[str] = None,
        **kwargs
    ):
        # Initialize the parent Model class
        super().__init__(
            model_id=model_id or f"onprem-{llm.model_name}",
            **kwargs
        )
        # Store the LLM instance
        self.llm = llm
        
    def generate(
        self,
        messages: List[Dict[str, Any] | ChatMessage],
        stop_sequences: Optional[List[str]] = None,
        response_format: Optional[Dict[str, str]] = None,
        tools_to_call_from: Optional[List[Any]] = None,
        **kwargs
    ) -> ChatMessage:
        """
        Process the input messages and return the model's response.
        
        Parameters:
            messages: A list of message dictionaries to be processed.
            stop_sequences: A list of strings that will stop generation if encountered.
            response_format: The response format to use in the model's response.
            tools_to_call_from: A list of tools that the model can use.
            **kwargs: Additional keyword arguments to pass to the LLM.
            
        Returns:
            ChatMessage: A chat message object containing the model's response.
        """
        # Convert smolagents messages to a format that onprem LLM can use
        messages = self.clean(messages)
        
        # Call the LLM with the processed messages
        response = self.llm.prompt(
            messages,
            stop=stop_sequences or [],
            **kwargs
        )

        # Remove stop sequences from LLM output
        if stop_sequences is not None:
            response = remove_stop_sequences(response, stop_sequences)
        
        # Create and return a ChatMessage with the response
        message =  ChatMessage(
            role=MessageRole.ASSISTANT,
            content=response,
            raw={"response": response},
            token_usage=None  # onprem LLM doesn't track tokens in a way we can use here
        )
        if tools_to_call_from:
            message.tool_calls = [
                get_tool_call_from_text(
                    re.sub(r".*?Action:(.*?\n\}).*", r"\1", response, flags=re.DOTALL), self.tool_name_key, self.tool_arguments_key
                )
            ]
        return message


    def clean(self, messages):
        """
        Gets a clean message list.

        Args:
            messages: input messages

        Returns:
            clean messages
        """

        # Get clean message list
        messages = get_clean_message_list(messages, role_conversions=tool_role_conversions, flatten_messages_as_text=self.flatten_messages_as_text)

        # Ensure all roles are strings and not enums for compability across LLM frameworks
        for message in messages:
            if "role" in message:
                message["role"] = message["role"].value if isinstance(message["role"], Enum) else message["role"]

        return messages


