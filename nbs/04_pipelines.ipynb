{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelines\n",
    "\n",
    "> Piplines for specific tasks like summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union, Callable\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from onprem.ingest import load_single_document, load_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "DEFAULT_MAP_PROMPT = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please write a concise summary. \n",
    "CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "DEFAULT_REDUCE_PROMPT = \"\"\"The following is set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary. \n",
    "SUMMARY:\"\"\"\n",
    "\n",
    "DEFAULT_BASE_REFINE_PROMPT = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "DEFAULT_REFINE_PROMPT = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary.\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class Summarizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        prompt_template: Optional[str] = None,              \n",
    "        map_prompt: Optional[str] = None,\n",
    "        reduce_prompt: Optional[str] = None,\n",
    "        refine_prompt: Optional[str] = None, \n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `Summarizer` summarizes one or more documents\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *llm*: An `onprem.LLM` object\n",
    "        - *prompt_template*: A model specific prompt_template with a single placeholder named \"{prompt}\".\n",
    "                             All prompts (e.g., Map-Reduce prompts) are wrapped within this prompt.\n",
    "                             If supplied, overrides the `prompt_template` supplied to the `LLM` constructor.\n",
    "        - *map_prompt*: Map prompt for Map-Reduce summarization. If None, default is used.\n",
    "        - *reduce_prompt*: Reduce prompt for Map-Reduce summarization. If None, default is used.\n",
    "        - *refine_prompt*: Refine prompt for Refine-based summarization. If None, default is used.\n",
    "\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.prompt_template = prompt_template if prompt_template is not None else llm.prompt_template\n",
    "        self.map_prompt = map_prompt if map_prompt else DEFAULT_MAP_PROMPT\n",
    "        self.reduce_prompt = reduce_prompt if reduce_prompt else DEFAULT_REDUCE_PROMPT\n",
    "        self.refine_prompt = refine_prompt if refine_prompt else DEFAULT_REFINE_PROMPT\n",
    "\n",
    "\n",
    "    def summarize(self, \n",
    "                  fpath:str, \n",
    "                  strategy:str='map_reduce',\n",
    "                  chunk_size:int=1000, \n",
    "                  chunk_overlap:int=0, \n",
    "                  token_max:int=2000,\n",
    "                  max_chunks_to_use: Optional[int] = None,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Summarize one or more documents (e.g., PDFs, MS Word, MS Powerpoint, plain text)\n",
    "        using either Langchain's Map-Reduce strategy or Refine strategy.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *fpath*: A path to either a folder of documents or a single file.\n",
    "        - *strategy*: One of {'map_reduce', 'refine'}. \n",
    "        - *chunk_size*: Number of characters of each chunk to summarize\n",
    "        - *chunk_overlap*: Number of characters that overlap between chunks\n",
    "        - *token_max*: Maximum number of tokens to group documents into\n",
    "        - *max_chunks_to_use*: Maximum number of chunks (starting from beginning) to use.\n",
    "                               Useful for documents that have abstracts or informative introductions.\n",
    "                               If None, all chunks are considered for summarizer.\n",
    "\n",
    "        **Returns:**\n",
    "\n",
    "        - str: a summary of your documents\n",
    "        \"\"\"\n",
    "          \n",
    "        if os.path.isfile(fpath):\n",
    "            docs = load_single_document(fpath)\n",
    "        else:\n",
    "            docs = load_documents(fpath)\n",
    "\n",
    "        if strategy == 'map_reduce':\n",
    "            summary = self._map_reduce(docs, \n",
    "                                      chunk_size=chunk_size, \n",
    "                                      chunk_overlap=chunk_overlap, \n",
    "                                      token_max=token_max,\n",
    "                                      max_chunks_to_use=max_chunks_to_use)\n",
    "        elif strategy == 'refine':\n",
    "            summary = self._refine(docs, \n",
    "                                   chunk_size=chunk_size, \n",
    "                                   chunk_overlap=chunk_overlap, \n",
    "                                   token_max=token_max,\n",
    "                                   max_chunks_to_use=max_chunks_to_use)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f'Unknown strategy: {self.strategy}')\n",
    "        return summary\n",
    "\n",
    "    \n",
    "    def _map_reduce(self, docs, chunk_size=1000, chunk_overlap=0, token_max=1000, \n",
    "                    max_chunks_to_use = None, **kwargs):\n",
    "        \"\"\" Map-Reduce summarization\"\"\"\n",
    "        langchain_llm = self.llm.llm\n",
    "\n",
    "        # Map\n",
    "        # map_template = \"\"\"The following is a set of documents\n",
    "        # {docs}\n",
    "        # Based on this list of docs, please identify the main themes \n",
    "        # Helpful Answer:\"\"\"\n",
    "        # map_template = \"\"\"The following is a set of documents\n",
    "        # {docs}\n",
    "        # Based on this list of docs, please write a concise summary.\n",
    "        # CONCISE SUMMARY:\"\"\"\n",
    "        map_template = self.map_prompt\n",
    "        if self.prompt_template: \n",
    "            map_template = self.prompt_template.format(**{'prompt':map_template})\n",
    "        map_prompt = PromptTemplate.from_template(map_template)\n",
    "        map_chain = LLMChain(llm=langchain_llm, prompt=map_prompt)\n",
    "\n",
    "        # Reduce\n",
    "        # reduce_template = \"\"\"The following is set of summaries:\n",
    "        # {docs}\n",
    "        # Take these and distill it into a final, consolidated summary. \n",
    "        # SUMMARY:\"\"\"\n",
    "        reduce_template = self.reduce_prompt\n",
    "        if self.prompt_template:\n",
    "            reduce_template = self.prompt_template.format(**{'prompt':reduce_template})\n",
    "        reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "        # Run chain\n",
    "        reduce_chain = LLMChain(llm=langchain_llm, prompt=reduce_prompt)\n",
    "        \n",
    "        # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "        combine_documents_chain = StuffDocumentsChain(\n",
    "            llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    "        )\n",
    "        \n",
    "        # Combines and iteravely reduces the mapped documents\n",
    "        reduce_documents_chain = ReduceDocumentsChain(\n",
    "            # This is final chain that is called.\n",
    "            combine_documents_chain=combine_documents_chain,\n",
    "            # If documents exceed context for `StuffDocumentsChain`\n",
    "            collapse_documents_chain=combine_documents_chain,\n",
    "            # The maximum number of tokens to group documents into.\n",
    "            token_max=token_max)\n",
    "\n",
    "        # Combining documents by mapping a chain over them, then combining results\n",
    "        map_reduce_chain = MapReduceDocumentsChain(\n",
    "            # Map chain\n",
    "            llm_chain=map_chain,\n",
    "            # Reduce chain\n",
    "            reduce_documents_chain=reduce_documents_chain,\n",
    "            # The variable name in the llm_chain to put the documents in\n",
    "            document_variable_name=\"docs\",\n",
    "            # Return the results of the map steps in the output\n",
    "            return_intermediate_steps=False,\n",
    "        )\n",
    "        \n",
    "        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        split_docs = split_docs[:max_chunks_to_use] if max_chunks_to_use else split_docs\n",
    "\n",
    "        return map_reduce_chain.invoke(split_docs)\n",
    "\n",
    "    def _refine(self, docs, chunk_size=1000, chunk_overlap=0, \n",
    "                max_chunks_to_use = None, **kwargs):\n",
    "        \"\"\" Refine summarization\"\"\"\n",
    "\n",
    "        # initial_template = \"\"\"Write a concise summary of the following:\n",
    "        # {text}\n",
    "        # CONCISE SUMMARY:\"\"\"\n",
    "        initial_template = self.refine_base_prompt\n",
    "        if self.prompt_template:\n",
    "            initial_template = self.prompt_template.format(**{'prompt':initial_template})\n",
    "        prompt = PromptTemplate.from_template(initial_template)\n",
    "        \n",
    "        # refine_template = (\n",
    "        #     \"Your job is to produce a final summary\\n\"\n",
    "        #     \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "        #     \"We have the opportunity to refine the existing summary\"\n",
    "        #     \"(only if needed) with some more context below.\\n\"\n",
    "        #     \"------------\\n\"\n",
    "        #     \"{text}\\n\"\n",
    "        #     \"------------\\n\"\n",
    "        #     \"Given the new context, refine the original summary.\"\n",
    "        #     \"If the context isn't useful, return the original summary.\"\n",
    "        # )\n",
    "        refine_template = self.refine_prompt\n",
    "        if self.prompt_template:\n",
    "            refine_template = self.prompt_template.format(**{'prompt':refine_template})\n",
    "        refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "        chain = load_summarize_chain(\n",
    "            llm=self.llm.llm,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=prompt,\n",
    "            refine_prompt=refine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            input_key=\"input_documents\",\n",
    "            output_key=\"output_text\",\n",
    "        )\n",
    "        \n",
    "        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        split_docs = split_docs[:max_chunks_to_use] if max_chunks_to_use else split_docs\n",
    "        result = chain({\"input_documents\": split_docs}, return_only_outputs=True)\n",
    "        print(result)\n",
    "        return result['output_text']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines.py#L79){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Summarizer.summarize\n",
       "\n",
       ">      Summarizer.summarize (fpath:str, strategy:str='map_reduce',\n",
       ">                            chunk_size:int=1000, chunk_overlap:int=0,\n",
       ">                            token_max:int=2000,\n",
       ">                            max_chunks_to_use:Optional[int]=None)\n",
       "\n",
       "*Summarize one or more documents (e.g., PDFs, MS Word, MS Powerpoint, plain text)\n",
       "using either Langchain's Map-Reduce strategy or Refine strategy.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *fpath*: A path to either a folder of documents or a single file.\n",
       "- *strategy*: One of {'map_reduce', 'refine'}. \n",
       "- *chunk_size*: Number of characters of each chunk to summarize\n",
       "- *chunk_overlap*: Number of characters that overlap between chunks\n",
       "- *token_max*: Maximum number of tokens to group documents into\n",
       "- *max_chunks_to_use*: Maximum number of chunks (starting from beginning) to use.\n",
       "                       Useful for documents that have abstracts or informative introductions.\n",
       "                       If None, all chunks are considered for summarizer.\n",
       "\n",
       "**Returns:**\n",
       "\n",
       "- str: a summary of your documents*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines.py#L79){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Summarizer.summarize\n",
       "\n",
       ">      Summarizer.summarize (fpath:str, strategy:str='map_reduce',\n",
       ">                            chunk_size:int=1000, chunk_overlap:int=0,\n",
       ">                            token_max:int=2000,\n",
       ">                            max_chunks_to_use:Optional[int]=None)\n",
       "\n",
       "*Summarize one or more documents (e.g., PDFs, MS Word, MS Powerpoint, plain text)\n",
       "using either Langchain's Map-Reduce strategy or Refine strategy.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *fpath*: A path to either a folder of documents or a single file.\n",
       "- *strategy*: One of {'map_reduce', 'refine'}. \n",
       "- *chunk_size*: Number of characters of each chunk to summarize\n",
       "- *chunk_overlap*: Number of characters that overlap between chunks\n",
       "- *token_max*: Maximum number of tokens to group documents into\n",
       "- *max_chunks_to_use*: Maximum number of chunks (starting from beginning) to use.\n",
       "                       Useful for documents that have abstracts or informative introductions.\n",
       "                       If None, all chunks are considered for summarizer.\n",
       "\n",
       "**Returns:**\n",
       "\n",
       "- str: a summary of your documents*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Summarizer.summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from syntok import segmenter\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        prompt_template: Optional[str] = None,              \n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `Extractor` applies a given prompt to each sentence or paragraph in a document and returns the results.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *llm*: An `onprem.LLM` object\n",
    "        - *prompt_template*: A model specific prompt_template with a single placeholder named \"{prompt}\".\n",
    "                             All prompts (e.g., Map-Reduce prompts) are wrapped within this prompt.\n",
    "                             If supplied, overrides the `prompt_template` supplied to the `LLM` constructor.\n",
    "\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.prompt_template = prompt_template if prompt_template is not None else llm.prompt_template\n",
    "\n",
    "\n",
    "\n",
    "    def apply(self,\n",
    "              ex_prompt_template:str, \n",
    "              fpath: Optional[str] = None,\n",
    "              content: Optional[str] = None,\n",
    "              unit:str='paragraph',\n",
    "              filter_fn: Optional[Callable] = None,\n",
    "              pdf_pages:List[int]=[],\n",
    "              maxchars = 2048,\n",
    "              stop:list=[]\n",
    "             ):\n",
    "        \"\"\"\n",
    "        Apply the prompt to each `unit` (where a \"unit\" is either a paragraph or sentence) optionally filtered by `filter_fn`.\n",
    "        Results are stored in a `pandas.Dataframe`.\n",
    "\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *ex_prompt_template*: A prompt to apply to each `unit` in document. Should have a single variable, `{text}`.\n",
    "                               Example: `\"Extract universities from the following text delimited by ###:\\n\\n###{text}###\"`\n",
    "        - *fpath*: A path to to a single file of interest (e.g., a PDF or MS Word document). Mutually-exclusive with `content`.\n",
    "        - *content*: Text content of a document of interest.  Mutually-exclusive with `fpath`.\n",
    "        - *unit*: One of {'sentence', 'paragraph'}. \n",
    "        - *filter_fn*: A function that accepts a sentence or paragraph and returns `True` if prompt should be applied to it.\n",
    "                       If `filter_fn` returns False, the text is ignored and excluded from results.\n",
    "        - *pdf_pages*: If `fpath` is a PDF document, only apply prompt to text on page numbers listed in `pdf_pages`.\n",
    "                       Page numbers start with 1, not 0 (e.g., `pdf_pages=[1,2,3]` for first three pages).\n",
    "                       If list is empty, prompt is applied to every page.\n",
    "        - *maxchars*: units (i.e., paragraphs or sentences) larger than `maxhcars` split.\n",
    "        - *stop*: list of characters to trigger the LLM to stop generating.\n",
    "\n",
    "\n",
    "\n",
    "        **Returns:**\n",
    "\n",
    "        - pd.Dataframe: a Dataframe with results\n",
    "        \"\"\"\n",
    "        if not(bool(fpath) != bool(content)):\n",
    "            raise ValueError('Either fpath argument or content argument must be supplied but not both.')\n",
    "            \n",
    "        # setup extraction prompt\n",
    "        extraction_prompt = ex_prompt_template if self.prompt_template is None else self.prompt_template.format(**{'prompt': ex_prompt_template})   \n",
    "\n",
    "        # extract text\n",
    "        if not content:\n",
    "            if not os.path.isfile(fpath):\n",
    "                raise ValueError(f'{fpath} is not a file')\n",
    "            docs = load_single_document(fpath)\n",
    "            ext = \".\" + fpath.rsplit(\".\", 1)[-1].lower()\n",
    "            if ext == '.pdf' and pdf_pages:\n",
    "                docs = [doc for i,doc in enumerate(docs) if i+1 in pdf_pages]\n",
    "            content = '\\n\\n'.join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # segment\n",
    "        chunks = self.segment(content)\n",
    "        extractions = []\n",
    "        texts = []\n",
    "        for chunk in chunks:\n",
    "            if filter_fn and not filter_fn(chunk): continue\n",
    "            prompt = extraction_prompt.format(text=chunk)\n",
    "            extractions.append(self.llm.prompt(prompt, stop=stop))\n",
    "            texts.append(chunk)\n",
    "        df = pd.DataFrame({'Extractions':extractions, 'Texts':texts})\n",
    "        return df\n",
    "            \n",
    "        return results\n",
    "\n",
    "\n",
    "    def segment(self, text:str, unit:str='paragraph', maxchars:int=2048):\n",
    "        \"\"\"\n",
    "        Segments text into a list of paragraphs or sentences depending on value of `unit`.\n",
    "        \"\"\"\n",
    "        units = []\n",
    "        for paragraph in segmenter.analyze(text):\n",
    "            sentences = []\n",
    "            for sentence in paragraph:\n",
    "                text = \"\"\n",
    "                for token in sentence:\n",
    "                    text += f'{token.spacing}{token.value}'\n",
    "                sentences.append(text)\n",
    "            if unit == 'sentence':\n",
    "                units.extend(sentences)\n",
    "            else:\n",
    "                units.append(\" \".join(sentences))\n",
    "        chunks = []\n",
    "        for s in units:\n",
    "            parts = textwrap.wrap(s, maxchars, break_long_words=False)\n",
    "            chunks.extend(parts)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines.py#L272){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Extractor.apply\n",
       "\n",
       ">      Extractor.apply (ex_prompt_template:str, fpath:Optional[str]=None,\n",
       ">                       content:Optional[str]=None, unit:str='paragraph',\n",
       ">                       filter_fn:Optional[Callable]=None,\n",
       ">                       pdf_pages:List[int]=[], maxchars=2048, stop:list=[])\n",
       "\n",
       "*Apply the prompt to each `unit` (where a \"unit\" is either a paragraph or sentence) optionally filtered by `filter_fn`.\n",
       "Results are stored in a `pandas.Dataframe`.\n",
       "\n",
       "        **Args:**\n",
       "\n",
       "        - *ex_prompt_template*: A prompt to apply to each `unit` in document. Should have a single variable, `{text}`.\n",
       "                               Example: `\"Extract universities from the following text delimited by ###:\n",
       "\n",
       "###{text}###\"`\n",
       "        - *fpath*: A path to to a single file of interest (e.g., a PDF or MS Word document). Mutually-exclusive with `content`.\n",
       "        - *content*: Text content of a document of interest.  Mutually-exclusive with `fpath`.\n",
       "        - *unit*: One of {'sentence', 'paragraph'}. \n",
       "        - *filter_fn*: A function that accepts a sentence or paragraph and returns `True` if prompt should be applied to it.\n",
       "                       If `filter_fn` returns False, the text is ignored and excluded from results.\n",
       "        - *pdf_pages*: If `fpath` is a PDF document, only apply prompt to text on page numbers listed in `pdf_pages`.\n",
       "                       Page numbers start with 1, not 0 (e.g., `pdf_pages=[1,2,3]` for first three pages).\n",
       "                       If list is empty, prompt is applied to every page.\n",
       "        - *maxchars*: units (i.e., paragraphs or sentences) larger than `maxhcars` split.\n",
       "        - *stop*: list of characters to trigger the LLM to stop generating.\n",
       "\n",
       "        **Returns:**\n",
       "\n",
       "        - pd.Dataframe: a Dataframe with results*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines.py#L272){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Extractor.apply\n",
       "\n",
       ">      Extractor.apply (ex_prompt_template:str, fpath:Optional[str]=None,\n",
       ">                       content:Optional[str]=None, unit:str='paragraph',\n",
       ">                       filter_fn:Optional[Callable]=None,\n",
       ">                       pdf_pages:List[int]=[], maxchars=2048, stop:list=[])\n",
       "\n",
       "*Apply the prompt to each `unit` (where a \"unit\" is either a paragraph or sentence) optionally filtered by `filter_fn`.\n",
       "Results are stored in a `pandas.Dataframe`.\n",
       "\n",
       "        **Args:**\n",
       "\n",
       "        - *ex_prompt_template*: A prompt to apply to each `unit` in document. Should have a single variable, `{text}`.\n",
       "                               Example: `\"Extract universities from the following text delimited by ###:\n",
       "\n",
       "###{text}###\"`\n",
       "        - *fpath*: A path to to a single file of interest (e.g., a PDF or MS Word document). Mutually-exclusive with `content`.\n",
       "        - *content*: Text content of a document of interest.  Mutually-exclusive with `fpath`.\n",
       "        - *unit*: One of {'sentence', 'paragraph'}. \n",
       "        - *filter_fn*: A function that accepts a sentence or paragraph and returns `True` if prompt should be applied to it.\n",
       "                       If `filter_fn` returns False, the text is ignored and excluded from results.\n",
       "        - *pdf_pages*: If `fpath` is a PDF document, only apply prompt to text on page numbers listed in `pdf_pages`.\n",
       "                       Page numbers start with 1, not 0 (e.g., `pdf_pages=[1,2,3]` for first three pages).\n",
       "                       If list is empty, prompt is applied to every page.\n",
       "        - *maxchars*: units (i.e., paragraphs or sentences) larger than `maxhcars` split.\n",
       "        - *stop*: list of characters to trigger the LLM to stop generating.\n",
       "\n",
       "        **Returns:**\n",
       "\n",
       "        - pd.Dataframe: a Dataframe with results*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Extractor.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines.py#L339){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Extractor.segment\n",
       "\n",
       ">      Extractor.segment (text:str, unit:str='paragraph', maxchars:int=2048)\n",
       "\n",
       "*Segments text into a list of paragraphs or sentences depending on value of `unit`.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines.py#L339){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Extractor.segment\n",
       "\n",
       ">      Extractor.segment (text:str, unit:str='paragraph', maxchars:int=2048)\n",
       "\n",
       "*Segments text into a list of paragraphs or sentences depending on value of `unit`.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Extractor.segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem import LLM\n",
    "from onprem.pipelines import Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "prompt_template = \"<s>[INST] {prompt} [/INST]\" # prompt template for Mistral\n",
    "llm = LLM(model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf', \n",
    "          n_gpu_layers=33,  # change based on your system\n",
    "          verbose=False, mute_stream=True, \n",
    "          prompt_template=prompt_template)\n",
    "extractor = Extractor(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Extract citations from the following sentences. Return #NA# if there are no citations in the text. Here are some examples:\n",
    "\n",
    "[SENTENCE]:pretrained BERT text classifier (Devlin et al., 2018), models for sequence tagging (Lample et al., 2016)\n",
    "[CITATIONS]:(Devlin et al., 2018), (Lample et al., 2016)\n",
    "[SENTENCE]:Machine learning (ML) is a powerful tool.\n",
    "[CITATIONS]:#NA#\n",
    "[SENTENCE]:Following inspiration from a blog post by Rachel Thomas of fast.ai (Howard and Gugger, 2020), we refer to this as Augmented Machine Learning or AugML\n",
    "[CITATIONS]:(Howard and Gugger, 2020)\n",
    "[SENTENCE]:{text}\n",
    "[CITATIONS]:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "content = \"\"\"\n",
    "For instance, the fit_onecycle method employs a 1cycle policy (Smith, 2018). \n",
    "\"\"\"\n",
    "df = extractor.apply(prompt, content=content, stop=['\\n'])\n",
    "assert df['Extractions'][0].strip().startswith('(Smith, 2018)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "content =\"\"\"In the case of text, this may involve language-specific preprocessing (e.g., tokenization).\"\"\"\n",
    "df = extractor.apply(prompt, content=content, stop=['\\n'])\n",
    "assert df['Extractions'][0].strip().startswith('#NA#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
