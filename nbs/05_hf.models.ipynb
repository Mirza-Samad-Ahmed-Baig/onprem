{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hf.models\n",
    "\n",
    "> Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp hf.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Models module\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from onprem.hf.onnx import OnnxModel\n",
    "\n",
    "\n",
    "class Models:\n",
    "    \"\"\"\n",
    "    Utility methods for working with machine learning models\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def checklength(config, tokenizer):\n",
    "        \"\"\"\n",
    "        Checks the length for a Hugging Face Transformers tokenizer using a Hugging Face Transformers config. Copies the\n",
    "        max_position_embeddings parameter if the tokenizer has no max_length set. This helps with backwards compatibility\n",
    "        with older tokenizers.\n",
    "\n",
    "        Args:\n",
    "            config: transformers config\n",
    "            tokenizer: transformers tokenizer\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack nested config, handles passing model directly\n",
    "        if hasattr(config, \"config\"):\n",
    "            config = config.config\n",
    "\n",
    "        if (\n",
    "            hasattr(config, \"max_position_embeddings\")\n",
    "            and tokenizer\n",
    "            and hasattr(tokenizer, \"model_max_length\")\n",
    "            and tokenizer.model_max_length == int(1e30)\n",
    "        ):\n",
    "            tokenizer.model_max_length = config.max_position_embeddings\n",
    "\n",
    "    @staticmethod\n",
    "    def maxlength(config, tokenizer):\n",
    "        \"\"\"\n",
    "        Gets the best max length to use for generate calls. This method will return config.max_length if it's set. Otherwise, it will return\n",
    "        tokenizer.model_max_length.\n",
    "\n",
    "        Args:\n",
    "            config: transformers config\n",
    "            tokenizer: transformers tokenizer\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack nested config, handles passing model directly\n",
    "        if hasattr(config, \"config\"):\n",
    "            config = config.config\n",
    "\n",
    "        # Get non-defaulted fields\n",
    "        keys = config.to_diff_dict()\n",
    "\n",
    "        # Use config.max_length if not set to default value, else use tokenizer.model_max_length if available\n",
    "        return config.max_length if \"max_length\" in keys or not hasattr(tokenizer, \"model_max_length\") else tokenizer.model_max_length\n",
    "\n",
    "    @staticmethod\n",
    "    def deviceid(gpu):\n",
    "        \"\"\"\n",
    "        Translates input gpu argument into a device id.\n",
    "\n",
    "        Args:\n",
    "            gpu: True/False if GPU should be enabled, also supports a device id/string/instance\n",
    "\n",
    "        Returns:\n",
    "            device id\n",
    "        \"\"\"\n",
    "\n",
    "        # Return if this is already a torch device\n",
    "        # pylint: disable=E1101\n",
    "        if isinstance(gpu, torch.device):\n",
    "            return gpu\n",
    "\n",
    "        # Always return -1 if gpu is None or an accelerator device is unavailable\n",
    "        if gpu is None or not Models.hasaccelerator():\n",
    "            return -1\n",
    "\n",
    "        # Default to device 0 if gpu is True and not otherwise specified\n",
    "        if isinstance(gpu, bool):\n",
    "            return 0 if gpu else -1\n",
    "\n",
    "        # Return gpu as device id if gpu flag is an int\n",
    "        return int(gpu)\n",
    "\n",
    "    @staticmethod\n",
    "    def device(deviceid):\n",
    "        \"\"\"\n",
    "        Gets a tensor device.\n",
    "\n",
    "        Args:\n",
    "            deviceid: device id\n",
    "\n",
    "        Returns:\n",
    "            tensor device\n",
    "        \"\"\"\n",
    "\n",
    "        # Torch device\n",
    "        # pylint: disable=E1101\n",
    "        return deviceid if isinstance(deviceid, torch.device) else torch.device(Models.reference(deviceid))\n",
    "\n",
    "    @staticmethod\n",
    "    def reference(deviceid):\n",
    "        \"\"\"\n",
    "        Gets a tensor device reference.\n",
    "\n",
    "        Args:\n",
    "            deviceid: device id\n",
    "\n",
    "        Returns:\n",
    "            device reference\n",
    "        \"\"\"\n",
    "\n",
    "        return (\n",
    "            deviceid\n",
    "            if isinstance(deviceid, str)\n",
    "            else (\n",
    "                \"cpu\"\n",
    "                if deviceid < 0\n",
    "                else f\"cuda:{deviceid}\" if torch.cuda.is_available() else \"mps\" if Models.hasmpsdevice() else Models.finddevice()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def hasaccelerator():\n",
    "        \"\"\"\n",
    "        Checks if there is an accelerator device available.\n",
    "\n",
    "        Returns:\n",
    "            True if an accelerator device is available, False otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.cuda.is_available() or Models.hasmpsdevice() or bool(Models.finddevice())\n",
    "\n",
    "    @staticmethod\n",
    "    def hasmpsdevice():\n",
    "        \"\"\"\n",
    "        Checks if there is a MPS device available.\n",
    "\n",
    "        Returns:\n",
    "            True if a MPS device is available, False otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        return os.environ.get(\"PYTORCH_MPS_DISABLE\") != \"1\" and torch.backends.mps.is_available()\n",
    "\n",
    "    @staticmethod\n",
    "    def finddevice():\n",
    "        \"\"\"\n",
    "        Attempts to find an alternative accelerator device.\n",
    "\n",
    "        Returns:\n",
    "            name of first alternative accelerator available or None if not found\n",
    "        \"\"\"\n",
    "\n",
    "        return next((device for device in [\"xpu\"] if hasattr(torch, device) and getattr(torch, device).is_available()), None)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path, config=None, task=\"default\", modelargs=None):\n",
    "        \"\"\"\n",
    "        Loads a machine learning model. Handles multiple model frameworks (ONNX, Transformers).\n",
    "\n",
    "        Args:\n",
    "            path: path to model\n",
    "            config: path to model configuration\n",
    "            task: task name used to lookup model type\n",
    "\n",
    "        Returns:\n",
    "            machine learning model\n",
    "        \"\"\"\n",
    "\n",
    "        # Detect ONNX models\n",
    "        if isinstance(path, bytes) or (isinstance(path, str) and os.path.isfile(path)):\n",
    "            return OnnxModel(path, config)\n",
    "\n",
    "        # Return path, if path isn't a string\n",
    "        if not isinstance(path, str):\n",
    "            return path\n",
    "\n",
    "        # Transformer models\n",
    "        models = {\n",
    "            \"default\": AutoModel.from_pretrained,\n",
    "            \"question-answering\": AutoModelForQuestionAnswering.from_pretrained,\n",
    "            \"summarization\": AutoModelForSeq2SeqLM.from_pretrained,\n",
    "            \"text-classification\": AutoModelForSequenceClassification.from_pretrained,\n",
    "            \"zero-shot-classification\": AutoModelForSequenceClassification.from_pretrained,\n",
    "        }\n",
    "\n",
    "        # Pass modelargs as keyword arguments\n",
    "        modelargs = modelargs if modelargs else {}\n",
    "\n",
    "        # Load model for supported tasks. Return path for unsupported tasks.\n",
    "        return models[task](path, **modelargs) if task in models else path\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer(path, **kwargs):\n",
    "        \"\"\"\n",
    "        Loads a tokenizer from path.\n",
    "\n",
    "        Args:\n",
    "            path: path to tokenizer\n",
    "            kwargs: optional additional keyword arguments\n",
    "\n",
    "        Returns:\n",
    "            tokenizer\n",
    "        \"\"\"\n",
    "\n",
    "        return AutoTokenizer.from_pretrained(path, **kwargs) if isinstance(path, str) else path\n",
    "\n",
    "    @staticmethod\n",
    "    def task(path, **kwargs):\n",
    "        \"\"\"\n",
    "        Attempts to detect the model task from path.\n",
    "\n",
    "        Args:\n",
    "            path: path to model\n",
    "            kwargs: optional additional keyword arguments\n",
    "\n",
    "        Returns:\n",
    "            inferred model task\n",
    "        \"\"\"\n",
    "\n",
    "        # Get model configuration\n",
    "        config = None\n",
    "        if isinstance(path, (list, tuple)) and hasattr(path[0], \"config\"):\n",
    "            config = path[0].config\n",
    "        elif isinstance(path, str):\n",
    "            config = AutoConfig.from_pretrained(path, **kwargs)\n",
    "\n",
    "        # Attempt to resolve task using configuration\n",
    "        task = None\n",
    "        if config:\n",
    "            architecture = config.architectures[0] if config.architectures else None\n",
    "            if architecture:\n",
    "                if any(x for x in [\"LMHead\", \"CausalLM\"] if x in architecture):\n",
    "                    task = \"language-generation\"\n",
    "                elif \"QuestionAnswering\" in architecture:\n",
    "                    task = \"question-answering\"\n",
    "                elif \"ConditionalGeneration\" in architecture:\n",
    "                    task = \"sequence-sequence\"\n",
    "\n",
    "        return task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
