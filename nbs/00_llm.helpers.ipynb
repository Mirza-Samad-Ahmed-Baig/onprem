{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm helpers\n",
    "\n",
    "> Helper utilities for using LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp llm.helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from onprem.utils import SafeFormatter\n",
    "import json\n",
    "import yaml\n",
    "from typing import List, Any, Union, Callable\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.documents import Document\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def truncate_prompt(model_or_pipeline, prompt, max_gen_tokens=512, truncate_from=\"start\", prompt_template=None):\n",
    "    \"\"\"\n",
    "    Truncate only the user prompt (not the full formatted prompt) to ensure\n",
    "    the total prompt (template + user prompt) fits in model's context.\n",
    "\n",
    "    Args:\n",
    "        model_or_pipeline: \n",
    "            - llama_cpp.Llama\n",
    "            - HuggingFace pipeline (with .tokenizer)\n",
    "            - HuggingFace tokenizer\n",
    "        prompt (str): The user-supplied prompt (to be truncated if needed).\n",
    "        max_gen_tokens (int): Tokens to reserve for generation.\n",
    "        truncate_from (str): 'start' or 'end'.\n",
    "        prompt_template (str, optional): Template with {prompt}. Used to reserve space for non-user text.\n",
    "\n",
    "    Returns:\n",
    "        str: Truncated user prompt only (template not applied).\n",
    "    \"\"\"\n",
    "    # Resolve tokenizer\n",
    "    if hasattr(model_or_pipeline, \"tokenize\") and hasattr(model_or_pipeline, \"detokenize\"):\n",
    "        # llama_cpp\n",
    "        tokenizer = model_or_pipeline\n",
    "        tokenize = lambda text: tokenizer.tokenize(text.encode(\"utf-8\"))\n",
    "        detokenize = lambda tokens: tokenizer.detokenize(tokens).decode(\"utf-8\")\n",
    "        n_ctx = tokenizer.n_ctx()\n",
    "    elif hasattr(model_or_pipeline, \"tokenizer\"):\n",
    "        # HuggingFace pipeline\n",
    "        tokenizer = model_or_pipeline.tokenizer\n",
    "        tokenize = lambda text: tokenizer.encode(text, add_special_tokens=False)\n",
    "        detokenize = lambda tokens: tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        n_ctx = getattr(tokenizer, \"model_max_length\", 4096)\n",
    "    elif hasattr(model_or_pipeline, \"encode\") and hasattr(model_or_pipeline, \"decode\"):\n",
    "        # HuggingFace tokenizer directly\n",
    "        tokenizer = model_or_pipeline\n",
    "        tokenize = lambda text: tokenizer.encode(text, add_special_tokens=False)\n",
    "        detokenize = lambda tokens: tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        n_ctx = getattr(tokenizer, \"model_max_length\", 4096)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model_or_pipeline type.\")\n",
    "\n",
    "    # If prompt_template is provided, reserve space for it\n",
    "    if prompt_template:\n",
    "        empty_filled_template = prompt_template.format(prompt=\"\")\n",
    "        template_tokens = tokenize(empty_filled_template)\n",
    "    else:\n",
    "        template_tokens = []\n",
    "\n",
    "    prompt_tokens = tokenize(prompt)\n",
    "    max_prompt_tokens = n_ctx - max_gen_tokens - len(template_tokens)\n",
    "\n",
    "    if max_prompt_tokens < 0:\n",
    "        raise ValueError(\"Template and generation tokens exceed model context window.\")\n",
    "\n",
    "    if len(prompt_tokens) > max_prompt_tokens:\n",
    "        if truncate_from == \"start\":\n",
    "            prompt_tokens = prompt_tokens[-max_prompt_tokens:]\n",
    "        elif truncate_from == \"end\":\n",
    "            prompt_tokens = prompt_tokens[:max_prompt_tokens]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid truncate_from='{truncate_from}'. Use 'start' or 'end'.\")\n",
    "\n",
    "    return detokenize(prompt_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def _marshal_llm_to_json(output: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract a substring containing valid JSON or array from a string.\n",
    "\n",
    "    **Args:**\n",
    "    \n",
    "        - output: A string that may contain a valid JSON object or array surrounded by extraneous characters or information.\n",
    "\n",
    "    **Returns:**\n",
    "        \n",
    "        - A string containing a valid JSON object or array.\n",
    "    \"\"\"\n",
    "    output = output.strip()\n",
    "\n",
    "    left_square = output.find(\"[\")\n",
    "    left_brace = output.find(\"{\")\n",
    "\n",
    "    if left_square < left_brace and left_square != -1:\n",
    "        left = left_square\n",
    "        right = output.rfind(\"]\")\n",
    "    else:\n",
    "        left = left_brace\n",
    "        right = output.rfind(\"}\")\n",
    "\n",
    "    return output[left : right + 1]\n",
    "\n",
    "\n",
    "def extract_json(text:str) -> str:\n",
    "    \"\"\"\n",
    "    Atttempts to extract json from markdown string.\n",
    "    If no json exists, then return empty string.\n",
    "    \"\"\"\n",
    "    if \"```json\" in text:\n",
    "        text = text.split(\"```json\")[1].strip().strip(\"```\").strip()\n",
    "    \n",
    "    return _marshal_llm_to_json(text)\n",
    "\n",
    "\n",
    "def parse_json_markdown(text: str) -> Any:\n",
    "    \"\"\"\n",
    "    Parse json embedded in markdown into dictionary\n",
    "    \"\"\"\n",
    "    json_string = extract_json(text)\n",
    "\n",
    "    try:\n",
    "        json_obj = json.loads(json_string)\n",
    "    except json.JSONDecodeError as e_json:\n",
    "        try:\n",
    "            # NOTE: parsing again with pyyaml\n",
    "            #       pyyaml is less strict, and allows for trailing commas\n",
    "            #       right now we rely on this since guidance program generates\n",
    "            #       trailing commas\n",
    "            json_obj = yaml.safe_load(json_string)\n",
    "        except yaml.YAMLError as e_yaml:\n",
    "            raise OutputParserException(\n",
    "                f\"Got invalid JSON object. Error: {e_json} {e_yaml}. \"\n",
    "                f\"Got JSON string: {json_string}\"\n",
    "            )\n",
    "        except NameError as exc:\n",
    "            raise ImportError(\"Please pip install PyYAML.\") from exc\n",
    "\n",
    "    return json_obj\n",
    "\n",
    "def parse_code_markdown(text: str, only_last: bool) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parsing embedded code out of markdown string\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to match code within triple-backticks\n",
    "    pattern = r\"```(.*?)```\"\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    # Return the last matched group if requested\n",
    "    code = matches[-1] if matches and only_last else matches\n",
    "\n",
    "    # If empty we optimistically assume the output is the code\n",
    "    if not code:\n",
    "        # we want to handle cases where the code may start or end with triple\n",
    "        # backticks\n",
    "        # we also want to handle cases where the code is surrounded by regular\n",
    "        # quotes\n",
    "        # we can't just remove all backticks due to JS template strings\n",
    "\n",
    "        candidate = text.strip()\n",
    "\n",
    "        if candidate.startswith('\"') and candidate.endswith('\"'):\n",
    "            candidate = candidate[1:-1]\n",
    "\n",
    "        if candidate.startswith(\"'\") and candidate.endswith(\"'\"):\n",
    "            candidate = candidate[1:-1]\n",
    "\n",
    "        if candidate.startswith(\"`\") and candidate.endswith(\"`\"):\n",
    "            candidate = candidate[1:-1]\n",
    "\n",
    "        # For triple backticks we split the handling of the start and end\n",
    "        # partly because there can be cases where only one and not the other\n",
    "        # is present, and partly because we don't need to be so worried\n",
    "        # about it being a string in a programming language\n",
    "        if candidate.startswith(\"```\"):\n",
    "            candidate = re.sub(r\"^```[a-zA-Z]*\", \"\", candidate)\n",
    "\n",
    "        if candidate.endswith(\"```\"):\n",
    "            candidate = candidate[:-3]\n",
    "        code = [candidate.strip()]\n",
    "\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "SUBQUESTION_PROMPT = \"\"\"\\\n",
    "Given a user question, output a list of relevant sub-questions \\\n",
    "in json markdown that when composed can help answer the full user question.\n",
    "Only return the JSON response, with no additional text or explanations.\n",
    "\n",
    "# Example 1\n",
    "<User Question>\n",
    "Compare and contrast the revenue growth and EBITDA of Uber and Lyft for year 2021\n",
    "\n",
    "<Output>\n",
    "```json\n",
    "{\n",
    "    \"items\": [\n",
    "        {\n",
    "            \"sub_question\": \"What is the revenue growth of Uber\",\n",
    "        },\n",
    "        {\n",
    "            \"sub_question\": \"What is the EBITDA of Uber\",\n",
    "        },\n",
    "        {\n",
    "            \"sub_question\": \"What is the revenue growth of Lyft\",\n",
    "        },\n",
    "        {\n",
    "            \"sub_question\": \"What is the EBITDA of Lyft\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 2\n",
    "<User Question>\n",
    "{query_str}\n",
    "\n",
    "<Output>\n",
    "\"\"\"\n",
    "\n",
    "def decompose_question(question:str, llm, parse=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Decompose a question into subquestions\n",
    "    \"\"\"\n",
    "    prompt = SafeFormatter({'query_str': question}).format(SUBQUESTION_PROMPT)\n",
    "    json_string = llm.prompt(prompt)\n",
    "    json_dict = parse_json_markdown(json_string)\n",
    "    subquestions = [d['sub_question'] for d in json_dict['items']]\n",
    "    return subquestions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "FOLLOWUP_PROMPT = \"\"\"\\\n",
    "Given a question, answer \"yes\" only if the question is complex and follow-up questions are needed or \"no\" if not.\n",
    "Always respond with \"no\" for short questions that are less than 8 words.\n",
    "Answer only with either \"yes\" or \"no\" with no additional text or explanations.\n",
    "\n",
    "# Example 1\n",
    "<User Question>\n",
    "Compare and contrast the revenue growth and EBITDA of Uber and Lyft for year 2021\n",
    "\n",
    "<Output>\n",
    "yes\n",
    "\n",
    "# Example 2\n",
    "<User Question>\n",
    "How is the Coast Guard using artificial intelligence?\n",
    "\n",
    "<Output>\n",
    "No\n",
    "\n",
    "# Example 3\n",
    "<User Question\n",
    "What is AutoGluon?\n",
    "\n",
    "<Output>\n",
    "No\n",
    "\n",
    "# Example 4\n",
    "<User Question>\n",
    "{query_str}\n",
    "\n",
    "<Output>\n",
    "\"\"\"\n",
    "\n",
    "def needs_followup(question:str, llm, parse=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Decide if follow-up questions are needed\n",
    "    \"\"\"\n",
    "    prompt = SafeFormatter({'query_str': question}).format(FOLLOWUP_PROMPT)\n",
    "    output = llm.prompt(prompt)\n",
    "    return \"yes\" in output.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "TITLE_PROMPT = \"\"\"\\\n",
    "Context: {context_str}.\\n\\nGive a title that summarizes what the context describes. \\\n",
    "Title: \"\"\"\n",
    "\n",
    "class Title(BaseModel):\n",
    "    title: str = Field(description=\"title of text\")\n",
    "\n",
    "def extract_title(docs_or_text:Union[List[Document], str], llm, max_words=1024, retries=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract or infer the title for the given text\n",
    "\n",
    "    **Args**\n",
    "      - docs_or_text: Either a list of LangChain Document objects or a single text string\n",
    "      - llm: An onprem.LLM instance\n",
    "      - max_words: Maximum words to consider\n",
    "      - retries: Number of tries to correctly extract title\n",
    "    \"\"\"\n",
    "    if not docs_or_text:\n",
    "        raise ValueError('docs_or_text cannot be empty')\n",
    "    if isinstance(docs_or_text, list):\n",
    "        text = \"\"\n",
    "        for doc in docs_or_text:\n",
    "            if not doc.page_content.strip() or len(doc.page_content.strip()) < 32:\n",
    "                continue\n",
    "            text = doc.page_content.strip()\n",
    "            break\n",
    "    else:\n",
    "        text = docs_or_text\n",
    "    text = \" \".join(text.split()[:max_words])\n",
    "    for i in range(retries+1):\n",
    "        obj = llm.pydantic_prompt(TITLE_PROMPT.replace(\"{context_str}\", text), pydantic_model=Title)\n",
    "        if not isinstance(obj, str):\n",
    "            break\n",
    "    return \"\" if isinstance(obj, str) else obj.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from onprem.utils import CAPTION_STR\n",
    "\n",
    "TABLE_PROMPT_EXACT= \"\"\"\\\n",
    "{context_str}\\n\\nWhat is this table about? Give a very concise summary (imagine you are adding a new caption and summary for this table), \\\n",
    "and output the real/existing table title/caption if context provided.\"\"\"\n",
    "TABLE_PROMPT= \"\"\"\\\n",
    "{context_str}\\n\\nWhat is this table about? Give a very concise summary (imagine you are adding a new caption and summary for this table).\"\"\"\n",
    "class TableSummary(BaseModel):\n",
    "    summary: str = Field(description=\"concise summary or caption of table\")\n",
    "\n",
    "def caption_table_text(table_text:str, llm, max_chars=4096, retries=1, attempt_exact=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Caption table text\n",
    "    \"\"\"\n",
    "    table_text = table_text[:max_chars]\n",
    "    for i in range(retries+1):\n",
    "        prompt = TABLE_PROMPT_EXACT if attempt_exact else TABLE_PROMPT\n",
    "        obj = llm.pydantic_prompt(prompt.replace(\"{context_str}\", table_text), pydantic_model=Title)\n",
    "        if not isinstance(obj, str):\n",
    "            break\n",
    "    return \"\" if isinstance(obj, str) else obj.title\n",
    "    \n",
    "def summarize_tables(docs:List[Document], llm, max_chars=4096, max_tables=3, retries=1, \n",
    "                   attempt_exact=False,\n",
    "                   only_caption_missing=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Given a list of Documents, auto-caption or summarize any tables within list.\n",
    "\n",
    "    **Args**\n",
    "      - docs_or_text: A list of LangChain Document objects\n",
    "      - llm: An onprem.LLM instance\n",
    "      - max_chars: Maximum characters to consider\n",
    "      - retries: Number of tries to correctly auto-caption table\n",
    "      - attempt_exact: Try to exact existing caption if it exists.\n",
    "      - only_caption_missing: Only caption tables without a caption\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        raise ValueError('docs_or_text cannot be empty')\n",
    "    n_processed = 0\n",
    "    for doc in docs:\n",
    "        if not 'table' in doc.metadata or not doc.metadata['table']:\n",
    "            continue\n",
    "        if only_caption_missing and CAPTION_STR in doc.page_content:\n",
    "            continue\n",
    "        doc.page_content = caption_table_text(doc.page_content, \n",
    "                                              llm=llm, max_chars=max_chars, retries=retries) + '\\n\\n' + doc.page_content        \n",
    "        n_processed +=1\n",
    "        if n_processed >= max_tables:\n",
    "            break\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "from onprem import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (3904) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "# |  notest\n",
    "\n",
    "llm = LLM(default_model='llama', n_gpu_layers=-1, verbose=False, mute_stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding on Follow-Up Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |  notest\n",
    "\n",
    "needs_followup('What is ktrain?', llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |  notest\n",
    "\n",
    "needs_followup('What is the capital of France?', llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |  notest\n",
    "\n",
    "needs_followup(\"How was Paul Grahams life different before, during, and after YC?\", llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |  notest\n",
    "\n",
    "needs_followup(\"Compare and contrast the customer segments and geographies of Lyft and Uber that grew the fastest.\", llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |  notest\n",
    "\n",
    "needs_followup(\"Compare and contrast Uber and Lyft.\", llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Follow-Up Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['What are the customer segments that drove growth for Uber', 'What are the geographies where Uber grew the fastest', 'What are the customer segments that drove growth for Lyft', 'What are the geographies where Lyft grew the fastest']\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "question = \"Compare and contrast the customer segments and geographies of Lyft and Uber that grew the fastest.\"\n",
    "subquestions = decompose_question(question, llm=llm, parse=False)\n",
    "print()\n",
    "print(subquestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem.ingest import load_single_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Low-Code Library for Augmented Machine Learning\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "docs = load_single_document('tests/sample_data/ktrain_paper/ktrain_paper.pdf')\n",
    "title = extract_title(docs, llm=llm)\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-Caption Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "docs = load_single_document('tests/sample_data/ktrain_paper/ktrain_paper.pdf', infer_table_structure=True)\n",
    "table_doc = [d for d in docs if d.metadata['table']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "summarize_tables([table_doc], llm, only_caption_missing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of ML Tasks Supported in Popular Libraries\n",
      "\n",
      "Table 1: A comparison of ML tasks supported out-of-the-box in popular low-code and AutoML libraries for tabular, image, audio, text and graph data.\n",
      "\n",
      "The following table in markdown format has the caption: Table 1: A comparison of ML tasks supported out-of-the-box in popular low-code and AutoML libraries for tabular, image, audio, text and graph data. The following table in markdown format includes this list of columns:\n",
      "- Task\n",
      "- ktrain\n",
      "- fastai\n",
      "- Ludwig\n",
      "- AutoKeras\n",
      "- AutoGluon\n",
      "\n",
      "|Task|ktrain|fastai|Ludwig|AutoKeras|AutoGluon|\n",
      "|---|---|---|---|---|---|\n",
      "|Tabular: Classification/Regression|✓|✓|✓|✓|✓|\n",
      "|Tabular: Causal Machine Learning|✓|None|None|None|None|\n",
      "|Tabular: Time Series Forecasting|None|None|✓|✓|None|\n",
      "|Tabular: Collaborative Filtering|None|✓|None|None|None|\n",
      "|Image: Classification/Regression|✓|✓|✓|✓|✓|\n",
      "|Image: Object Detection|prefitted*|✓|None|None|✓|\n",
      "|Image: Image Captioning|prefitted*|None|✓|None|None|\n",
      "|Image: Segmentation|None|✓|None|None|None|\n",
      "|Image: GANs|None|✓|None|None|None|\n",
      "|Image: Keypoint/Pose Estimation|None|✓|None|None|None|\n",
      "|Audio: Classification/Regression|None|None|✓|None|None|\n",
      "|Audio: Speech Transcription|prefitted*|None|✓|None|None|\n",
      "|Text: Classification/Regression|✓|✓|✓|✓|✓|\n",
      "|Text: Sequence-Tagging|✓|None|✓|None|None|\n",
      "|Text: Unsupervised Topic Modeling|✓|None|None|None|None|\n",
      "|Text: Semantic Search|✓|None|None|None|None|\n",
      "|Text: End-to-End Question-Answering|✓*|None|None|None|None|\n",
      "|Text: Zero-Shot Learning|✓|None|None|None|None|\n",
      "|Text: Language Translation|prefitted*|None|✓|None|None|\n",
      "|Text: Summarization|prefitted*|None|✓|None|None|\n",
      "|Text: Text Extraction|✓|None|None|None|None|\n",
      "|Text: QA-Based Information Extraction|✓*|None|None|None|None|\n",
      "|Text: Keyphrase Extraction|✓|None|None|None|None|\n",
      "|Graph: Node Classification|✓|None|None|None|None|\n",
      "|Graph: Link Prediction|✓|None|None|None|None|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "print(table_doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `caption_tables` function pre-pended the table text with an alternative caption in this example.\n",
    "You can skip over tables that already have captions by supplying `only_caption_missing=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
