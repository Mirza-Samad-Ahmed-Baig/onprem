{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hf.data.base\n",
    "\n",
    "> Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp hf.data.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Data module\n",
    "\"\"\"\n",
    "\n",
    "from onprem.hf.data.tokens import Tokens\n",
    "\n",
    "\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    Base data tokenization class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, columns, maxlength):\n",
    "        \"\"\"\n",
    "        Creates new base instance for tokenizing data.\n",
    "\n",
    "        Args:\n",
    "            tokenizer: model tokenizer\n",
    "            columns: column names\n",
    "            maxlength: maximum sequence length\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.columns = columns\n",
    "        self.maxlength = maxlength\n",
    "\n",
    "    def __call__(self, train, validation, workers):\n",
    "        \"\"\"\n",
    "        Tokenizes training and validation data and returns processed datasets.\n",
    "\n",
    "        Args:\n",
    "            train: training data\n",
    "            validation: validation data\n",
    "            workers: number of concurrent tokenizers when processing datasets, only main process used when set to None\n",
    "\n",
    "        Returns:\n",
    "            (train, validation)\n",
    "        \"\"\"\n",
    "\n",
    "        return (self.prepare(train, self.process, workers), self.prepare(validation, self.process, workers) if validation else None)\n",
    "\n",
    "    def prepare(self, data, fn, workers):\n",
    "        \"\"\"\n",
    "        Prepares and tokenizes data for training.\n",
    "\n",
    "        Args:\n",
    "            data: input data\n",
    "            fn: tokenize processing function to apply\n",
    "            workers: number of concurrent tokenizers when processing datasets, only main process used when set to None\n",
    "\n",
    "        Returns:\n",
    "            tokens\n",
    "        \"\"\"\n",
    "\n",
    "        if hasattr(data, \"map\"):\n",
    "            # Hugging Face dataset\n",
    "            tokens = data.map(fn, batched=True, num_proc=workers, remove_columns=data.column_names)\n",
    "        else:\n",
    "            # Re-orient data into columns for efficient batch tokenization\n",
    "            columns = {}\n",
    "            if hasattr(data, \"columns\"):\n",
    "                # Polars/pandas DataFrame\n",
    "                for column in data.columns:\n",
    "                    columns[column] = list(data[column])\n",
    "            else:\n",
    "                # Iterable dicts\n",
    "                for row in data:\n",
    "                    for column in row.keys():\n",
    "                        if column not in columns:\n",
    "                            columns[column] = []\n",
    "\n",
    "                        columns[column].append(row[column])\n",
    "\n",
    "            # Process column-oriented data\n",
    "            tokens = Tokens(fn(columns))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def labels(self, data):\n",
    "        \"\"\"\n",
    "        Extracts a list of unique labels from data.\n",
    "\n",
    "        Args:\n",
    "            data: input data\n",
    "\n",
    "        Returns:\n",
    "            list of unique labels\n",
    "        \"\"\"\n",
    "\n",
    "        # Last column is label\n",
    "        column = self.columns[-1]\n",
    "\n",
    "        # Return length of labels if it's an array\n",
    "        length = self.length(data[column][0] if hasattr(data, \"columns\") else data[0][column])\n",
    "        if length:\n",
    "            return length\n",
    "\n",
    "        if hasattr(data, \"map\"):\n",
    "            # Hugging Face dataset\n",
    "            labels = sorted(data.unique(self.columns[-1]))\n",
    "        elif hasattr(data, \"columns\"):\n",
    "            # Polars/pandas DataFrame\n",
    "            labels = sorted(data[self.columns[-1]].unique())\n",
    "        else:\n",
    "            # Iterable dicts\n",
    "            labels = sorted({row[self.columns[-1]] for row in data})\n",
    "\n",
    "        # Labels are single numeric values per entry\n",
    "        #   - Consider a regression task if at least one label isn't an integer\n",
    "        #   - Otherwise use number of labels for a classification task\n",
    "        return 1 if [x for x in labels if float(x) != int(x)] else len(labels)\n",
    "\n",
    "    def process(self, data):\n",
    "        \"\"\"\n",
    "        Tokenizes batch of input data\n",
    "\n",
    "        Args:\n",
    "            data: input data batch\n",
    "\n",
    "        Returns:\n",
    "            tokenized data\n",
    "        \"\"\"\n",
    "\n",
    "        return data\n",
    "\n",
    "    def length(self, value):\n",
    "        \"\"\"\n",
    "        Returns the length of value if value has a len function defined. Otherwise,\n",
    "        None is returned.\n",
    "\n",
    "        Args:\n",
    "            value: value to check\n",
    "\n",
    "        Returns:\n",
    "            length of value if available, otherwise returns None\n",
    "        \"\"\"\n",
    "\n",
    "        return len(value) if hasattr(value, \"__len__\") else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
