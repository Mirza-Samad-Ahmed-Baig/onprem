{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from onprem.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Prompts to Solve Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows various examples of using **[OnPrem.LLM](https://github.com/amaiya/onprem)** to solve different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the `LLM` instance\n",
    "\n",
    "In this notebook, we will use a model called **[Mistral-7B-Instruct-v0.1](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)**.  When selecting a model, it is important to inspect the model's home page and identify the correct prompt format.  The prompt format for this model is [located here](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF#prompt-template-mistral), and we will supply it directly to the `LLM` constructor along with the URL to the specific model file we want (i.e., *mistral-7b-instruct-v0.1.Q4_K_M.gguf*).  We will offload layers to our GPU(s) to speed up inference using the `n_gpu_layers` parameter. (For more information on GPU acceleration, see [here](https://amaiya.github.io/onprem/#speeding-up-inference-using-a-gpu).) For the purposes of this notebook, we also supply `temperature=0` so that there is no variability in outputs.  You can increase this value for more creativity in the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "from onprem import LLM\n",
    "import os\n",
    "\n",
    "llm = LLM(model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf', \n",
    "          prompt_template= \"<s>[INST] {prompt} [/INST]\",\n",
    "          n_gpu_layers=33,\n",
    "          temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction\n",
    "\n",
    "This is an example of *zero-shot prompting*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cillian Murphy, Florence Pugh"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "prompt = \"\"\"Extract the names of people in the supplied sentences. Separate the names with commas.\n",
    "[Sentence]: I like Cillian Murphy's acting. Florence Pugh is great, too.\n",
    "[People]:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt, stop=['\\n\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complicated example of **Information Extraction** using *few-shot prompting*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name: Norton Allan Schwartz | Current Position: President | Current Company: Institute for Defense Analyses"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\" Extract the Name, Current Position, and Current Company from each piece of Text. \n",
    "\n",
    "Text: Alan F. Estevez serves as the Under Secretary of Commerce for Industry and Security.  As Under Secretary, Mr. Estevez leads\n",
    "the Bureau of Industry and Security, which advances U.S. national security, foreign policy, and economic objectives by ensuring an\n",
    "effective export control and treaty compliance system and promoting U.S. strategic technology leadership.\n",
    "A: Name:  Alan F. Estevez | Current Position: Under Secretary | Current Company: Bureau of Industry and Security\n",
    "\n",
    "Text: Pichai Sundararajan (born June 10, 1972[3][4][5]), better known as Sundar Pichai (/ˈsʊndɑːr pɪˈtʃaɪ/), is an Indian-born American\n",
    "business executive.[6][7] He is the chief executive officer (CEO) of Alphabet Inc. and its subsidiary Google.[8]\n",
    "A: Name:   Sundar Pichai | Current Position: CEO | Current Company: Google\n",
    "\n",
    "Text: Norton Allan Schwartz (born December 14, 1951)[1] is a retired United States Air Force general[2] who served as the 19th Chief of Staff of the \n",
    "Air Force from August 12, 2008, until his retirement in 2012.[3] He previously served as commander, United States Transportation Command from \n",
    "September 2005 to August 2008. He is currently the president of the Institute for Defense Analyses, serving since January 2, 2020.[4]\n",
    "A:\"\"\"\n",
    "saved_output = llm.prompt(prompt, stop=['\\n\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I do not want to go."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Correct the grammar and spelling in the supplied sentences.  Here are some examples.\n",
    "[Sentence]:\n",
    "I love goin to the beach.\n",
    "[Correction]: I love going to the beach.\n",
    "[Sentence]:\n",
    "Let me hav it!\n",
    "[Correction]: Let me have it!\n",
    "[Sentence]:\n",
    "It have too many drawbacks.\n",
    "[Correction]: It has too many drawbacks.\n",
    "[Sentence]:\n",
    "I do not wan to go\n",
    "[Correction]:\"\"\"\n",
    "saved_output = llm.prompt(prompt, stop=['\\n\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Positive"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Classify each sentence as either positive, negative, or neutral.  Here are some examples.\n",
    "[Sentence]: I love going to the beach.\n",
    "[[Classification]: Positive\n",
    "[Sentence]: It is 10am right now.\n",
    "[Classification]: Neutral\n",
    "[Sentence]: I just got fired from my job.\n",
    "[Classification]: Negative\n",
    "[Sentence]: The reactivity of  your team has been amazing, thanks!\n",
    "[Classification]:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt, stop=['\\n\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Following the withdrawal of American troops, as decided by Presidents Trump and Biden, Kabul, the capital of Afghanistan, was captured by the Taliban in just a few hours without any opposition."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Paraphrase the following text delimited by triple backticks using a single sentence. \n",
    "```After a war lasting 20 years, following the decision taken first by President Trump and then by President Biden to withdraw American troops, Kabul, the capital of Afghanistan, fell within a few hours to the Taliban, without resistance.```\n",
    "\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU plan"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\" Compelte the answer:\n",
    "[Question]: When was NLP Cloud founded?\n",
    "[Context]: NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production.\n",
    "[Answer]: 2021\n",
    "\n",
    "[Question]:  What did NLP Cloud develop?\n",
    "[Context]: NLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then.\n",
    "[Answer]: API\n",
    "\n",
    "[Question]: When can plans be stopped?\n",
    "[Context]: All plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice.\n",
    "[Answer]: Anytime\n",
    "\n",
    "[Question]: Which plan is recommended for GPT-J?\n",
    "[Context]: The main challenge with GPT-J is memory consumption. Using a GPU plan is recommended.\n",
    "[Answer]:\"\"\"\n",
    "saved_output = llm.prompt(prompt, stop=['\\n\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Product Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stylish t-shirts for men at an affordable price of $39."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Generate a short Sentence from the Keywords. Here are some examples.\n",
    "[Keywords]: shoes, women, $59\n",
    "[Sentence]: Beautiful shoes for women at the price of $59.\n",
    "\n",
    "[Keywords]: trousers, men, $69\n",
    "[Sentence]: Modern trousers for men, for $69 only.\n",
    "\n",
    "[Keywords]: gloves, winter, $19\n",
    "[Sentence]:  Amazingly hot gloves for cold winters, at $19.\n",
    "\n",
    "[Keywords]:  t-shirt, men, $39\n",
    "[Sentence]:\"\"\"\n",
    "saved_output = llm.prompt(prompt, stop=['\\n\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Climate change is real and it's time for action. Let's work together to reduce our carbon footprint and protect our planet for future generations. #ClimateActionNow"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "prompt = \"\"\"Generate a tweet based on the supplied Keyword. Here are some examples.\n",
    "[Keyword]:\n",
    "markets\n",
    "[Tweet]:\n",
    "Take feedback from nature and markets, not from people\n",
    "###\n",
    "[Keyword]:\n",
    "children\n",
    "[Tweet]:\n",
    "Maybe we die so we can come back as children.\n",
    "###\n",
    "[Keyword]:\n",
    "startups\n",
    "[Tweet]: \n",
    "Startups should not worry about how to put out fires, they should worry about how to start them.\n",
    "###\n",
    "[Keyword]:\n",
    "climate change\n",
    "[Tweet]:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "(FYI: The [Zephyr-7B](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF) model seems to be a bit better at story-telling than Mistral-7B.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subject: Introducing Tesla to Our Shareholders\n",
      "\n",
      "Dear Valued Shareholders,\n",
      "\n",
      "We are excited to announce that we have recently acquired a stake in Tesla, Inc., a leading electric vehicle and clean energy company. As one of our valued shareholders, we believe this investment aligns with our commitment to sustainable practices and long-term growth opportunities.\n",
      "\n",
      "Tesla is at the forefront of innovation in the automotive industry, with a mission to accelerate the world's transition to sustainable energy. Their electric vehicles are not only eco-friendly but also offer cutting-edge technology and exceptional performance. In addition to their automotive business, Tesla has made significant strides in the development of renewable energy solutions, including solar panels and battery storage systems.\n",
      "\n",
      "We believe that our investment in Tesla will provide us with exposure to a rapidly growing market and the opportunity to generate attractive returns for our shareholders. We are confident in Tesla's ability to continue innovating and disrupting traditional industries, and we look forward to seeing the company's growth and success in the years to come.\n",
      "\n",
      "As always, our primary goal is to create value for our shareholders, and we believe that this investment is a step towards achieving that goal. We will keep you updated on our progress and any developments related to our investment in Tesla.\n",
      "\n",
      "Thank you for your continued support and trust in our company.\n",
      "\n",
      "Sincerely,\n",
      "[Your Company]"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "prompt = \"\"\"Generate an email introducing Tesla to shareholders.\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk to Your Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from ./sample_data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 3/3 [00:00<00:00, 22.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 new documents from ./sample_data/\n",
      "Split into 153 chunks of text (max. 500 chars each)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "llm.ingest(\"./sample_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ktrain is a low-code library for augmented machine learning that automates or semi-automates the full machine learning workflow from curating and preprocessing inputs to training, tuning, troubleshooting, and applying models. It is designed to help further democratize machine learning by enabling beginners and domain experts with minimal programming or data science experience."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "result = llm.ask(\"What is ktrain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro-Tip**: You can try different models or re-phrase the question, which may provide better performance for certain tasks.  For instance, you can use the default [Wizard-Vicuna-7B-Uncensored](https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGUF) model, which can also solve the above tasks, as shown in this example [Google Colab notebook](https://colab.research.google.com/drive/1LVeacsQ9dmE1BVzwR3eTLukpeRIMmUqi?usp=sharing) of **OnPrem.LLM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
