{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Prompts to Solve Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows various examples of using **[OnPrem.LLM](https://github.com/amaiya/onprem)** to solve different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the `LLM` instance\n",
    "\n",
    "In this notebook, we will use the Llama-3.1-8B model from Meta. In particular, we will use **[Meta-Llama-3.1-8B-Instruct-GGUF](https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF)**. There are different instances of this model on the Hugging Face model hub, and we will use the one from LM Studio. When selecting a model that is different than the [default ones](https://amaiya.github.io/onprem/#setup) in **OnPrem.LLM**, it is important to inspect the model's home page and identify the correct prompt format.  The prompt format for this model is [located here](https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF#prompt-template), and we will supply it directly to the `LLM` constructor along with the URL to the specific model file we want (i.e., *Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf*).  We will offload layers to our GPU(s) to speed up inference using the `n_gpu_layers` parameter. (For more information on GPU acceleration, see [here](https://amaiya.github.io/onprem/#speeding-up-inference-using-a-gpu).) For the purposes of this notebook, we also supply `temperature=0` so that there is no variability in outputs.  You can increase this value for more creativity in the outputs. Note that you can change the *system prompt* (i.e., \"*You are a super-intelligent helpful assistant...*\") to fit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "from onprem import LLM\n",
    "import os\n",
    "prompt_template = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a super-intelligent helpful assistant that executes instructions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (3904) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "llm = LLM(model_url='https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf', \n",
    "          prompt_template= prompt_template,\n",
    "          n_gpu_layers=-1,\n",
    "          temperature=0, \n",
    "          verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, if supplying the convenience parameter, `default_model='llama'` to the `LLM` constructor, `model_url` and `prompt_template` are set automatically and do not need to be supplied as we did above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction\n",
    "\n",
    "This is an example of *zero-shot prompting*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cillian Murphy, Florence Pugh"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "prompt = \"\"\"Extract the names of people in the supplied sentences. Separate the names with commas.\n",
    "[Sentence]: I like Cillian Murphy's acting. Florence Pugh is great, too.\n",
    "[People]:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt, stop=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complicated example of **Information Extraction** using *few-shot prompting*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Norton Allan Schwartz | Current Position: President | Current Company: Institute for Defense Analyses"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\" Extract the Name, Current Position, and Current Company from each piece of Text. \n",
    "\n",
    "Text: Alan F. Estevez serves as the Under Secretary of Commerce for Industry and Security.  As Under Secretary, Mr. Estevez leads\n",
    "the Bureau of Industry and Security, which advances U.S. national security, foreign policy, and economic objectives by ensuring an\n",
    "effective export control and treaty compliance system and promoting U.S. strategic technology leadership.\n",
    "A: Name:  Alan F. Estevez | Current Position: Under Secretary | Current Company: Bureau of Industry and Security\n",
    "\n",
    "Text: Pichai Sundararajan (born June 10, 1972[3][4][5]), better known as Sundar Pichai (/ˈsʊndɑːr pɪˈtʃaɪ/), is an Indian-born American\n",
    "business executive.[6][7] He is the chief executive officer (CEO) of Alphabet Inc. and its subsidiary Google.[8]\n",
    "A: Name:   Sundar Pichai | Current Position: CEO | Current Company: Google\n",
    "\n",
    "Now, provide the answer (A) from this Text:\n",
    "\n",
    "Text: Norton Allan Schwartz (born December 14, 1951)[1] is a retired United States Air Force general[2] who served as the 19th Chief of Staff of the \n",
    "Air Force from August 12, 2008, until his retirement in 2012.[3] He previously served as commander, United States Transportation Command from \n",
    "September 2005 to August 2008. He is currently the president of the Institute for Defense Analyses, serving since January 2, 2020.[4]\n",
    "A:\"\"\"\n",
    "saved_output = llm.prompt(prompt, stop=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Parsing\n",
    "\n",
    "Resume parsing is yet an even more complex example of information extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-13 12:52:50--  https://arun.maiya.net/asmcv.pdf\n",
      "Resolving arun.maiya.net (arun.maiya.net)... 185.199.109.153, 185.199.108.153, 185.199.111.153, ...\n",
      "Connecting to arun.maiya.net (arun.maiya.net)|185.199.109.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 62791 (61K) [application/pdf]\n",
      "Saving to: ‘/tmp/cv.pdf’\n",
      "\n",
      "/tmp/cv.pdf         100%[===================>]  61.32K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2024-11-13 12:52:51 (36.3 MB/s) - ‘/tmp/cv.pdf’ saved [62791/62791]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "!wget https://arun.maiya.net/asmcv.pdf -O /tmp/cv.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem.ingest import load_single_document\n",
    "docs = load_single_document('/tmp/cv.pdf')\n",
    "resume_text = docs[0].page_content # we'll only consider the first page of CV as \"resume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"\n",
    "Analyze the resume below and extract the relevant details. Format the response in JSON according to the specified structure below. \n",
    "Only return the JSON response, with no additional text or explanations.\n",
    "\n",
    "Ensure to:\n",
    "- Format the full name in proper case.\n",
    "- Remove any spaces and country code from the contact number.\n",
    "- Format dates as \"dd-mm-yyyy\" if given in a more complex format, or retain the year if only the year is present.\n",
    "- Do not make up a phone number. \n",
    "- Extract only the first two jobs for Work Experience.\n",
    "\n",
    "Use the following JSON structure:\n",
    "\n",
    "```json\n",
    "{\n",
    " \"Personal Information\": {\n",
    "    \"Name\": \" \",\n",
    "    \"Contact Number\": \" \",\n",
    "    \"Address\": \" \",\n",
    "    \"Email\": \" \",\n",
    "    \"Date of Birth\": \" \"\n",
    "  },\n",
    "  \"Education\": [\n",
    "    {\n",
    "      \"Degree\": \" \",\n",
    "      \"Institution\": \" \",\n",
    "      \"Year\": \" \"\n",
    "    },\n",
    "    // Additional educational qualifications in a similar format\n",
    "  ],\n",
    "  \"Work Experience\": [\n",
    "    {\n",
    "      \"Position\": \" \",\n",
    "      \"Organization\": \" \",\n",
    "      \"Duration\": \" \",\n",
    "      \"Responsibilities\": \" \"\n",
    "    },\n",
    "    // Additional work experiences in a similar format\n",
    "  ],\n",
    "  \"Skills\": [\n",
    "  {\n",
    "    \"Skills\": \" \", // e.g., Python, R, Java, statistics, quantitative psychology, applied mathematics, machine learning, gel electrophoresis\n",
    "  },\n",
    "  // A list of skills or fields that the person has experience with\n",
    "  ],\n",
    "}\n",
    "```\n",
    "\n",
    "Here is the text of the resume:\n",
    "\n",
    "---RESUMETXT---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"Personal Information\": {\n",
      "    \"Name\": \"Arun S. Maiya\",\n",
      "    \"Contact Number\": \"\",\n",
      "    \"Address\": \"\",\n",
      "    \"Email\": \"arun@maiya.net\",\n",
      "    \"Date of Birth\": \"\"\n",
      "  },\n",
      "  \"Education\": [\n",
      "    {\n",
      "      \"Degree\": \"Ph.D.\",\n",
      "      \"Institution\": \"University of Illinois at Chicago\",\n",
      "      \"Year\": \" \"\n",
      "    },\n",
      "    {\n",
      "      \"Degree\": \"M.S.\",\n",
      "      \"Institution\": \"DePaul University\",\n",
      "      \"Year\": \" \"\n",
      "    },\n",
      "    {\n",
      "      \"Degree\": \"B.S.\",\n",
      "      \"Institution\": \"University of Illinois at Urbana-Champaign\",\n",
      "      \"Year\": \" \"\n",
      "    }\n",
      "  ],\n",
      "  \"Work Experience\": [\n",
      "    {\n",
      "      \"Position\": \"Research Leader\",\n",
      "      \"Organization\": \"Institute for Defense Analyses – Alexandria, VA USA\",\n",
      "      \"Duration\": \"2011-Present\",\n",
      "      \"Responsibilities\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"Position\": \"Researcher\",\n",
      "      \"Organization\": \"University of Illinois at Chicago\",\n",
      "      \"Duration\": \"2007-2011\",\n",
      "      \"Responsibilities\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"Skills\": [\n",
      "    {\n",
      "      \"Skills\": \"applied machine learning, data science, natural language processing (NLP), network science, computer vision\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "json_string = llm.prompt(prompt.replace('---RESUMETXT---', resume_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the output to a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Personal Information', 'Education', 'Work Experience', 'Skills'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "import json\n",
    "d = json.loads(json_string)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arun S. Maiya'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "d['Personal Information']['Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Outputs\n",
    "\n",
    "In the example above, we prompted the model to output results as a JSON string with some prompt engineering.  The `LLM.pydantic_prompt` method lets you more easily describe your desired output structure by defining a Pydantic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"value\": \"35\",\n",
      "  \"unit\": \"mph\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "class MeasuredQuantity(BaseModel):\n",
    "    value: str = Field(description=\"Numeric value of the measurement\")\n",
    "    unit: str = Field(description=\"Unit of measurement (e.g., 'kg', 'm', 's')\")\n",
    "structured_output = llm.pydantic_prompt('He was going 35 mph.', pydantic_model=MeasuredQuantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "mph\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "print(structured_output.value) # 35\n",
    "print(structured_output.unit)  # mph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:**\n",
    "\n",
    "The `attempt_fix` parameter allows you to have the LLM attempt to fix any malformed or incomplete outputs.  The `fix_llm` parameter allows you to specific a different LLM to make the fix (the current LLM is used if `fix_llm=None`):\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "structured_output = llm.pydantic_prompt('He was going 35 mph.', pydantic_model=MeasuredQuantity,\n",
    "                                        attempt_fix=True, fix_llm=ChatOpenAI())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theme Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After analyzing the 20 survey responses, I have identified several thematic codes that capture the essence of what respondents noticed about nature. Here are the thematic codes:\n",
      "\n",
      "**Code 1: Wildlife Observations (6 responses)**\n",
      "\n",
      "* Examples:\n",
      "\t+ \"I saw a family of ducks waddling through a nearby park this morning.\"\n",
      "\t+ \"A squirrel darted up a tree, leaving a trail of nuts behind it.\"\n",
      "\n",
      "**Code 2: Natural Beauty and Patterns (7 responses)**\n",
      "\n",
      "* Examples:\n",
      "\t+ \"The sun's rays filtered through the leaves of trees, casting intricate patterns on the ground below.\"\n",
      "\t+ \"The dew on spider webs sparkled like diamonds in the morning sunlight\"\n",
      "\n",
      "**Code 3: Sounds and Music of Nature (4 responses)**\n",
      "\n",
      "* Examples:\n",
      "\t+ \"The sound of birdsong filled the air as I walked through a park this afternoon.\"\n",
      "\t+ \"The chirping of crickets filled the air as I walked past a field this evening.\"\n",
      "\n",
      "**Code 4: Movement and Activity in Nature (3 responses)**\n",
      "\n",
      "* Examples:\n",
      "\t+ \"A group of geese honked in unison as they flew overhead this afternoon.\"\n",
      "\t+ \"As I watched, a group of ants worked together to move a large pebble across the sidewalk.\"\n",
      "\n",
      "These thematic codes provide a framework for understanding the common themes and patterns that emerged from the survey responses."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Please provide thematic coding for the following 20 survey responses to the question: \"What did you notice about nature today?\"\n",
    "\n",
    "1. I noticed a family of ducks waddling through a nearby park this morning.\n",
    "2. As I walked by a tree, I saw a flurry of feathers and realized a bird had just landed in its branches.\n",
    "3. While driving, I observed a herd of deer gracefully moving through a meadow.\n",
    "4. The sun's rays filtered through the leaves of trees, casting intricate patterns on the ground below.\n",
    "5. As I stepped outside, a gentle breeze carried with it the fragrance of blooming flowers.\n",
    "6. A butterfly fluttered past me as I was sitting in my garden, reminding me to enjoy life's simple pleasures.\n",
    "7. The sound of birdsong filled the air as I walked through a park this afternoon.\n",
    "8. I saw a group of ants working together to move a large pebble across the sidewalk.\n",
    "9. A squirrel darted up a tree, leaving a trail of nuts behind it.\n",
    "10. The leaves on the trees rustled as if whispering secrets in the wind.\n",
    "11. While hiking, I noticed the way sunlight filtered through the canopy of trees, creating patterns on the forest floor below.\n",
    "12. A dragonfly landed on a nearby pond, dipping its long legs into the water to drink.\n",
    "13. The chirping of crickets filled the air as I walked past a field this evening.\n",
    "14. The sky transformed from shades of blue to orange and red as the sun began to set.\n",
    "15. As the day came to a close, I watched as fireflies danced among the trees.\n",
    "16. A group of geese honked in unison as they flew overhead this afternoon.\n",
    "17. The way a butterfly's wings looked like delicate stained glass as it perched on a flower.\n",
    "18. The way the sun's rays seemed to bathe everything around me in a warm, golden light.\n",
    "19. As I walked by a field, I saw a group of rabbits darting through the tall grass.\n",
    "20. The way the dew on spider webs sparkled like diamonds in the morning sunlight\n",
    "\"\"\"\n",
    "saved_output = llm.prompt(prompt, stop=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not want to go."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Here are some examples.\n",
    "[Sentence]:\n",
    "I love goin to the beach.\n",
    "[Correction]: I love going to the beach.\n",
    "[Sentence]:\n",
    "Let me hav it!\n",
    "[Correction]: Let me have it!\n",
    "[Sentence]:\n",
    "It have too many drawbacks.\n",
    "[Correction]: It has too many drawbacks.\n",
    "\n",
    "What is the correction for the following sentence?\n",
    "\n",
    "[Sentence]:\n",
    "I do not wan to go\n",
    "[Correction]:\"\"\"\n",
    "saved_output = llm.prompt(prompt, stop=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Classify each sentence as either positive, negative, or neutral.  Here are some examples.\n",
    "[Sentence]: I love going to the beach.\n",
    "[[Classification]: Positive\n",
    "[Sentence]: It is 10am right now.\n",
    "[Classification]: Neutral\n",
    "[Sentence]: I just got fired from my job.\n",
    "[Classification]: Negative\n",
    "\n",
    "What is the classification for the following sentence? Answer with either Positive or Negative only.\n",
    "[Sentence]: The reactivity of  your team has been amazing, thanks!\n",
    "[Classification]:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt, stop=['\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After a 20-year war, Kabul fell to the Taliban within hours after US troops withdrew under decisions made by Presidents Trump and Biden."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Paraphrase the following text delimited by triple backticks using a single sentence. \n",
    "```After a war lasting 20 years, following the decision taken first by President Trump and then by President Biden to withdraw American troops, Kabul, the capital of Afghanistan, fell within a few hours to the Taliban, without resistance.```\n",
    "\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU plan"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\" Compelte the correct answer based on the Context. Answer should be a short word or phrase from Context.\n",
    "[Question]: When was NLP Cloud founded?\n",
    "[Context]: NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production.\n",
    "[Answer]: 2021\n",
    "\n",
    "[Question]:  What did NLP Cloud develop?\n",
    "[Context]: NLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then.\n",
    "[Answer]: API\n",
    "\n",
    "[Question]: When can plans be stopped?\n",
    "[Context]: All plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice.\n",
    "[Answer]: Anytime\n",
    "\n",
    "[Question]: Which plan is recommended for GPT-J?\n",
    "[Context]: The main challenge with GPT-J is memory consumption. Using a GPU plan is recommended.\n",
    "[Answer]:\"\"\"\n",
    "saved_output = llm.prompt(prompt, stop=['\\n\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Product Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A comfortable t-shirt for men, available at $39."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Generate a short Sentence from the Keywords. Here are some examples.\n",
    "[Keywords]: shoes, women, $59\n",
    "[Sentence]: Beautiful shoes for women at the price of $59.\n",
    "\n",
    "[Keywords]: trousers, men, $69\n",
    "[Sentence]: Modern trousers for men, for $69 only.\n",
    "\n",
    "[Keywords]: gloves, winter, $19\n",
    "[Sentence]:  Amazingly hot gloves for cold winters, at $19.\n",
    "\n",
    "Generate a sentence for the following Keywords and nothing else:\n",
    "\n",
    "[Keywords]:  t-shirt, men, $39\n",
    "[Sentence]:\"\"\"\n",
    "saved_output = llm.prompt(prompt, stop=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The climate is not changing, it's us who are changing the climate."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "prompt = \"\"\"Generate a tweet based on the supplied Keyword. Here are some examples.\n",
    "[Keyword]:\n",
    "markets\n",
    "[Tweet]:\n",
    "Take feedback from nature and markets, not from people\n",
    "###\n",
    "[Keyword]:\n",
    "children\n",
    "[Tweet]:\n",
    "Maybe we die so we can come back as children.\n",
    "###\n",
    "[Keyword]:\n",
    "startups\n",
    "[Tweet]: \n",
    "Startups should not worry about how to put out fires, they should worry about how to start them.\n",
    "\n",
    "Generate a Tweet for the following keyword and nothing else:\n",
    "\n",
    "###\n",
    "[Keyword]:\n",
    "climate change\n",
    "[Tweet]:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating an Email Draft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a draft email introducing Tesla to shareholders:\n",
      "\n",
      "Subject: Welcome to Tesla, Inc.\n",
      "\n",
      "Dear valued shareholder,\n",
      "\n",
      "I am thrilled to introduce you to Tesla, Inc., the pioneering electric vehicle and clean energy company. As a shareholder, you are part of our mission to accelerate the world's transition to sustainable energy.\n",
      "\n",
      "At Tesla, we are committed to pushing the boundaries of innovation and sustainability. Our products and services include:\n",
      "\n",
      "* Electric vehicles: We design, manufacture, and sell electric vehicles that are not only environmentally friendly but also technologically advanced.\n",
      "* Energy storage: Our energy storage products, such as the Powerwall and Powerpack, enable homeowners and businesses to store excess energy generated by their solar panels or other renewable sources.\n",
      "* Solar energy: We design, manufacture, and install solar panel systems for residential and commercial customers.\n",
      "\n",
      "As a shareholder, you are part of our journey towards a sustainable future. I invite you to explore our website and social media channels to learn more about our products and services.\n",
      "\n",
      "Thank you for your support and trust in Tesla, Inc.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "[Your Name]\n",
      "Tesla, Inc.\n",
      "\n",
      "Note: This is just a draft email and may not be suitable for actual use."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "prompt = \"\"\"Generate an email introducing Tesla to shareholders.\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk to Your Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to existing vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from ./sample_data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 1/1 [00:16<00:00, 16.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 new documents from ./sample_data/\n",
      "Split into 12 chunks of text (max. 500 chars each)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "llm.ingest(\"./tests/sample_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, ktrain is a tool that automates various aspects of the machine learning (ML) workflow. However, unlike traditional automation tools, ktrain also allows users to make choices and decisions that best fit their unique application requirements.\n",
      "\n",
      "In essence, ktrain uses automation to augment and complement human engineers, rather than attempting to entirely replace them."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "result = llm.ask(\"What is ktrain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro-Tip**: You can try different models or re-phrase the question/prompts accordingly, which may provide better performance for certain tasks.  For instance, by supplying `default_model=zephyr` to the `LLM` constructor and leaving `model_url` blank, the default `Zephyr-7B-beta` model will be used and also performs well on the above tasks. If not supplying any arguments to `LLM`, the default `Mistral-7B-v0.2` model used, as shown in this example [Google Colab notebook](https://colab.research.google.com/drive/1LVeacsQ9dmE1BVzwR3eTLukpeRIMmUqi?usp=sharing) of **OnPrem.LLM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
