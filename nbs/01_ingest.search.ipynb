{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.search\n",
    "\n",
    "> full-text search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "import math\n",
    "\n",
    "from whoosh import index\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "from whoosh.fields import *\n",
    "from whoosh.filedb.filestore import RamStorage\n",
    "from whoosh.qparser import MultifieldParser\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# IMPORTANT: Metadata fields in langchain_core.documents.Document objects\n",
    "#            (i.e., the input to WSearch.index_documents) should\n",
    "#            ideally match schema fields below, but this is not strictly required.\n",
    "#\n",
    "#            The page_content field is the only truly required field in supplied\n",
    "#            Document objects. All other fields, including dynamic fields, are optional. \n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "DEFAULT_SCHEMA = Schema(\n",
    "    page_content=TEXT(stored=True), # REQUIRED\n",
    "    id=ID(stored=True, unique=True),\n",
    "    source=KEYWORD(stored=True, commas=True), \n",
    "    source_search=TEXT(stored=True),\n",
    "    filepath=KEYWORD(stored=True, commas=True),\n",
    "    filepath_search=TEXT(stored=True),\n",
    "    filename=KEYWORD(stored=True),\n",
    "    ocr=BOOLEAN(stored=True),\n",
    "    table=BOOLEAN(stored=True),\n",
    "    markdown=BOOLEAN(stored=True),\n",
    "    page=NUMERIC(stored=True),\n",
    "    document_title=TEXT(stored=True),\n",
    "    md5=KEYWORD(stored=True),\n",
    "    mimetype=KEYWORD(stored=True),\n",
    "    extension=KEYWORD(stored=True),\n",
    "    filesize=NUMERIC(stored=True),\n",
    "    createdate=DATETIME(stored=True),\n",
    "    modifydate=DATETIME(stored=True),\n",
    "    tags=KEYWORD(stored=True, commas=True),\n",
    "    notes=TEXT(stored=True),\n",
    "    msg=TEXT(stored=True),\n",
    "    )\n",
    "DEFAULT_SCHEMA.add(\"*_t\", TEXT(stored=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_k\", KEYWORD(stored=True, commas=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_b\", BOOLEAN(stored=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_n\", NUMERIC(stored=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_d\", DATETIME(stored=True), glob=True)\n",
    "\n",
    "\n",
    "def default_schema():\n",
    "    schema = DEFAULT_SCHEMA\n",
    "    #if \"raw\" not in schema.stored_names():\n",
    "        #schema.add(\"raw\", TEXT(stored=True))\n",
    "    return schema\n",
    "\n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(self,\n",
    "                index_path: Optional[str]=None, # path to folder where search index is stored\n",
    "                index_name: Optional[str] = None # name of index\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializes full-text search engine\n",
    "        \"\"\"\n",
    "        self.index_path = index_path\n",
    "        self.index_name = index_name\n",
    "        if index_path and not index_name:\n",
    "            raise ValueError('index_name is required if index_path is supplied')\n",
    "        if index_path:\n",
    "            if not index.exists_in(index_path, indexname=index_name):\n",
    "                self.ix = __class__.initialize_index(index_path, index_name)\n",
    "            else:\n",
    "                self.ix = index.open_dir(index_path, indexname=index_name)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"No index_path was supplied, so an in-memory only index\"\n",
    "                \"was created using DEFAULT_SCHEMA\"\n",
    "            )\n",
    "            self.ix = RamStorage().create_index(default_schema())\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def index_exists_in(cls, index_path: str, index_name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Returns True if index exists with name, *indexname*, and path, *index_path*.\n",
    "        \"\"\"\n",
    "        return index.exists_in(index_path, indexname=index_name)\n",
    "\n",
    "    @classmethod\n",
    "    def initialize_index(\n",
    "        cls, index_path: str, index_name: str, schema: Optional[Schema] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize index\n",
    "\n",
    "        **Args**\n",
    "\n",
    "        - *index_path*: path to folder storing search index\n",
    "        - *index_name*: name of index\n",
    "        - *schema*: optional whoosh.fields.Schema object.\n",
    "                    If None, DEFAULT_SCHEMA is used\n",
    "        \"\"\"\n",
    "        schema = default_schema() if not schema else schema\n",
    "\n",
    "        if index.exists_in(index_path, indexname=index_name):\n",
    "            raise ValueError(\n",
    "                f\"There is already an existing index named {index_name}  with path {index_path} \\n\"\n",
    "                + f\"Delete {index_path} manually and try again.\"\n",
    "            )\n",
    "        if not os.path.exists(index_path):\n",
    "            os.makedirs(index_path)\n",
    "        ix = index.create_in(index_path, indexname=index_name, schema=schema)\n",
    "        return ix\n",
    "\n",
    "    def doc2dict(self, doc:Document):\n",
    "        \"\"\"\n",
    "        Convert LangChain Document to expected format\n",
    "        \"\"\"\n",
    "        stored_names = self.ix.schema.stored_names()\n",
    "        d = {}\n",
    "        for k,v in doc.metadata.items():\n",
    "            suffix = None\n",
    "            if k in stored_names:\n",
    "                suffix = ''\n",
    "            elif isinstance(v, bool):\n",
    "                suffix = '_b' if not k.endswith('_b') else ''\n",
    "            elif isinstance(v, str):\n",
    "                if k.endswith('_date'):\n",
    "                    suffix = '_d'\n",
    "                else:\n",
    "                    suffix = '_k'if not k.endswith('_k') else ''\n",
    "            elif isinstance(v, (int, float)):\n",
    "                suffix = '_n'if not k.endswith('_n') else ''\n",
    "            if suffix is not None:\n",
    "                d[k+suffix] = v\n",
    "        d['id'] = uuid.uuid4().hex\n",
    "        d['page_content' ] = doc.page_content\n",
    "        #d['raw'] = json.dumps(d)\n",
    "        if 'source' in d:\n",
    "            d['source_search'] = d['source']\n",
    "        if 'filepath' in d:\n",
    "            d['filepath_search'] = d['filepath']\n",
    "        return d\n",
    "\n",
    "\n",
    "    def index_documents(self,\n",
    "                        docs: Sequence[Document], # list of LangChain Documents\n",
    "                        verbose:bool=True, # Set to False to disable progress bar\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Indexes documents.\n",
    "        \"\"\"\n",
    "        writer = self.ix.writer()\n",
    "        for doc in tqdm(docs, total=len(docs), disable=not verbose):\n",
    "            d = self.doc2dict(doc)\n",
    "            writer.update_document(**d)\n",
    "        writer.commit(optimize=True)\n",
    "\n",
    "\n",
    "    def clear_index(self, confirm=True):\n",
    "        \"\"\"\n",
    "        Clears index\n",
    "        \"\"\"\n",
    "        shall = True\n",
    "        if confirm:\n",
    "            msg = (\n",
    "                f\"You are about to remove all documents from the search index.\"\n",
    "                + f\"(Original documents on file system will remain.) Are you sure?\"\n",
    "            )\n",
    "            shall = input(\"%s (Y/n) \" % msg) == \"Y\"\n",
    "        if shall and index.exists_in(\n",
    "            self.index_path, indexname=self.index_name\n",
    "        ):\n",
    "            ix = index.create_in(\n",
    "                self.index_path,\n",
    "                indexname=self.index_name,\n",
    "                schema=default_schema(),\n",
    "            )\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def get_index_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Gets size of index\n",
    "        \"\"\"\n",
    "        return self.ix.doc_count_all()\n",
    "\n",
    "\n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a generator to iterate through all indexed documents\n",
    "        \"\"\"\n",
    "        return self.ix.searcher().documents()\n",
    "\n",
    "\n",
    "    def get_doc(self, id:str):\n",
    "        \"\"\"\n",
    "        Get an indexed record by ID\n",
    "        \"\"\"\n",
    "        r = self.query(f'id:{id}')\n",
    "        return r['hits'][0] if len(r['hits']) > 0 else None\n",
    "\n",
    "\n",
    "    def query(\n",
    "            self,\n",
    "            q: str,\n",
    "            fields: Sequence = [\"page_content\"],\n",
    "            highlight: bool = True,\n",
    "            limit:int=10,\n",
    "            page:int=1,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Queries the index\n",
    "\n",
    "        **Args**\n",
    "\n",
    "        - *q*: the query string\n",
    "        - *fields*: a list of fields to search\n",
    "        - *highlight*: If True, highlight hits\n",
    "        - *limit*: results per page\n",
    "        - *page*: page of hits to return\n",
    "        \"\"\"\n",
    "        search_results = []\n",
    "        with self.ix.searcher() as searcher:\n",
    "            if page == 1:\n",
    "                results = searcher.search(\n",
    "                    MultifieldParser(fields, schema=self.ix.schema).parse(q), limit=limit)\n",
    "            else:\n",
    "                results = searcher.search_page(\n",
    "                    MultifieldParser(fields, schema=self.ix.schema).parse(q), page, limit)\n",
    "            total_hits = results.scored_length()\n",
    "            if page > math.ceil(total_hits/limit):\n",
    "               results = []\n",
    "            for r in results:\n",
    "                #d = json.loads(r[\"raw\"])\n",
    "                d = dict(r)\n",
    "                if highlight:\n",
    "                    for f in fields:\n",
    "                        if r[f] and isinstance(r[f], str):\n",
    "                            d['hl_'+f] = r.highlights(f) or r[f]\n",
    "\n",
    "                search_results.append(d)\n",
    "\n",
    "        return {'hits':search_results, 'total_hits':total_hits}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
