{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from onprem.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk to Your Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example of [OnPrem.LLM](https://github.com/amaiya/onprem) demonstrates retrieval augmented generation or RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the `LLM` instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use a model called [Zephyr-7B](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF).  When selecting a model, it is important to inspect the model's home page and identify the correct prompt format.  The prompt format for this model is [located here](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF), and we will supply it directly to the `LLM` constructor along with the URL to the model file we want (i.e., *zephyr-7b-beta.Q4_K_M.gguf*).  We will offload layers to our GPU(s) to speed up inference using the `n_gpu_layers` parameter. (For more information on GPU acceleration, see [here](https://amaiya.github.io/onprem/#speeding-up-inference-using-a-gpu).) For the purposes of this notebook, we also supply `temperature=0` so that there is no variability in outputs.  You can increase this value for more creativity in the outputs. Finally, we will choose a non-default location for our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem import LLM\n",
    "import tempfile\n",
    "\n",
    "vectordb_path = tempfile.mkdtemp()\n",
    "\n",
    "llm = LLM(model_url='https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf', \n",
    "          prompt_template= \"<|system|>\\n</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\",\n",
    "          n_gpu_layers=33,\n",
    "          temperature=0,\n",
    "          vectordb_path=vectordb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /tmp/tmpqzgfqepe\n",
      "Loading documents from ./sample_data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 3/3 [00:00<00:00, 23.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 new documents from ./sample_data/\n",
      "Split into 153 chunks of text (max. 500 chars each)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "llm.ingest(\"./sample_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking Questions to Your Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ktrain is a low-code library for augmented machine learning that aims to democratize machine learning by facilitating the full machine learning workflow from curating and preprocessing inputs to training, tuning, troubleshooting, and applying models. It places less emphasis on automating feature engineering compared to other automated machine learning tools like Auto-WEKA and H2O Driverless AI, but instead focuses on partially or fully automating other aspects of the machine learning workflow. Ktrain allows users to make choices that best fit their unique application requirements while also automating certain tasks algorithmically or through setting well-performing defaults. Its goal is to augment and complement human engineers rather than attempting to entirely replace them, thereby better exploiting the strengths of both humans and machines."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3411.77 ms\n",
      "llama_print_timings:      sample time =      66.89 ms /   162 runs   (    0.41 ms per token,  2421.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3410.57 ms /   514 tokens (    6.64 ms per token,   150.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3324.09 ms /   161 runs   (   20.65 ms per token,    48.43 tokens per second)\n",
      "llama_print_timings:       total time =    7391.30 ms\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "result = llm.ask(\"What is ktrain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is stored in `results['answer']`. The documents retrieved from the vector store used to generate the answer are stored in `results['source_documents']` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='lection (He et al., 2019). By contrast, ktrain places less emphasis on this aspect of au-\\ntomation and instead focuses on either partially or fully automating other aspects of the\\nmachine learning (ML) workﬂow. For these reasons, ktrain is less of a traditional Au-\\n2' metadata={'author': '', 'creationDate': \"D:20220406214054-04'00'\", 'creator': 'LaTeX with hyperref', 'file_path': '/home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf', 'format': 'PDF 1.4', 'keywords': '', 'modDate': \"D:20220406214054-04'00'\", 'page': 1, 'producer': 'dvips + GPL Ghostscript GIT PRERELEASE 9.22', 'source': '/home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf', 'subject': '', 'title': '', 'total_pages': 9, 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "print(result[\"source_documents\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatting with Your Documents\n",
    "\n",
    "Unlike `LLM.ask`, the `LLM.chat` method retains conversational memory at the expense of a larger context and an extra call to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ktrain is a low-code library for augmented machine learning that facilitates the full machine learning workflow from curating and preprocessing inputs to training, tuning, troubleshooting, and applying models. It automates or semi-automates certain aspects of the machine learning process, making it well-suited for domain experts who may have less experience with machine learning and software coding."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3411.77 ms\n",
      "llama_print_timings:      sample time =      32.12 ms /    83 runs   (    0.39 ms per token,  2583.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1314.72 ms /   478 tokens (    2.75 ms per token,   363.58 tokens per second)\n",
      "llama_print_timings:        eval time =    1714.36 ms /    82 runs   (   20.91 ms per token,    47.83 tokens per second)\n",
      "llama_print_timings:       total time =    3351.26 ms\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "result = llm.chat(\"What is ktrain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Can ktrain be used for image classification tasks in augmented machine learning?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3411.77 ms\n",
      "llama_print_timings:      sample time =       6.38 ms /    16 runs   (    0.40 ms per token,  2509.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     756.03 ms /   146 tokens (    5.18 ms per token,   193.12 tokens per second)\n",
      "llama_print_timings:        eval time =     342.13 ms /    15 runs   (   22.81 ms per token,    43.84 tokens per second)\n",
      "llama_print_timings:       total time =    1160.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, ktrain can be used for image classification tasks in augmented machine learning as it supports various types of data including images. The library provides a standard template for building supervised learning models that includes loading and preprocessing data, training and tuning models, evaluating and applying models, and visualizing results. Ktrain is designed to make machine learning more accessible and easier to apply by providing a unified interface to disparate machine learning tasks, reducing cognitive load, and facilitating the full machine learning workflow from curating and preprocessing inputs to training, tuning, troubleshooting, and applying models."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3411.77 ms\n",
      "llama_print_timings:      sample time =      48.29 ms /   126 runs   (    0.38 ms per token,  2609.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1669.11 ms /   600 tokens (    2.78 ms per token,   359.47 tokens per second)\n",
      "llama_print_timings:        eval time =    2603.06 ms /   125 runs   (   20.82 ms per token,    48.02 tokens per second)\n",
      "llama_print_timings:       total time =    4765.84 ms\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "result = llm.chat(\"Does it support image classification?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, ktrain can be used for image classification tasks in augmented machine learning as it supports various types of data including images. The library provides a standard template for building supervised learning models that includes loading and preprocessing data, training and tuning models, evaluating and applying models, and visualizing results. Ktrain is designed to make machine learning more accessible and easier to apply by providing a unified interface to disparate machine learning tasks, reducing cognitive load, and facilitating the full machine learning workflow from curating and preprocessing inputs to training, tuning, troubleshooting, and applying models.\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
