{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sk.clf\n",
    "\n",
    "> scikit-learn text classification module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp sk.clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Scikit-Learn Text Classification module\n",
    "\"\"\"\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from onprem.sk import base as U\n",
    "from joblib import dump, load\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model._base import LinearClassifierMixin, SparseCoefMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.sparse import coo_matrix, spmatrix\n",
    "\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, model=None):\n",
    "        \"\"\"\n",
    "        instantiate a classifier with an optional previously-saved model\n",
    "        \"\"\"\n",
    "        self.model = None\n",
    "\n",
    "    def create_model(self, ctype:str, texts:list=None, use_tfidf:bool=False, **kwargs):\n",
    "        \"\"\"\n",
    "        ```\n",
    "        create a model\n",
    "        Args:\n",
    "          - ctype(str): one of {'nbsvm', 'logreg', 'sgdclassifier'}\n",
    "          - texts(list): list of texts (optional: only used to infer token pattern)\n",
    "          - use_tfidf (bool):  If True, use TFIDFVectorizer. Otherwise, use CountVectorizer.\n",
    "          \n",
    "          - kwargs(dict):   additional parameters should have one of the following prefixes:\n",
    "                           vec__ :  hyperparameters to CountVectorizer (e.g., vec__max_features=10000)\n",
    "                           tfidf__ :  hyperparameters to TfidfTransformer\n",
    "                           clf__:   hyperparameters to classifier (specific to ctype).\n",
    "                                    If ctype='logreg', then an example is clf__solver='liblinear'.\n",
    "        ```\n",
    "        \"\"\"\n",
    "        if ctype == \"nbsvm\":\n",
    "            if kwargs.get(\"vec__binary\", False) is False:\n",
    "                warnings.warn(\"nbsvm must use binary=True - changing automatically\")\n",
    "            if use_tfidf:\n",
    "                warnings.warn(\"nbsvm must use use_tfidf=False = changing automatically\")\n",
    "        vec_kwargs = dict(\n",
    "            (k.replace(\"vec__\", \"\"), kwargs[k]) for k in kwargs if k.startswith(\"vec__\")\n",
    "        )\n",
    "        tfidf_kwargs = dict(\n",
    "            (k.replace(\"tfidf__\", \"\"), kwargs[k])\n",
    "            for k in kwargs\n",
    "            if k.startswith(\"tfidf__\")\n",
    "        )\n",
    "        clf_kwargs = dict(\n",
    "            (k.replace(\"clf__\", \"\"), kwargs[k]) for k in kwargs if k.startswith(\"clf__\")\n",
    "        )\n",
    "\n",
    "        if texts and U.is_chinese(U.detect_lang(texts)) and not vec_kwargs.get(\"token_pattern\", None):\n",
    "            vec_kwargs[\"token_pattern\"] = r\"(?u)\\b\\w+\\b\"\n",
    "        elif not kwargs.get(\"vec__token_pattern\", None):\n",
    "            vec_kwargs[\"token_pattern\"] = r\"\\w+|[%s]\" % string.punctuation\n",
    "\n",
    "        if ctype == \"nbsvm\":\n",
    "            clf = NBSVM(**clf_kwargs)\n",
    "        elif ctype == \"logreg\":\n",
    "            clf = LogisticRegression(**clf_kwargs)\n",
    "        elif ctype == \"sgdclassifier\":\n",
    "            clf = SGDClassifier(**clf_kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown ctype: %s\" % (ctype))\n",
    "\n",
    "        pipeline = [(\"vect\", CountVectorizer(**vec_kwargs))]\n",
    "        if use_tfidf:\n",
    "            pipeline.append((\"tfidf\", TfidfTransformer(**tfidf_kwargs)))\n",
    "        pipeline.append((\"clf\", clf))\n",
    "        self.model = Pipeline(pipeline)\n",
    "        return\n",
    "\n",
    "    @classmethod\n",
    "    def load_texts_from_folder(\n",
    "        cls, folder_path, subfolders=None, shuffle=True, encoding=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ```\n",
    "        load text files from folder\n",
    "\n",
    "        Args:\n",
    "          folder_path(str): path to folder containing documents\n",
    "                            The supplied folder should contain a subfolder\n",
    "                            for each category, which will be used as the class label\n",
    "          subfolders(list): list of subfolders under folder_path to consider\n",
    "                            Example: If folder_path contains subfolders pos, neg, and\n",
    "                            unlabeled, then unlabeled folder can be ignored by\n",
    "                            setting subfolders=['pos', 'neg']\n",
    "          shuffle(bool):  If True, list of texts will be shuffled\n",
    "          encoding(str): encoding to use.  default:None (auto-detected)\n",
    "        Returns:\n",
    "          tuple: (texts, labels, label_names)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        bunch = load_files(folder_path, categories=subfolders, shuffle=shuffle)\n",
    "        texts = bunch.data\n",
    "        labels = bunch.target\n",
    "        label_names = bunch.target_names\n",
    "        # print('target names:')\n",
    "        # for idx, label_name in enumerate(bunch.target_names):\n",
    "        # print('\\t%s:%s' % (idx, label_name))\n",
    "\n",
    "        # decode based on supplied encoding\n",
    "        if encoding is None:\n",
    "            encoding = U.detect_encoding(texts)\n",
    "            if encoding != \"utf-8\":\n",
    "                print(\"detected encoding: %s\" % (encoding))\n",
    "\n",
    "        try:\n",
    "            texts = [text.decode(encoding) for text in texts]\n",
    "        except:\n",
    "            print(\n",
    "                \"Decoding with %s failed 1st attempt - using %s with skips\"\n",
    "                % (encoding, encoding)\n",
    "            )\n",
    "            texts = U.decode_by_line(texts, encoding=encoding)\n",
    "        return (texts, labels, label_names)\n",
    "\n",
    "    @classmethod\n",
    "    def load_texts_from_csv(\n",
    "        cls,\n",
    "        csv_filepath,\n",
    "        text_column=\"text\",\n",
    "        label_column=\"label\",\n",
    "        sep=\",\",\n",
    "        encoding=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ```\n",
    "        load text files from csv file\n",
    "        CSV should have at least two columns.\n",
    "        Example:\n",
    "        Text               | Label\n",
    "        I love this movie. | positive\n",
    "        I hated this movie.| negative\n",
    "\n",
    "\n",
    "        Args:\n",
    "          csv_filepath(str): path to CSV file\n",
    "          text_column(str): name of column containing the texts. default:'text'\n",
    "          label_column(str): name of column containing the labels in string format\n",
    "                             default:'label'\n",
    "          sep(str): character that separates columns in CSV. default:','\n",
    "          encoding(str): encoding to use. default:None (auto-detected)\n",
    "        Returns:\n",
    "          tuple: (texts, labels, label_names)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        if encoding is None:\n",
    "            with open(csv_filepath, \"rb\") as f:\n",
    "                encoding = U.detect_encoding([f.read()])\n",
    "                if encoding != \"utf-8\":\n",
    "                    print(\"detected encoding: %s (if wrong, set manually)\" % (encoding))\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.read_csv(csv_filepath, encoding=encoding, sep=sep)\n",
    "        texts = df[text_column].fillna(\"fillna\").values\n",
    "        labels = df[label_column].values\n",
    "        le = LabelEncoder()\n",
    "        le.fit(labels)\n",
    "        labels = le.transform(labels)\n",
    "        return (texts, labels, le.classes_)\n",
    "\n",
    "    def fit(self, x_train, y_train, ctype=\"logreg\", **kwargs):\n",
    "        \"\"\"\n",
    "        Train a text  classifier. Extra kwargs fed diretly to `self.model.fit`.\n",
    "        \n",
    "        **Args:**\n",
    "          - x_train(list or np.ndarray):  training texts\n",
    "          - y_train(np.ndarray):  training labels\n",
    "          - ctype(str):  One of {'logreg', 'nbsvm', 'sgdclassifier'}.  default:nbsvm\n",
    "        \"\"\"\n",
    "        lang = U.detect_lang(x_train)\n",
    "        if U.is_chinese(lang):\n",
    "            x_train = U.split_chinese(x_train)\n",
    "        if self.model is None:\n",
    "            self.create_model(ctype, x_train)\n",
    "        self.model.fit(x_train, y_train, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_test, return_proba=False):\n",
    "        \"\"\"\n",
    "        ```\n",
    "        make predictions on text data\n",
    "        Args:\n",
    "          x_test(list or np.ndarray or str): array of texts on which to make predictions or a string representing text\n",
    "        ```\n",
    "        \"\"\"\n",
    "        if return_proba and not hasattr(self.model[\"clf\"], \"predict_proba\"):\n",
    "            raise ValueError(\n",
    "                \"%s does not support predict_proba\" % (type(self.model[\"clf\"]).__name__)\n",
    "            )\n",
    "        if isinstance(x_test, str):\n",
    "            x_test = [x_test]\n",
    "        lang = U.detect_lang(x_test)\n",
    "        if U.is_chinese(lang):\n",
    "            x_test = U.split_chinese(x_test)\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"model is None - call fit or load to set the model\")\n",
    "        if return_proba:\n",
    "            predicted = self.model.predict_proba(x_test)\n",
    "        else:\n",
    "            predicted = self.model.predict(x_test)\n",
    "        if len(predicted) == 1:\n",
    "            predicted = predicted[0]\n",
    "        return predicted\n",
    "\n",
    "    def predict_proba(self, x_test):\n",
    "        \"\"\"\n",
    "        predict_proba\n",
    "        \"\"\"\n",
    "        return self.predict(x_test, return_proba=True)\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        \"\"\"\n",
    "        ```\n",
    "        evaluate\n",
    "        Args:\n",
    "          x_test(list or np.ndarray):  training texts\n",
    "          y_test(np.ndarray):  training labels\n",
    "        ```\n",
    "        \"\"\"\n",
    "        predicted = self.predict(x_test)\n",
    "        return np.mean(predicted == y_test)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        save model\n",
    "        \"\"\"\n",
    "        dump(self.model, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"\n",
    "        load model\n",
    "        \"\"\"\n",
    "        self.model = load(filename)\n",
    "\n",
    "    def grid_search(self, params, x_train, y_train, n_jobs=-1):\n",
    "        \"\"\"\n",
    "        ```\n",
    "        Performs grid search to find optimal set of hyperparameters\n",
    "        Args:\n",
    "          params (dict):  A dictionary defining the space of the search.\n",
    "                          Example for finding optimal value of alpha in NBSVM:\n",
    "                        parameters = {\n",
    "                                      #'clf__C': (1e0, 1e-1, 1e-2),\n",
    "                                      'clf__alpha': (0.1, 0.2, 0.4, 0.5, 0.75, 0.9, 1.0),\n",
    "                                      #'clf__fit_intercept': (True, False),\n",
    "                                      #'clf__beta' : (0.1, 0.25, 0.5, 0.9)\n",
    "                                      }\n",
    "          n_jobs(int): number of jobs to run in parallel.  default:-1 (use all processors)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        gs_clf = GridSearchCV(self.model, params, n_jobs=n_jobs)\n",
    "        gs_clf = gs_clf.fit(x_train, y_train)\n",
    "        # gs_clf.best_score_\n",
    "        for param_name in sorted(params.keys()):\n",
    "            print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "        return\n",
    "\n",
    "\n",
    "class NBSVM(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):\n",
    "    def __init__(self, alpha=0.75, C=0.01, beta=0.25, fit_intercept=False):\n",
    "        self.alpha = alpha\n",
    "        self.C = C\n",
    "        self.beta = beta\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        if len(self.classes_) == 2:\n",
    "            coef_, intercept_ = self._fit_binary(X, y)\n",
    "            self.coef_ = coef_\n",
    "            self.intercept_ = intercept_\n",
    "        else:\n",
    "            coef_, intercept_ = zip(\n",
    "                *[self._fit_binary(X, y == class_) for class_ in self.classes_]\n",
    "            )\n",
    "            self.coef_ = np.concatenate(coef_)\n",
    "            self.intercept_ = np.array(intercept_).flatten()\n",
    "        return self\n",
    "\n",
    "    def _fit_binary(self, X, y):\n",
    "        p = np.asarray(self.alpha + X[y == 1].sum(axis=0)).flatten()\n",
    "        q = np.asarray(self.alpha + X[y == 0].sum(axis=0)).flatten()\n",
    "        r = np.log(p / np.abs(p).sum()) - np.log(q / np.abs(q).sum())\n",
    "        b = np.log((y == 1).sum()) - np.log((y == 0).sum())\n",
    "\n",
    "        if isinstance(X, spmatrix):\n",
    "            indices = np.arange(len(r))\n",
    "            r_sparse = coo_matrix((r, (indices, indices)), shape=(len(r), len(r)))\n",
    "            X_scaled = X * r_sparse\n",
    "        else:\n",
    "            X_scaled = X * r\n",
    "\n",
    "        lsvc = LinearSVC(\n",
    "            C=self.C, fit_intercept=self.fit_intercept, max_iter=10000, dual=True\n",
    "        ).fit(X_scaled, y)\n",
    "\n",
    "        mean_mag = np.abs(lsvc.coef_).mean()\n",
    "\n",
    "        coef_ = (1 - self.beta) * mean_mag * r + self.beta * (r * lsvc.coef_)\n",
    "\n",
    "        intercept_ = (1 - self.beta) * mean_mag * b + self.beta * lsvc.intercept_\n",
    "\n",
    "        return coef_, intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
