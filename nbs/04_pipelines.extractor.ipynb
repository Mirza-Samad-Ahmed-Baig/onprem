{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelines.extractor\n",
    "\n",
    "> Pipline for information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines.extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union, Callable\n",
    "import pandas as pd\n",
    "from onprem.utils import segment\n",
    "\n",
    "from onprem.ingest import load_single_document\n",
    "\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm, # An `onprem.LLM` object\n",
    "        prompt_template: Optional[str] = None, # A model specific prompt_template with a single placeholder named \"{prompt}\". If supplied, overrides the `prompt_template` supplied to the `LLM` constructor.\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `Extractor` applies a given prompt to each sentence or paragraph in a document and returns the results.\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.prompt_template = prompt_template if prompt_template is not None else llm.prompt_template\n",
    "\n",
    "\n",
    "\n",
    "    def apply(self,\n",
    "              ex_prompt_template:str, # A prompt to apply to each `unit` in document. Should have a single variable, `{text}`\n",
    "              fpath: Optional[str] = None, # A path to to a single file of interest (e.g., a PDF or MS Word document). Mutually-exclusive with `content`.\n",
    "              content: Optional[str] = None, # Text content of a document of interest.  Mutually-exclusive with `fpath`.\n",
    "              unit:str='paragraph', # One of {'sentence', 'paragraph'}. \n",
    "              preproc_fn: Optional[Callable] = None, # Function should accept a text string and returns a new preprocessed input.\n",
    "              filter_fn: Optional[Callable] = None, # A function that accepts a sentence or paragraph and returns `True` if prompt should be applied to it.\n",
    "              clean_fn: Optional[Callable] = None, # A function that accepts a sentence or paragraph and returns \"cleaned\" version of the text. (applied after `filter_fn`)\n",
    "              pdf_pages:List[int]=[], # If `fpath` is a PDF document, only apply prompt to text on page numbers listed in `pdf_pages` (starts at 1).\n",
    "              maxchars = 2048, # units (i.e., paragraphs or sentences) larger than `maxchars` split.\n",
    "              stop:list=[], # list of characters to trigger the LLM to stop generating.\n",
    "              pdf_use_unstructured:bool=False, # If True, use unstructured package to extract text from PDF.\n",
    "              **kwargs, # N/A\n",
    "             ):\n",
    "        \"\"\"\n",
    "        Apply the prompt to each `unit` (where a \"unit\" is either a paragraph or sentence) optionally filtered by `filter_fn`.\n",
    "        Results are stored in a `pandas.Dataframe`.\n",
    "        \"\"\"\n",
    "        if not(bool(fpath) != bool(content)):\n",
    "            raise ValueError('Either fpath argument or content argument must be supplied but not both.')\n",
    "        if pdf_pages and pdf_use_unstructured:\n",
    "            raise ValueError('The parameters pdf_pages and pdf_use_unstructured are mutually exclusive.')\n",
    "            \n",
    "        # setup extraction prompt\n",
    "        extraction_prompt = ex_prompt_template if self.prompt_template is None else self.prompt_template.format(**{'prompt': ex_prompt_template})   \n",
    "\n",
    "        # extract text\n",
    "        if not content:\n",
    "            if not os.path.isfile(fpath):\n",
    "                raise ValueError(f'{fpath} is not a file')\n",
    "            docs = load_single_document(fpath, pdf_use_unstructured=pdf_use_unstructured, **kwargs)\n",
    "            ext = \".\" + fpath.rsplit(\".\", 1)[-1].lower()\n",
    "            if ext == '.pdf' and pdf_pages:\n",
    "                docs = [doc for i,doc in enumerate(docs) if i+1 in pdf_pages]\n",
    "            content = '\\n\\n'.join([preproc_fn(doc.page_content) if preproc_fn else doc.page_content for doc in docs])\n",
    "        \n",
    "        # segment\n",
    "        chunks = segment(content, maxchars=maxchars, unit=unit)\n",
    "        extractions = []\n",
    "        texts = []\n",
    "        for chunk in chunks:\n",
    "            if filter_fn and not filter_fn(chunk): continue\n",
    "            if clean_fn: chunk = clean_fn(chunk)\n",
    "            prompt = extraction_prompt.format(text=chunk)\n",
    "            extractions.append(self.llm.prompt(prompt, stop=stop))\n",
    "            texts.append(chunk)\n",
    "        df = pd.DataFrame({'Extractions':extractions, 'Texts':texts})\n",
    "        return df\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/extractor.py#L32){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Extractor.apply\n",
       "\n",
       ">      Extractor.apply (ex_prompt_template:str, fpath:Optional[str]=None,\n",
       ">                       content:Optional[str]=None, unit:str='paragraph',\n",
       ">                       preproc_fn:Optional[Callable]=None,\n",
       ">                       filter_fn:Optional[Callable]=None,\n",
       ">                       clean_fn:Optional[Callable]=None,\n",
       ">                       pdf_pages:List[int]=[], maxchars=2048, stop:list=[],\n",
       ">                       pdf_use_unstructured:bool=False, **kwargs)\n",
       "\n",
       "*Apply the prompt to each `unit` (where a \"unit\" is either a paragraph or sentence) optionally filtered by `filter_fn`.\n",
       "Results are stored in a `pandas.Dataframe`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| ex_prompt_template | str |  | A prompt to apply to each `unit` in document. Should have a single variable, `{text}` |\n",
       "| fpath | Optional | None | A path to to a single file of interest (e.g., a PDF or MS Word document). Mutually-exclusive with `content`. |\n",
       "| content | Optional | None | Text content of a document of interest.  Mutually-exclusive with `fpath`. |\n",
       "| unit | str | paragraph | One of {'sentence', 'paragraph'}. |\n",
       "| preproc_fn | Optional | None | Function should accept a text string and returns a new preprocessed input. |\n",
       "| filter_fn | Optional | None | A function that accepts a sentence or paragraph and returns `True` if prompt should be applied to it. |\n",
       "| clean_fn | Optional | None | A function that accepts a sentence or paragraph and returns \"cleaned\" version of the text. (applied after `filter_fn`) |\n",
       "| pdf_pages | List | [] | If `fpath` is a PDF document, only apply prompt to text on page numbers listed in `pdf_pages` (starts at 1). |\n",
       "| maxchars | int | 2048 | units (i.e., paragraphs or sentences) larger than `maxchars` split. |\n",
       "| stop | list | [] | list of characters to trigger the LLM to stop generating. |\n",
       "| pdf_use_unstructured | bool | False | If True, use unstructured package to extract text from PDF. |\n",
       "| kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/extractor.py#L32){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Extractor.apply\n",
       "\n",
       ">      Extractor.apply (ex_prompt_template:str, fpath:Optional[str]=None,\n",
       ">                       content:Optional[str]=None, unit:str='paragraph',\n",
       ">                       preproc_fn:Optional[Callable]=None,\n",
       ">                       filter_fn:Optional[Callable]=None,\n",
       ">                       clean_fn:Optional[Callable]=None,\n",
       ">                       pdf_pages:List[int]=[], maxchars=2048, stop:list=[],\n",
       ">                       pdf_use_unstructured:bool=False, **kwargs)\n",
       "\n",
       "*Apply the prompt to each `unit` (where a \"unit\" is either a paragraph or sentence) optionally filtered by `filter_fn`.\n",
       "Results are stored in a `pandas.Dataframe`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| ex_prompt_template | str |  | A prompt to apply to each `unit` in document. Should have a single variable, `{text}` |\n",
       "| fpath | Optional | None | A path to to a single file of interest (e.g., a PDF or MS Word document). Mutually-exclusive with `content`. |\n",
       "| content | Optional | None | Text content of a document of interest.  Mutually-exclusive with `fpath`. |\n",
       "| unit | str | paragraph | One of {'sentence', 'paragraph'}. |\n",
       "| preproc_fn | Optional | None | Function should accept a text string and returns a new preprocessed input. |\n",
       "| filter_fn | Optional | None | A function that accepts a sentence or paragraph and returns `True` if prompt should be applied to it. |\n",
       "| clean_fn | Optional | None | A function that accepts a sentence or paragraph and returns \"cleaned\" version of the text. (applied after `filter_fn`) |\n",
       "| pdf_pages | List | [] | If `fpath` is a PDF document, only apply prompt to text on page numbers listed in `pdf_pages` (starts at 1). |\n",
       "| maxchars | int | 2048 | units (i.e., paragraphs or sentences) larger than `maxchars` split. |\n",
       "| stop | list | [] | list of characters to trigger the LLM to stop generating. |\n",
       "| pdf_use_unstructured | bool | False | If True, use unstructured package to extract text from PDF. |\n",
       "| kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Extractor.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem import LLM\n",
    "from onprem.pipelines import Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaiya/mambaforge/envs/llm/lib/python3.9/site-packages/langchain_core/language_models/llms.py:239: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt_template = \"<s>[INST] {prompt} [/INST]\" # prompt template for Mistral\n",
    "llm = LLM(model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf', \n",
    "          n_gpu_layers=33,  # change based on your system\n",
    "          verbose=False, mute_stream=True, \n",
    "          prompt_template=prompt_template)\n",
    "extractor = Extractor(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Extract citations from the following sentences. Return #NA# if there are no citations in the text. Here are some examples:\n",
    "\n",
    "[SENTENCE]:pretrained BERT text classifier (Devlin et al., 2018), models for sequence tagging (Lample et al., 2016)\n",
    "[CITATIONS]:(Devlin et al., 2018), (Lample et al., 2016)\n",
    "[SENTENCE]:Machine learning (ML) is a powerful tool.\n",
    "[CITATIONS]:#NA#\n",
    "[SENTENCE]:Following inspiration from a blog post by Rachel Thomas of fast.ai (Howard and Gugger, 2020), we refer to this as Augmented Machine Learning or AugML\n",
    "[CITATIONS]:(Howard and Gugger, 2020)\n",
    "[SENTENCE]:{text}\n",
    "[CITATIONS]:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "content = \"\"\"\n",
    "For instance, the fit_onecycle method employs a 1cycle policy (Smith, 2018). \n",
    "\"\"\"\n",
    "df = extractor.apply(prompt, content=content, stop=['\\n'])\n",
    "assert df['Extractions'][0].strip().startswith('(Smith, 2018)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "content =\"\"\"In the case of text, this may involve language-specific preprocessing (e.g., tokenization).\"\"\"\n",
    "df = extractor.apply(prompt, content=content, stop=['\\n'])\n",
    "assert df['Extractions'][0].strip().startswith('#NA#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
