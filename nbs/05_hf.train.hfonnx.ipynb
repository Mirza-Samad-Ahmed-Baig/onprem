{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hf.train.hfonnx\n",
    "\n",
    "> Hugging Face Transformers ONNX export module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp hf.train.hfonnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Hugging Face Transformers ONNX export module\n",
    "\"\"\"\n",
    "\n",
    "from collections import OrderedDict\n",
    "from io import BytesIO\n",
    "from itertools import chain\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "# Conditional import\n",
    "try:\n",
    "    from onnxruntime.quantization import quantize_dynamic\n",
    "\n",
    "    ONNX_RUNTIME = True\n",
    "except ImportError:\n",
    "    ONNX_RUNTIME = False\n",
    "\n",
    "from torch import nn\n",
    "from torch.onnx import export\n",
    "\n",
    "from transformers import AutoModel, AutoModelForQuestionAnswering, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from onprem.hf.models.pooling.factory import PoolingFactory\n",
    "from onprem.hf.tensors import Tensors\n",
    "\n",
    "\n",
    "class HFOnnx(Tensors):\n",
    "    \"\"\"\n",
    "    Exports a Hugging Face Transformer model to ONNX.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, path, task=\"default\", output=None, quantize=False, opset=14):\n",
    "        \"\"\"\n",
    "        Exports a Hugging Face Transformer model to ONNX.\n",
    "\n",
    "        Args:\n",
    "            path: path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple\n",
    "            task: optional model task or category, determines the model type and outputs, defaults to export hidden state\n",
    "            output: optional output model path, defaults to return byte array if None\n",
    "            quantize: if model should be quantized (requires onnx to be installed), defaults to False\n",
    "            opset: onnx opset, defaults to 14\n",
    "\n",
    "        Returns:\n",
    "            path to model output or model as bytes depending on output parameter\n",
    "        \"\"\"\n",
    "\n",
    "        inputs, outputs, model = self.parameters(task)\n",
    "\n",
    "        if isinstance(path, (list, tuple)):\n",
    "            model, tokenizer = path\n",
    "            model = model.cpu()\n",
    "        else:\n",
    "            model = model(path)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "        # Generate dummy inputs\n",
    "        dummy = dict(tokenizer([\"test inputs\"], return_tensors=\"pt\"))\n",
    "\n",
    "        # Default to BytesIO if no output file provided\n",
    "        output = output if output else BytesIO()\n",
    "\n",
    "        # Export model to ONNX\n",
    "        export(\n",
    "            model,\n",
    "            (dummy,),\n",
    "            output,\n",
    "            opset_version=opset,\n",
    "            do_constant_folding=True,\n",
    "            input_names=list(inputs.keys()),\n",
    "            output_names=list(outputs.keys()),\n",
    "            dynamic_axes=dict(chain(inputs.items(), outputs.items())),\n",
    "        )\n",
    "\n",
    "        # Quantize model\n",
    "        if quantize:\n",
    "            if not ONNX_RUNTIME:\n",
    "                raise ImportError('onnxruntime is not available: pip install onnxruntime')\n",
    "\n",
    "            output = self.quantization(output)\n",
    "\n",
    "        if isinstance(output, BytesIO):\n",
    "            # Reset stream and return bytes\n",
    "            output.seek(0)\n",
    "            output = output.read()\n",
    "\n",
    "        return output\n",
    "\n",
    "    def quantization(self, output):\n",
    "        \"\"\"\n",
    "        Quantizes an ONNX model.\n",
    "\n",
    "        Args:\n",
    "            output: path to ONNX model or BytesIO with model data\n",
    "\n",
    "        Returns:\n",
    "            quantized model as file path or bytes\n",
    "        \"\"\"\n",
    "\n",
    "        temp = None\n",
    "        if isinstance(output, BytesIO):\n",
    "            with NamedTemporaryFile(suffix=\".quant\", delete=False) as tmpfile:\n",
    "                temp = tmpfile.name\n",
    "\n",
    "            with open(temp, \"wb\") as f:\n",
    "                f.write(output.getbuffer())\n",
    "\n",
    "            output = temp\n",
    "\n",
    "        # Quantize model\n",
    "        quantize_dynamic(output, output, extra_options={\"MatMulConstBOnly\": False})\n",
    "\n",
    "        # Read file back to bytes if temp file was created\n",
    "        if temp:\n",
    "            with open(temp, \"rb\") as f:\n",
    "                output = f.read()\n",
    "\n",
    "        return output\n",
    "\n",
    "    def parameters(self, task):\n",
    "        \"\"\"\n",
    "        Defines inputs and outputs for an ONNX model.\n",
    "\n",
    "        Args:\n",
    "            task: task name used to lookup model configuration\n",
    "\n",
    "        Returns:\n",
    "            (inputs, outputs, model function)\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = OrderedDict(\n",
    "            [\n",
    "                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n",
    "                (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n",
    "                (\"token_type_ids\", {0: \"batch\", 1: \"sequence\"}),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        config = {\n",
    "            \"default\": (OrderedDict({\"last_hidden_state\": {0: \"batch\", 1: \"sequence\"}}), AutoModel.from_pretrained),\n",
    "            \"pooling\": (OrderedDict({\"embeddings\": {0: \"batch\", 1: \"sequence\"}}), lambda x: PoolingOnnx(x, -1)),\n",
    "            \"question-answering\": (\n",
    "                OrderedDict(\n",
    "                    {\n",
    "                        \"start_logits\": {0: \"batch\", 1: \"sequence\"},\n",
    "                        \"end_logits\": {0: \"batch\", 1: \"sequence\"},\n",
    "                    }\n",
    "                ),\n",
    "                AutoModelForQuestionAnswering.from_pretrained,\n",
    "            ),\n",
    "            \"text-classification\": (OrderedDict({\"logits\": {0: \"batch\"}}), AutoModelForSequenceClassification.from_pretrained),\n",
    "        }\n",
    "\n",
    "        # Aliases\n",
    "        config[\"zero-shot-classification\"] = config[\"text-classification\"]\n",
    "\n",
    "        return (inputs,) + config[task]\n",
    "\n",
    "\n",
    "class PoolingOnnx(nn.Module):\n",
    "    \"\"\"\n",
    "    Extends Pooling methods to name inputs to model, which is required to export to ONNX.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, device):\n",
    "        \"\"\"\n",
    "        Creates a new PoolingOnnx instance.\n",
    "\n",
    "        Args:\n",
    "            path: path to model, accepts Hugging Face model hub id or local path\n",
    "            device: tensor device id\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Create pooling method based on configuration\n",
    "        self.model = PoolingFactory.create({\"path\": path, \"device\": device})\n",
    "\n",
    "    # pylint: disable=W0221\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
    "        \"\"\"\n",
    "        Runs inputs through pooling model and returns outputs.\n",
    "\n",
    "        Args:\n",
    "            inputs: model inputs\n",
    "\n",
    "        Returns:\n",
    "            model outputs\n",
    "        \"\"\"\n",
    "\n",
    "        # Build list of arguments dynamically since some models take token_type_ids\n",
    "        # and others don't\n",
    "        inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "        if token_type_ids is not None:\n",
    "            inputs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "        return self.model.forward(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
