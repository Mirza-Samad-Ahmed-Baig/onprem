{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.stores.dense\n",
    "\n",
    "> vector database for question-answering and other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.stores.dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "from typing import List, Optional, Callable, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from onprem.utils import get_datadir, DEFAULT_DB\n",
    "from onprem.ingest.base import batchify_chunks, process_folder, does_vectorstore_exist, VectorStore\n",
    "from onprem.ingest.base import CHROMA_MAX\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    CHROMA_INSTALLED = True\n",
    "except ImportError:\n",
    "    CHROMA_INSTALLED = False\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "COLLECTION_NAME = \"onprem_chroma\"\n",
    "\n",
    "\n",
    "class DenseStore(VectorStore):\n",
    "    def __init__(\n",
    "        self,\n",
    "        persist_directory: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_folder` (previously-ingested documents are ignored)\n",
    "\n",
    "        **Args**:\n",
    "\n",
    "          - *persist_directory*: Path to vector database (created if it doesn't exist).\n",
    "                                 Default is `onprem_data/vectordb` in user's home directory.\n",
    "          - *embedding_model*: name of sentence-transformers model\n",
    "          - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.\n",
    "          - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                       embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "\n",
    "\n",
    "        **Returns**: `None`\n",
    "        \"\"\"\n",
    "        if not CHROMA_INSTALLED:\n",
    "            raise ImportError('Please install chromadb: pip install chromadb langchain_chroma')\n",
    "\n",
    "        from langchain_chroma import Chroma\n",
    "        import chromadb\n",
    "        from chromadb.config import Settings\n",
    "\n",
    "        self.persist_directory = persist_directory or os.path.join(\n",
    "            get_datadir(), DEFAULT_DB\n",
    "        )\n",
    "        self.init_embedding_model(**kwargs) # stored in self.embeddings\n",
    "\n",
    "        self.chroma_settings = Settings(\n",
    "            persist_directory=self.persist_directory, anonymized_telemetry=False\n",
    "        )\n",
    "        self.chroma_client = chromadb.PersistentClient(\n",
    "            settings=self.chroma_settings, path=self.persist_directory\n",
    "        )\n",
    "        return\n",
    "\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Returns an instance to the `langchain_chroma.Chroma` instance\n",
    "        \"\"\"\n",
    "        db = Chroma(\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings,\n",
    "            client_settings=self.chroma_settings,\n",
    "            client=self.chroma_client,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            collection_name=COLLECTION_NAME,\n",
    "        )\n",
    "        return db if does_vectorstore_exist(db) else None\n",
    "\n",
    "\n",
    "    def exists(self):\n",
    "        return self.get_db() is not None\n",
    "\n",
    "\n",
    "    def add_documents(self, documents, batch_size:int=CHROMA_MAX):\n",
    "        \"\"\"\n",
    "        Stores instances of `langchain_core.documents.base.Document` in vectordb\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return\n",
    "        db = self.get_db()\n",
    "        if db:\n",
    "            print(\"Creating embeddings. May take some minutes...\")\n",
    "            chunk_batches, total_chunks = batchify_chunks(documents, batch_size=batch_size)\n",
    "            for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                db.add_documents(lst)\n",
    "        else:\n",
    "            chunk_batches, total_chunks = batchify_chunks(documents, batch_size)\n",
    "            print(\"Creating embeddings. May take some minutes...\")\n",
    "            db = None\n",
    "\n",
    "            for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                if not db:\n",
    "                    db = Chroma.from_documents(\n",
    "                        lst,\n",
    "                        self.embeddings,\n",
    "                        persist_directory=self.persist_directory,\n",
    "                        client_settings=self.chroma_settings,\n",
    "                        client=self.chroma_client,\n",
    "                        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                        collection_name=COLLECTION_NAME,\n",
    "                    )\n",
    "                else:\n",
    "                    db.add_documents(lst)\n",
    "        return\n",
    "\n",
    "\n",
    "    def remove_document(self, id_to_delete):\n",
    "        \"\"\"\n",
    "        Remove a single document with ID, `id_to_delete`.\n",
    "        \"\"\"\n",
    "        if not self.exists(): return\n",
    "        self.get_db().delete(ids=[id_to_delete])\n",
    "        return\n",
    "\n",
    "    def _convert_to_dict(self, raw_results):\n",
    "        \"\"\"\n",
    "        Convert raw results to dictionary\n",
    "        \"\"\"\n",
    "        ids = raw_results['ids']\n",
    "        texts = raw_results['documents']\n",
    "        metadatas = raw_results['metadatas']\n",
    "        results = []\n",
    "        for i, m in enumerate(metadatas):\n",
    "            m['page_content'] = texts[i]\n",
    "            m['id'] = ids[i]\n",
    "            results.append(m)\n",
    "        return results\n",
    "\n",
    "    \n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns all docs\n",
    "        \"\"\"\n",
    "        if not self.exists(): return []\n",
    "\n",
    "        raw_results =  self.get_db().get()\n",
    "        return self._convert_to_dict(raw_results)\n",
    "\n",
    "\n",
    "    def get_doc(self, id):\n",
    "        \"\"\"\n",
    "        Retrieve a record by ID\n",
    "        \"\"\"\n",
    "        if not self.exists(): return None\n",
    "        raw_results = self.get_db().get(ids=[id])\n",
    "        return self._convert_to_dict(raw_results)[0] if len(raw_results['ids']) > 0 else None\n",
    "\n",
    "    \n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        Get total number of records\n",
    "        \"\"\"\n",
    "        if not self.exists(): return 0\n",
    "        return len(self.get_db().get()['documents'])\n",
    "\n",
    "    \n",
    "    def erase(self, confirm=True):\n",
    "        \"\"\"\n",
    "        Resets collection and removes and stored documents\n",
    "        \"\"\"\n",
    "        if not self.exists(): return True\n",
    "        shall = True\n",
    "        if confirm:\n",
    "            msg = (\n",
    "                f\"You are about to remove all documents from the vector database.\"\n",
    "                + f\"(Original documents on file system will remain.) Are you sure?\"\n",
    "            )\n",
    "            shall = input(\"%s (Y/n) \" % msg) == \"Y\"\n",
    "        if shall:\n",
    "            self.get_db().reset_collection()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def query(self,\n",
    "              query:str, # query string\n",
    "              k:int = 4, # max number of results to return\n",
    "              filters:Optional[Dict[str, str]] = None, # filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True})\n",
    "              where_document:Optional[Dict[str, str]] = None, # filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
    "              **kwargs):\n",
    "        \"\"\"\n",
    "        Perform a semantic search of the vector DB\n",
    "        \"\"\"\n",
    "        if not self.exists(): return []\n",
    "        db = self.get_db()\n",
    "        results = db.similarity_search_with_score(query, \n",
    "                                                  filter=filters,\n",
    "                                                  where_document=where_document,\n",
    "                                                  k = k, **kwargs)\n",
    "        if not results: return []\n",
    "        docs, scores = zip(*results)\n",
    "        for doc, score in zip(docs, scores):\n",
    "            simscore = 1 - score\n",
    "            doc.metadata[\"score\"] = 1-score\n",
    "        return docs      \n",
    "\n",
    "    def semantic_search(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Semantic search is equivalent to queries in this class\n",
    "        \"\"\"\n",
    "        return self.query(*args, **kwargs)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L79){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.add_documents\n",
       "\n",
       ">      DenseStore.add_documents (documents, batch_size:int=41000)\n",
       "\n",
       "*Stores instances of `langchain_core.documents.base.Document` in vectordb*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L79){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.add_documents\n",
       "\n",
       ">      DenseStore.add_documents (documents, batch_size:int=41000)\n",
       "\n",
       "*Stores instances of `langchain_core.documents.base.Document` in vectordb*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.add_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L135){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_all_docs\n",
       "\n",
       ">      DenseStore.get_all_docs ()\n",
       "\n",
       "*Returns all docs*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L135){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_all_docs\n",
       "\n",
       ">      DenseStore.get_all_docs ()\n",
       "\n",
       "*Returns all docs*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.get_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L154){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_size\n",
       "\n",
       ">      DenseStore.get_size ()\n",
       "\n",
       "*Get total number of records*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L154){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_size\n",
       "\n",
       ">      DenseStore.get_size ()\n",
       "\n",
       "*Get total number of records*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.get_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L162){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.erase\n",
       "\n",
       ">      DenseStore.erase (confirm=True)\n",
       "\n",
       "*Resets collection and removes and stored documents*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L162){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.erase\n",
       "\n",
       ">      DenseStore.erase (confirm=True)\n",
       "\n",
       "*Resets collection and removes and stored documents*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.erase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L180){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.query\n",
       "\n",
       ">      DenseStore.query (query:str, k:int=4,\n",
       ">                        filters:Optional[Dict[str,str]]=None,\n",
       ">                        where_document:Optional[Dict[str,str]]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | query string |\n",
       "| k | int | 4 | max number of results to return |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"}) |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L180){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.query\n",
       "\n",
       ">      DenseStore.query (query:str, k:int=4,\n",
       ">                        filters:Optional[Dict[str,str]]=None,\n",
       ">                        where_document:Optional[Dict[str,str]]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | query string |\n",
       "| k | int | 4 | max number of results to return |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"}) |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.get_embedding_model\n",
       "\n",
       ">      VectorStore.get_embedding_model ()\n",
       "\n",
       "*Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.get_embedding_model\n",
       "\n",
       ">      VectorStore.get_embedding_model ()\n",
       "\n",
       "*Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.get_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L60){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_db\n",
       "\n",
       ">      DenseStore.get_db ()\n",
       "\n",
       "*Returns an instance to the `langchain_chroma.Chroma` instance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L60){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_db\n",
       "\n",
       ">      DenseStore.get_db ()\n",
       "\n",
       "*Returns an instance to the `langchain_chroma.Chroma` instance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.get_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mDenseStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Helper class that provides a standard way to create an ABC using\n",
       "inheritance.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Ingests all documents in `source_folder` (previously-ingested documents are ignored)\n",
       "\n",
       "**Args**:\n",
       "\n",
       "  - *persist_directory*: Path to vector database (created if it doesn't exist).\n",
       "                         Default is `onprem_data/vectordb` in user's home directory.\n",
       "  - *embedding_model*: name of sentence-transformers model\n",
       "  - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.\n",
       "  - *embedding_encode_kwargs*: arguments to encode method of\n",
       "                               embedding model (e.g., `{'normalize_embeddings': False}`).\n",
       "\n",
       "\n",
       "**Returns**: `None`\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DenseStore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "import tempfile\n",
    "\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "tempfolder = temp_dir.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /tmp/tmpmrschkmy\n",
      "Loading documents from tests/sample_data/ktrain_paper/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Processing and chunking 6 new documents: 100%|███████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 814.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 41 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "store = DenseStore(persist_directory=tempfolder)\n",
    "store.ingest(\"tests/sample_data/ktrain_paper/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "store.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "a_document = store.get_all_docs()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "store.remove_document(a_document['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "store.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
