{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.stores.dense\n",
    "\n",
    "> vector database for question-answering and other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.stores.dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "from typing import List, Optional, Callable, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from onprem.utils import get_datadir, DEFAULT_DB\n",
    "from onprem.ingest.base import batchify_chunks, process_folder, does_vectorstore_exist, VectorStore\n",
    "from onprem.ingest.base import DEFAULT_CHUNK_SIZE, DEFAULT_CHUNK_OVERLAP, TABLE_CHUNK_SIZE, CHROMA_MAX\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "COLLECTION_NAME = \"onprem_chroma\"\n",
    "CHROMA_MAX = 41000\n",
    "\n",
    "\n",
    "\n",
    "class DenseStore(VectorStore):\n",
    "    def __init__(\n",
    "        self,\n",
    "        persist_directory: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_folder` (previously-ingested documents are ignored)\n",
    "\n",
    "        **Args**:\n",
    "\n",
    "          - *persist_directory*: Path to vector database (created if it doesn't exist).\n",
    "                                 Default is `onprem_data/vectordb` in user's home directory.\n",
    "          - *embedding_model*: name of sentence-transformers model\n",
    "          - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.\n",
    "          - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                       embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "\n",
    "\n",
    "        **Returns**: `None`\n",
    "        \"\"\"\n",
    "        self.persist_directory = persist_directory or os.path.join(\n",
    "            get_datadir(), DEFAULT_DB\n",
    "        )\n",
    "        self.init_embedding_model(**kwargs) # stored in self.embeddings\n",
    "\n",
    "        self.chroma_settings = Settings(\n",
    "            persist_directory=self.persist_directory, anonymized_telemetry=False\n",
    "        )\n",
    "        self.chroma_client = chromadb.PersistentClient(\n",
    "            settings=self.chroma_settings, path=self.persist_directory\n",
    "        )\n",
    "        return\n",
    "\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Returns an instance to the `langchain_chroma.Chroma` instance\n",
    "        \"\"\"\n",
    "        db = Chroma(\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings,\n",
    "            client_settings=self.chroma_settings,\n",
    "            client=self.chroma_client,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            collection_name=COLLECTION_NAME,\n",
    "        )\n",
    "        return db if does_vectorstore_exist(db) else None\n",
    "\n",
    "\n",
    "    def exists(self):\n",
    "        return self.get_db() is not None\n",
    "\n",
    "\n",
    "    def add_documents(self, documents, batch_size:int=CHROMA_MAX):\n",
    "        \"\"\"\n",
    "        Stores instances of `langchain_core.documents.base.Document` in vectordb\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return\n",
    "        db = self.get_db()\n",
    "        if db:\n",
    "            print(\"Creating embeddings. May take some minutes...\")\n",
    "            chunk_batches, total_chunks = batchify_chunks(documents, batch_size=batch_size)\n",
    "            for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                db.add_documents(lst)\n",
    "        else:\n",
    "            chunk_batches, total_chunks = batchify_chunks(documents, batch_size)\n",
    "            print(\"Creating embeddings. May take some minutes...\")\n",
    "            db = None\n",
    "\n",
    "            for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                if not db:\n",
    "                    db = Chroma.from_documents(\n",
    "                        lst,\n",
    "                        self.embeddings,\n",
    "                        persist_directory=self.persist_directory,\n",
    "                        client_settings=self.chroma_settings,\n",
    "                        client=self.chroma_client,\n",
    "                        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                        collection_name=COLLECTION_NAME,\n",
    "                    )\n",
    "                else:\n",
    "                    db.add_documents(lst)\n",
    "        return\n",
    "\n",
    "\n",
    "    def remove_document(self, id_to_delete):\n",
    "        \"\"\"\n",
    "        Remove a single document with ID, `id_to_delete`.\n",
    "        \"\"\"\n",
    "        if not self.exists(): return\n",
    "        self.get_db().delete(ids=[id_to_delete])\n",
    "        return\n",
    "\n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a list of files previously added to vector database (typically via `LLM.ingest`)\n",
    "        \"\"\"\n",
    "        if not self.exists(): return []\n",
    "        return set([d['source'] for d in self.get_db().get()['metadatas']])\n",
    "\n",
    "\n",
    "    def get_doc(self, id):\n",
    "        \"\"\"\n",
    "        Retrieve a record by ID\n",
    "        \"\"\"\n",
    "        if not self.exists(): return None\n",
    "        return self.get_db().get(ids=[id])\n",
    "\n",
    "\n",
    "\n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        Get total number of records\n",
    "        \"\"\"\n",
    "        if not self.exists(): return 0\n",
    "        return self.get_db().count()\n",
    "\n",
    "    \n",
    "    def erase(self, confirm=True):\n",
    "        \"\"\"\n",
    "        Resets collection and removes and stored documents\n",
    "        \"\"\"\n",
    "        if not self.exists(): return True\n",
    "        shall = True\n",
    "        if confirm:\n",
    "            msg = (\n",
    "                f\"You are about to remove all documents from the vector database.\"\n",
    "                + f\"(Original documents on file system will remain.) Are you sure?\"\n",
    "            )\n",
    "            shall = input(\"%s (Y/n) \" % msg) == \"Y\"\n",
    "        if shall:\n",
    "            self.get_db().reset_collection()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def query(self,\n",
    "              query:str, # query string\n",
    "              k:int = 4, # max number of results to return\n",
    "              filters:Optional[Dict[str, str]] = None, # filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True})\n",
    "              where_document:Optional[Dict[str, str]] = None, # filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
    "              **kwargs):\n",
    "        \"\"\"\n",
    "        Perform a semantic search of the vector DB\n",
    "        \"\"\"\n",
    "        if not self.exists(): return []\n",
    "        db = self.get_db()\n",
    "        results = db.similarity_search_with_score(query, \n",
    "                                                  filter=filters,\n",
    "                                                  where_document=where_document,\n",
    "                                                  k = k, **kwargs)\n",
    "        return results\n",
    "\n",
    "    def semantic_search(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Semantic search is equivalent to queries in this class\n",
    "        \"\"\"\n",
    "        return self.query(*args, **kwargs)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def ingest(\n",
    "        self,\n",
    "        source_directory: str, # path to folder containing document store\n",
    "        chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n",
    "        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "        ignore_fn:Optional[Callable] = None, # Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested.\n",
    "        batch_size:int=CHROMA_MAX, # batch size used when processing documents\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_directory` (previously-ingested documents are\n",
    "        ignored). When retrieved, the\n",
    "        [Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
    "        objects will each have a `metadata` dict with the absolute path to the file\n",
    "        in `metadata[\"source\"]`.\n",
    "        Extra kwargs fed to `ingest.load_single_document`.\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(source_directory):\n",
    "            raise ValueError(\"The source_directory does not exist.\")\n",
    "        elif os.path.isfile(source_directory):\n",
    "            raise ValueError(\n",
    "                \"The source_directory argument must be a folder, not a file.\"\n",
    "            )\n",
    "        texts = None\n",
    "        db = self.get_db()\n",
    "        if db:\n",
    "            # Update and store locally vectorstore\n",
    "            print(f\"Appending to existing vectorstore at {self.persist_directory}\")\n",
    "            collection = db.get()\n",
    "            ignored_files=[ metadata[\"source\"] for metadata in collection[\"metadatas\"]]\n",
    "        else:\n",
    "            print(f\"Creating new vectorstore at {self.persist_directory}\")\n",
    "            ignored_files = []\n",
    "\n",
    "        texts = process_folder(\n",
    "            source_directory,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            ignored_files=ignored_files,\n",
    "            ignore_fn=ignore_fn,\n",
    "            batch_size=batch_size,\n",
    "            **kwargs\n",
    "\n",
    "        )\n",
    "\n",
    "        texts = list(texts)\n",
    "        print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} chars each for text; max. {TABLE_CHUNK_SIZE} chars for tables)\")\n",
    "\n",
    "        self.add_documents(texts, batch_size=batch_size)\n",
    "\n",
    "        if texts:\n",
    "            print(\n",
    "                \"Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\"\n",
    "            )\n",
    "        db = None\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L69){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.add_documents\n",
       "\n",
       ">      DenseStore.add_documents (documents, batch_size:int=41000)\n",
       "\n",
       "*Stores instances of `langchain_core.documents.base.Document` in vectordb*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L69){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.add_documents\n",
       "\n",
       ">      DenseStore.add_documents (documents, batch_size:int=41000)\n",
       "\n",
       "*Stores instances of `langchain_core.documents.base.Document` in vectordb*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.add_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L115){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_all_docs\n",
       "\n",
       ">      DenseStore.get_all_docs ()\n",
       "\n",
       "*Returns a list of files previously added to vector database (typically via `LLM.ingest`)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L115){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_all_docs\n",
       "\n",
       ">      DenseStore.get_all_docs ()\n",
       "\n",
       "*Returns a list of files previously added to vector database (typically via `LLM.ingest`)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.get_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L122){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_size\n",
       "\n",
       ">      DenseStore.get_size ()\n",
       "\n",
       "*Get total number of records*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L122){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_size\n",
       "\n",
       ">      DenseStore.get_size ()\n",
       "\n",
       "*Get total number of records*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.get_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L129){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.erase\n",
       "\n",
       ">      DenseStore.erase (confirm=True)\n",
       "\n",
       "*Resets collection and removes and stored documents*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L129){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.erase\n",
       "\n",
       ">      DenseStore.erase (confirm=True)\n",
       "\n",
       "*Resets collection and removes and stored documents*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.erase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L146){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.query\n",
       "\n",
       ">      DenseStore.query (query:str, k:int=4,\n",
       ">                        filters:Optional[Dict[str,str]]=None,\n",
       ">                        where_document:Optional[Dict[str,str]]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | query string |\n",
       "| k | int | 4 | max number of results to return |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"}) |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L146){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.query\n",
       "\n",
       ">      DenseStore.query (query:str, k:int=4,\n",
       ">                        filters:Optional[Dict[str,str]]=None,\n",
       ">                        where_document:Optional[Dict[str,str]]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | query string |\n",
       "| k | int | 4 | max number of results to return |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"}) |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L177){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_embedding_model\n",
       "\n",
       ">      DenseStore.get_embedding_model ()\n",
       "\n",
       "*Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L177){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_embedding_model\n",
       "\n",
       ">      DenseStore.get_embedding_model ()\n",
       "\n",
       "*Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.get_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L163){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_db\n",
       "\n",
       ">      DenseStore.get_db ()\n",
       "\n",
       "*Returns an instance to the `langchain_chroma.Chroma` instance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L163){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.get_db\n",
       "\n",
       ">      DenseStore.get_db ()\n",
       "\n",
       "*Returns an instance to the `langchain_chroma.Chroma` instance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.get_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L184){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.ingest\n",
       "\n",
       ">      DenseStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                         chunk_overlap:int=50,\n",
       ">                         ignore_fn:Optional[Callable]=None,\n",
       ">                         batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L184){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.ingest\n",
       "\n",
       ">      DenseStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                         chunk_overlap:int=50,\n",
       ">                         ignore_fn:Optional[Callable]=None,\n",
       ">                         batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from tests/sample_data/ktrain_paper/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00, 10.09it/s]\n",
      "Processing and chunking 6 new documents: 100%|███████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 899.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 41 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "store = DenseStore()\n",
    "store.ingest(\"tests/sample_data/ktrain_paper/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
