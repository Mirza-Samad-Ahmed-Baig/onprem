{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hf.models.onnx\n",
    "\n",
    "> ONNX module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp hf.models.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "ONNX module\n",
    "\"\"\"\n",
    "\n",
    "# Conditional import\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "\n",
    "    ONNX_RUNTIME = True\n",
    "except ImportError:\n",
    "    ONNX_RUNTIME = False\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "\n",
    "from onprem.hf.models.registry import Registry\n",
    "\n",
    "\n",
    "# pylint: disable=W0223\n",
    "class OnnxModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    Provides a Transformers/PyTorch compatible interface for ONNX models. Handles casting inputs\n",
    "    and outputs with minimal to no copying of data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, config=None):\n",
    "        \"\"\"\n",
    "        Creates a new OnnxModel.\n",
    "\n",
    "        Args:\n",
    "            model: path to model or InferenceSession\n",
    "            config: path to model configuration\n",
    "        \"\"\"\n",
    "\n",
    "        if not ONNX_RUNTIME:\n",
    "            raise ImportError('onnxruntime is not available: pip install onnxruntime')\n",
    "\n",
    "        super().__init__(AutoConfig.from_pretrained(config) if config else OnnxConfig())\n",
    "\n",
    "        # Create ONNX session\n",
    "        self.model = ort.InferenceSession(model, ort.SessionOptions(), self.providers())\n",
    "\n",
    "        # Add references for this class to supported AutoModel classes\n",
    "        Registry.register(self)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\"\n",
    "        Returns model device id.\n",
    "\n",
    "        Returns:\n",
    "            model device id\n",
    "        \"\"\"\n",
    "\n",
    "        return -1\n",
    "\n",
    "    def providers(self):\n",
    "        \"\"\"\n",
    "        Returns a list of available and usable providers.\n",
    "\n",
    "        Returns:\n",
    "            list of available and usable providers\n",
    "        \"\"\"\n",
    "\n",
    "        # Create list of providers, prefer CUDA provider if available\n",
    "        # CUDA provider only available if GPU is available and onnxruntime-gpu installed\n",
    "        if torch.cuda.is_available() and \"CUDAExecutionProvider\" in ort.get_available_providers():\n",
    "            return [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "\n",
    "        # Default when CUDA provider isn't available\n",
    "        return [\"CPUExecutionProvider\"]\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        \"\"\"\n",
    "        Runs inputs through an ONNX model and returns outputs. This method handles casting inputs\n",
    "        and outputs between torch tensors and numpy arrays as shared memory (no copy).\n",
    "\n",
    "        Args:\n",
    "            inputs: model inputs\n",
    "\n",
    "        Returns:\n",
    "            model outputs\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = self.parse(inputs)\n",
    "\n",
    "        # Run inputs through ONNX model\n",
    "        results = self.model.run(None, inputs)\n",
    "\n",
    "        # pylint: disable=E1101\n",
    "        # Detect if logits is an output and return classifier output in that case\n",
    "        if any(x.name for x in self.model.get_outputs() if x.name == \"logits\"):\n",
    "            return SequenceClassifierOutput(logits=torch.from_numpy(np.array(results[0])))\n",
    "\n",
    "        return torch.from_numpy(np.array(results))\n",
    "\n",
    "    def parse(self, inputs):\n",
    "        \"\"\"\n",
    "        Parse model inputs and handle converting to ONNX compatible inputs.\n",
    "\n",
    "        Args:\n",
    "            inputs: model inputs\n",
    "\n",
    "        Returns:\n",
    "            ONNX compatible model inputs\n",
    "        \"\"\"\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        # Select features from inputs\n",
    "        for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "            if key in inputs:\n",
    "                value = inputs[key]\n",
    "\n",
    "                # Cast torch tensors to numpy\n",
    "                if hasattr(value, \"cpu\"):\n",
    "                    value = value.cpu().numpy()\n",
    "\n",
    "                # Cast to numpy array if not already one\n",
    "                features[key] = np.asarray(value)\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "class OnnxConfig(PretrainedConfig):\n",
    "    \"\"\"\n",
    "    Configuration for ONNX models.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
