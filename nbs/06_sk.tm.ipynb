{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sk.tm\n",
    "\n",
    "> scikit-learn topic modeling module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp sk.tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "TopicModel module\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Union, List, Any\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from onprem.sk import base as U\n",
    "\n",
    "\n",
    "class TopicModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts:Optional[List[str]]=None,\n",
    "        n_topics:Optional[int]=None,\n",
    "        n_features:int=100000,\n",
    "        min_df:int=5,\n",
    "        max_df:float=0.5,\n",
    "        stop_words:Union[str,list]=\"english\",\n",
    "        model_type:str=\"nmf\",\n",
    "        max_iter:int=10,\n",
    "        lda_max_iter:Optional[int]=None,\n",
    "        lda_mode:str=\"online\",\n",
    "        token_pattern:Optional[str]=None,\n",
    "        verbose:bool=True,\n",
    "        hyperparam_kwargs:Optional[dict]=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fits a topic model to documents in <texts>.\n",
    "        Example:\n",
    "            ```python\n",
    "            tm = get_topic_model(docs, n_topics=20,\n",
    "                                 n_features=1000, min_df=2, max_df=0.95)\n",
    "            ```\n",
    "            \n",
    "        **Args:**\n",
    "        \n",
    "          - texts (list of str): list of texts\n",
    "          - n_topics (int): number of topics.\n",
    "                            If None, n_topics = min{400, sqrt[# documents/2]})\n",
    "          - n_features (int):  maximum words to consider\n",
    "          - max_df (float): words in more than max_df proportion of docs discarded\n",
    "          - stop_words (str or list): either 'english' for built-in stop words or\n",
    "                                      a list of stop words to ignore\n",
    "          - model_type(str): type of topic model to fit. One of {'lda', 'nmf'}.  Default:'lda'\n",
    "          - max_iter (int): maximum iterations.  5 is default if using lda_mode='online' or nmf.\n",
    "                                If lda_mode='batch', this should be increased (e.g., 1500).\n",
    "          - lda_max_iter (int): alias for max_iter for backwards compatilibity\n",
    "          - lda_mode (str):  one of {'online', 'batch'}. Ignored if model_type !='lda'\n",
    "          - token_pattern(str): regex pattern to use to tokenize documents.\n",
    "          - verbose(bool): verbosity\n",
    "          - hyperparam_kwargs(dict): hyperparameters for LDA/NMF\n",
    "                                     Keys in this dict can be any of the following:\n",
    "                                         alpha: alpha for LDA  default: 5./n_topics\n",
    "                                         beta: beta for LDA.  default:0.01\n",
    "                                         nmf_alpha: alias for alpha for backwars compatilibity\n",
    "                                         l1_ratio: l1_ratio for NMF. default: 0\n",
    "                                         ngram_range:  whether to consider bigrams, trigrams. default: (1,1)\n",
    "\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # estimate n_topics\n",
    "        if n_topics is None:\n",
    "            if texts is None:\n",
    "                raise ValueError(\"If n_topics is None, texts must be supplied\")\n",
    "            estimated = max(1, int(math.floor(math.sqrt(len(texts) / 2))))\n",
    "            n_topics = min(400, estimated)\n",
    "            if verbose:\n",
    "                print(\"n_topics automatically set to %s\" % (n_topics))\n",
    "\n",
    "        # train model\n",
    "        if texts is not None:\n",
    "            (model, vectorizer) = self.train(\n",
    "                texts,\n",
    "                model_type=model_type,\n",
    "                n_topics=n_topics,\n",
    "                n_features=n_features,\n",
    "                min_df=min_df,\n",
    "                max_df=max_df,\n",
    "                stop_words=stop_words,\n",
    "                max_iter=max_iter,\n",
    "                lda_max_iter=lda_max_iter,\n",
    "                lda_mode=lda_mode,\n",
    "                token_pattern=token_pattern,\n",
    "                hyperparam_kwargs=hyperparam_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            vectorizer = None\n",
    "            model = None\n",
    "\n",
    "        # save model and vectorizer and hyperparameter settings\n",
    "        self.vectorizer = vectorizer\n",
    "        self.model = model\n",
    "        self.n_topics = n_topics\n",
    "        self.n_features = n_features\n",
    "        if verbose:\n",
    "            print(\"done.\")\n",
    "\n",
    "        # these variables are set by self.build():\n",
    "        self.topic_dict = None\n",
    "        self.doc_topics = None\n",
    "        self.bool_array = None\n",
    "\n",
    "        self.scorer = None  # set by self.train_scorer()\n",
    "        self.recommender = None  # set by self.train_recommender()\n",
    "        return\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        texts:list,\n",
    "        model_type:str=\"lda\",\n",
    "        n_topics:Optional[int]=None,\n",
    "        n_features:int=10000,\n",
    "        min_df:int=5,\n",
    "        max_df:float=0.5,\n",
    "        stop_words:Union[str,list]=\"english\",\n",
    "        max_iter:int=5,\n",
    "        lda_max_iter:Optional[int]=None,\n",
    "        lda_mode:int=\"online\",\n",
    "        token_pattern:Optional[str]=None,\n",
    "        hyperparam_kwargs:Optional[dict]=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fits a topic model to documents in `texts`.\n",
    "                                            \n",
    "        **Args:**\n",
    "        \n",
    "          - texts (list of str): list of texts\n",
    "          - n_topics (int): number of topics.\n",
    "                            If None, n_topics = min{400, sqrt[# documents/2]})\n",
    "          - n_features (int):  maximum words to consider\n",
    "          - max_df (float): words in more than max_df proportion of docs discarded\n",
    "          - stop_words (str or list): either 'english' for built-in stop words or\n",
    "                                     a list of stop words to ignore\n",
    "          - max_iter (int): maximum iterations for 'lda'.  5 is default if using lda_mode='online'.\n",
    "          - lda_max_iter (int): alias for max_iter for backwards compatibility\n",
    "                                If lda_mode='batch', this should be increased (e.g., 1500).\n",
    "                                Ignored if model_type != 'lda'\n",
    "          - lda_mode (str):  one of {'online', 'batch'}. Ignored of model_type !='lda'\n",
    "          - token_pattern(str): regex pattern to use to tokenize documents.\n",
    "                                If None, a default tokenizer will be used\n",
    "          - hyperparam_kwargs(dict): hyperparameters for LDA/NMF\n",
    "                                     Keys in this dict can be any of the following:\n",
    "                                         alpha: alpha for LDA  default: 5./n_topics\n",
    "                                         beta: beta for LDA.  default:0.01\n",
    "                                         nmf_alpha_W: alpha for NMF alpha_W (default is 0.0)\n",
    "                                         nmf_alpha_H: alpha for NMF alpha_H (default is 'same')\n",
    "                                         l1_ratio: l1_ratio for NMF. default: 0\n",
    "                                         ngram_range:  whether to consider bigrams, trigrams. default: (1,1)\n",
    "\n",
    "        **Returns:**\n",
    "        \n",
    "            A tuple in the form: (model, vectorizer)\n",
    "        \"\"\"\n",
    "        max_iter = lda_max_iter if lda_max_iter is not None else max_iter\n",
    "        if hyperparam_kwargs is None:\n",
    "            hyperparam_kwargs = {}\n",
    "        alpha = hyperparam_kwargs.get(\"alpha\", 5.0 / n_topics)\n",
    "        nmf_alpha_W = hyperparam_kwargs.get(\"nmf_alpha_W\", 0.0)\n",
    "        nmf_alpha_H = hyperparam_kwargs.get(\"nmf_alpha_H\", \"same\")\n",
    "        beta = hyperparam_kwargs.get(\"beta\", 0.01)\n",
    "        l1_ratio = hyperparam_kwargs.get(\"l1_ratio\", 0)\n",
    "        ngram_range = hyperparam_kwargs.get(\"ngram_range\", (1, 1))\n",
    "\n",
    "        # adjust defaults based on language detected\n",
    "        if texts is not None:\n",
    "            lang = U.detect_lang(texts)\n",
    "            if lang != \"en\":\n",
    "                stopwords = None if stop_words == \"english\" else stop_words\n",
    "                token_pattern = (\n",
    "                    r\"(?u)\\b\\w+\\b\" if token_pattern is None else token_pattern\n",
    "                )\n",
    "            if U.is_nospace_lang(lang):\n",
    "                texts = U.spit_chinese(texts)\n",
    "            if self.verbose:\n",
    "                print(\"lang: %s\" % (lang))\n",
    "\n",
    "        # preprocess texts\n",
    "        if self.verbose:\n",
    "            print(\"preprocessing texts...\")\n",
    "        if token_pattern is None:\n",
    "            token_pattern = U.DEFAULT_TOKEN_PATTERN\n",
    "        # if token_pattern is None: token_pattern = r'(?u)\\b\\w\\w+\\b'\n",
    "        vectorizer = CountVectorizer(\n",
    "            max_df=max_df,\n",
    "            min_df=min_df,\n",
    "            max_features=n_features,\n",
    "            stop_words=stop_words,\n",
    "            token_pattern=token_pattern,\n",
    "            ngram_range=ngram_range,\n",
    "        )\n",
    "\n",
    "        x_train = vectorizer.fit_transform(texts)\n",
    "\n",
    "        # fit model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"fitting model...\")\n",
    "        if model_type == \"lda\":\n",
    "            model = LatentDirichletAllocation(\n",
    "                n_components=n_topics,\n",
    "                max_iter=max_iter,\n",
    "                learning_method=lda_mode,\n",
    "                learning_offset=50.0,\n",
    "                doc_topic_prior=alpha,\n",
    "                topic_word_prior=beta,\n",
    "                verbose=self.verbose,\n",
    "                random_state=0,\n",
    "            )\n",
    "        elif model_type == \"nmf\":\n",
    "            model = NMF(\n",
    "                n_components=n_topics,\n",
    "                max_iter=max_iter,\n",
    "                verbose=self.verbose,\n",
    "                alpha_W=nmf_alpha_W,\n",
    "                alpha_H=nmf_alpha_H,\n",
    "                l1_ratio=l1_ratio,\n",
    "                random_state=0,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"unknown model type:\", str(model_type))\n",
    "        model.fit(x_train)\n",
    "\n",
    "        # save model and vectorizer and hyperparameter settings\n",
    "        return (model, vectorizer)\n",
    "\n",
    "    @property\n",
    "    def topics(self):\n",
    "        \"\"\"\n",
    "        convenience method/property\n",
    "        \"\"\"\n",
    "        return self.get_topics()\n",
    "\n",
    "    def get_document_topic_distribution(self):\n",
    "        \"\"\"\n",
    "        Gets the document-topic distribution.\n",
    "        Each row is a document and each column is a topic\n",
    "        The output of this method is equivalent to invoking get_doctopics with no arguments.\n",
    "        \"\"\"\n",
    "        self._check_build()\n",
    "        return self.doc_topics\n",
    "\n",
    "    def get_sorted_docs(self, topic_id:int):\n",
    "        \"\"\"\n",
    "        Returns all docs sorted by relevance to <topic_id>.\n",
    "        Unlike get_docs, this ranks documents by the supplied topic_id rather\n",
    "        than the topic_id to which document is most relevant.\n",
    "        \"\"\"\n",
    "        docs = self.get_docs()\n",
    "        d = {}\n",
    "        for doc in docs:\n",
    "            d[doc[\"doc_id\"]] = doc\n",
    "        m = self.get_document_topic_distribution()\n",
    "        doc_ids = (-m[:, topic_id]).argsort()\n",
    "        return [d[doc_id] for doc_id in doc_ids]\n",
    "\n",
    "    def get_word_weights(self, topic_id:int, n_words:int=100):\n",
    "        \"\"\"\n",
    "        Returns a list tuples of the form: (word, weight) for given topic_id.\n",
    "        The weight can be interpreted as the number of times word was assigned to topic with given topic_id.\n",
    "        REFERENCE: https://stackoverflow.com/a/48890889/13550699\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "          - topic_id(int): topic ID\n",
    "          - n_words=int): number of top words\n",
    "        \"\"\"\n",
    "        self._check_model()\n",
    "        if topic_id + 1 > len(self.model.components_):\n",
    "            raise ValueError(\n",
    "                \"topic_id must be less than %s\" % (len(self.model.components_))\n",
    "            )\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        word_probs = self.model.components_[topic_id]\n",
    "        word_ids = [i for i in word_probs.argsort()[: -n_words - 1 : -1]]\n",
    "        words = [feature_names[i] for i in word_ids]\n",
    "        probs = [word_probs[i] for i in word_ids]\n",
    "        return list(zip(words, probs))\n",
    "\n",
    "    def get_topics(self, n_words:int=10, as_string:bool=True, show_counts:bool=False):\n",
    "        \"\"\"\n",
    "        Returns a list of discovered topics\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "          - n_words(int): number of words to use in topic summary\n",
    "          - as_string(bool): If True, each summary is a space-delimited string instead of list of words\n",
    "          - show_counts(bool): If True, returns list of tuples of form (id, topic summary, count).\n",
    "                               Otherwise, a list of topic summaries.\n",
    "        \n",
    "        **Returns:**\n",
    "        \n",
    "          List of topic summaries if  show_count is False.\n",
    "          Dictionary where key is topic ID and value is a tuple of form (topic summary, count) if show_count is True.\n",
    "\n",
    "        \"\"\"\n",
    "        self._check_model()\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        topic_summaries = []\n",
    "        for topic_idx, topic in enumerate(self.model.components_):\n",
    "            summary = [feature_names[i] for i in topic.argsort()[: -n_words - 1 : -1]]\n",
    "            if as_string:\n",
    "                summary = \" \".join(summary)\n",
    "            topic_summaries.append(summary)\n",
    "\n",
    "        if show_counts:\n",
    "            self._check_build()\n",
    "            topic_counts = sorted(\n",
    "                [(k, topic_summaries[k], len(v)) for k, v in self.topic_dict.items()],\n",
    "                key=lambda kv: kv[-1],\n",
    "                reverse=True,\n",
    "            )\n",
    "            return dict((t[0], t[1:]) for t in topic_counts)\n",
    "\n",
    "        return topic_summaries\n",
    "\n",
    "    def print_topics(self, n_words:int=10, show_counts:bool=False):\n",
    "        \"\"\"\n",
    "        print topics\n",
    "\n",
    "        **Args**:\n",
    "        \n",
    "          - n_words(int): number of words to describe each topic\n",
    "          - show_counts(bool): If True, print topics with document counts, where\n",
    "                             the count is the number of documents with that topic as primary.\n",
    "        \"\"\"\n",
    "        topics = self.get_topics(n_words=n_words, as_string=True)\n",
    "        if show_counts:\n",
    "            self._check_build()\n",
    "            topic_counts = sorted(\n",
    "                [(k, topics[k], len(v)) for k, v in self.topic_dict.items()],\n",
    "                key=lambda kv: kv[-1],\n",
    "                reverse=True,\n",
    "            )\n",
    "            for idx, topic, count in topic_counts:\n",
    "                print(\"topic:%s | count:%s | %s\" % (idx, count, topic))\n",
    "        else:\n",
    "            for i, t in enumerate(topics):\n",
    "                print(\"topic %s | %s\" % (i, t))\n",
    "        return\n",
    "\n",
    "    def build(self, texts:List[str], threshold:Optional[float]=None):\n",
    "        \"\"\"\n",
    "        Builds the document-topic distribution showing the topic probability distirbution\n",
    "        for each document in <texts> with respect to the learned topic space.\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "          - texts (list of str): list of text documents\n",
    "          - threshold (float): If not None, documents with whose highest topic probability\n",
    "                               is less than threshold are filtered out.\n",
    "        \"\"\"\n",
    "        if threshold is not None:\n",
    "            doc_topics, bool_array = self.predict(texts, threshold=threshold)\n",
    "        else:\n",
    "            doc_topics = self.predict(texts)\n",
    "            bool_array = np.array([True] * len(texts))\n",
    "\n",
    "        self.doc_topics = doc_topics\n",
    "        self.bool_array = bool_array\n",
    "\n",
    "        texts = [text for i, text in enumerate(texts) if bool_array[i]]\n",
    "        self.topic_dict = self._rank_documents(texts, doc_topics=doc_topics)\n",
    "        return\n",
    "\n",
    "    def filter(self, obj:Any):\n",
    "        \"\"\"\n",
    "        The build method may prune documents based on threshold.\n",
    "        This method prunes other lists based on how build pruned documents.\n",
    "        This is useful to filter lists containing metadata associated with documents\n",
    "        for use with visualize_documents.\n",
    "        \n",
    "        **Args:**\n",
    "          - obj (list|np.ndarray|pandas.DataFrame):a list, numpy array, or DataFrame of data\n",
    "            \n",
    "        **Returns:**\n",
    "        \n",
    "          - filtered obj\n",
    "        \"\"\"\n",
    "        length = (\n",
    "            obj.shape[0] if isinstance(obj, (pd.DataFrame, np.ndarray)) else len(obj)\n",
    "        )\n",
    "        if length != self.bool_array.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Length of obj is not consistent with the number of documents \"\n",
    "                + \"supplied to get_topic_model\"\n",
    "            )\n",
    "        from itertools import compress\n",
    "\n",
    "        return pd.DataFrame(data=compress(obj.itertuples(index=False), self.bool_array),\n",
    "                            columns=obj.columns) if isinstance(obj, pd.DataFrame) \\\n",
    "                                else list(compress(obj, self.bool_array))\n",
    "\n",
    "    def get_docs(self, topic_ids:list=[], doc_ids:list=[], rank:bool=False):\n",
    "        \"\"\"\n",
    "        Returns document entries for supplied topic_ids.\n",
    "        Documents returned are those whose primary topic is topic with given topic_id\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "          - topic_ids(list of ints): list of topid IDs where each id is in the range\n",
    "                                     of range(self.n_topics).\n",
    "          - doc_ids (list of ints): list of document IDs where each id is an index\n",
    "                                    into self.doctopics\n",
    "          - rank(bool): If True, the list is sorted first by topic_id (ascending)\n",
    "                        and then ty topic probability (descending).\n",
    "                        Otherwise, list is sorted by doc_id (i.e., the order\n",
    "                        of texts supplied to self.build (which is the order of self.doc_topics).\n",
    "\n",
    "        **Returns:**\n",
    "        \n",
    "            list of dicts:  list of dicts with keys:\n",
    "                            'text': text of document\n",
    "                            'doc_id': ID of document\n",
    "                            'topic_proba': topic probability (or score)\n",
    "                            'topic_id': ID of topic\n",
    "\n",
    "        \"\"\"\n",
    "        self._check_build()\n",
    "        if not topic_ids:\n",
    "            topic_ids = list(range(self.n_topics))\n",
    "        result_texts = []\n",
    "        for topic_id in topic_ids:\n",
    "            if topic_id not in self.topic_dict:\n",
    "                continue\n",
    "            texts = [\n",
    "                {\n",
    "                    \"text\": tup[0],\n",
    "                    \"doc_id\": tup[1],\n",
    "                    \"topic_proba\": tup[2],\n",
    "                    \"topic_id\": topic_id,\n",
    "                }\n",
    "                for tup in self.topic_dict[topic_id]\n",
    "                if not doc_ids or tup[1] in doc_ids\n",
    "            ]\n",
    "            result_texts.extend(texts)\n",
    "        if not rank:\n",
    "            result_texts = sorted(result_texts, key=lambda x: x[\"doc_id\"])\n",
    "        return result_texts\n",
    "\n",
    "    def get_doctopics(self, topic_ids:list=[], doc_ids:list=[]):\n",
    "        \"\"\"\n",
    "        Returns a topic probability distribution for documents\n",
    "        with primary topic that is one of <topic_ids> and with doc_id in <doc_ids>.\n",
    "\n",
    "        If no topic_ids or doc_ids are provided, then topic distributions for all documents\n",
    "        are returned (which equivalent to the output of get_document_topic_distribution).\n",
    "\n",
    "        **Args:**\n",
    "        \n",
    "          - topic_ids(list of ints): list of topid IDs where each id is in the range\n",
    "                                     of range(self.n_topics).\n",
    "          - doc_ids (list of ints): list of document IDs where each id is an index\n",
    "                                    into self.doctopics\n",
    "        **Returns:**\n",
    "        \n",
    "            np.ndarray: Each row is the topic probability distribution of a document.\n",
    "                        Array is sorted in the order returned by self.get_docs.\n",
    "\n",
    "        \"\"\"\n",
    "        docs = self.get_docs(topic_ids=topic_ids, doc_ids=doc_ids)\n",
    "        return np.array([self.doc_topics[idx] for idx in [x[\"doc_id\"] for x in docs]])\n",
    "\n",
    "    def get_texts(self, topic_ids:list=[]):\n",
    "        \"\"\"\n",
    "        Returns texts for documents\n",
    "        with primary topic that is one of <topic_ids>\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "          - topic_ids(list of ints): list of topic IDs\n",
    "          \n",
    "        **Returns:**\n",
    "        \n",
    "            list of str\n",
    "        \"\"\"\n",
    "        if not topic_ids:\n",
    "            topic_ids = list(range(self.n_topics))\n",
    "        docs = self.get_docs(topic_ids)\n",
    "        return [x[0] for x in docs]\n",
    "\n",
    "    def predict(self, texts:List[str], threshold:Optional[float]=None, harden:bool=False):\n",
    "        \"\"\"\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "          - texts (list of str): list of texts\n",
    "          - threshold (float): If not None, documents with maximum topic scores\n",
    "                                less than <threshold> are filtered out\n",
    "          - harden(bool): If True, each document is assigned to a single topic for which\n",
    "                          it has the highest score\n",
    "        \n",
    "        **Returns:**\n",
    "        \n",
    "            if threshold is None:\n",
    "                np.ndarray: topic distribution for each text document\n",
    "            else:\n",
    "                (np.ndarray, np.ndarray): topic distribution and boolean array\n",
    "        \"\"\"\n",
    "        self._check_model()\n",
    "        transformed_texts = self.vectorizer.transform(texts)\n",
    "        X_topics = self.model.transform(transformed_texts)\n",
    "        _idx = np.array([True] * len(texts))\n",
    "        if threshold is not None:\n",
    "            _idx = (\n",
    "                np.amax(X_topics, axis=1) > threshold\n",
    "            )  # idx of doc that above the threshold\n",
    "            _idx = np.array(_idx)\n",
    "            X_topics = X_topics[_idx]\n",
    "        if harden:\n",
    "            X_topics = self._harden_topics(X_topics)\n",
    "        if threshold is not None:\n",
    "            return (X_topics, _idx)\n",
    "        else:\n",
    "            return X_topics\n",
    "\n",
    "    def visualize_documents(\n",
    "        self,\n",
    "        texts=None,\n",
    "        doc_topics=None,\n",
    "        width=700,\n",
    "        height=700,\n",
    "        point_size=5,\n",
    "        title=\"Document Visualization\",\n",
    "        extra_info={},\n",
    "        colors=None,\n",
    "        filepath=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates a visualization of a set of documents based on model.\n",
    "        If <texts> is supplied, raw documents will be first transformed into document-topic\n",
    "        matrix.  If <doc_topics> is supplied, then this will be used for visualization instead.\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "          - texts(list of str): list of document texts.  Mutually-exclusive with <doc_topics>\n",
    "          - doc_topics(ndarray): pre-computed topic distribution for each document in texts.\n",
    "                                 Mutually-exclusive with <texts>.\n",
    "          - width(int): width of image\n",
    "          - height(int): height of image\n",
    "          - point_size(int): size of circles in plot\n",
    "          - title(str):  title of visualization\n",
    "          - extra_info(dict of lists): A user-supplied information for each datapoint (attributes of the datapoint).\n",
    "                                       The keys are field names.  The values are lists - each of which must\n",
    "                                       be the same number of elements as <texts> or <doc_topics>. These fields are displayed\n",
    "                                       when hovering over datapoints in the visualization.\n",
    "          - colors(list of str):  list of Hex color codes for each datapoint.\n",
    "                                  Length of list must match either len(texts) or doc_topics.shape[0]\n",
    "          - filepath(str):             Optional filepath to save the interactive visualization\n",
    "        \"\"\"\n",
    "\n",
    "        # error-checking\n",
    "        if texts is not None:\n",
    "            length = len(texts)\n",
    "        else:\n",
    "            length = doc_topics.shape[0]\n",
    "        if colors is not None and len(colors) != length:\n",
    "            raise ValueError(\n",
    "                \"length of colors is not consistent with length of texts or doctopics\"\n",
    "            )\n",
    "        if texts is not None and doc_topics is not None:\n",
    "            raise ValueError(\"texts is mutually-exclusive with doc_topics\")\n",
    "        if texts is None and doc_topics is None:\n",
    "            raise ValueError(\"One of texts or doc_topics is required.\")\n",
    "        if extra_info:\n",
    "            invalid_keys = [\"x\", \"y\", \"topic\", \"fill_color\"]\n",
    "            for k in extra_info.keys():\n",
    "                if k in invalid_keys:\n",
    "                    raise ValueError('cannot use \"%s\" as key in extra_info' % (k))\n",
    "                lst = extra_info[k]\n",
    "                if len(lst) != length:\n",
    "                    raise ValueError(\"texts and extra_info lists must be same size\")\n",
    "\n",
    "        # check fo bokeh\n",
    "        try:\n",
    "            import bokeh.plotting as bp\n",
    "            from bokeh.io import output_notebook\n",
    "            from bokeh.models import HoverTool\n",
    "            from bokeh.plotting import save\n",
    "        except:\n",
    "            warnings.warn(\n",
    "                \"visualize_documents method requires bokeh package: pip install bokeh\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # prepare data\n",
    "        if doc_topics is not None:\n",
    "            X_topics = doc_topics\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"transforming texts...\", end=\"\")\n",
    "            X_topics = self.predict(texts, harden=False)\n",
    "            if self.verbose:\n",
    "                print(\"done.\")\n",
    "\n",
    "        # reduce to 2-D\n",
    "        if self.verbose:\n",
    "            print(\"reducing to 2 dimensions...\", end=\"\")\n",
    "        tsne_model = TSNE(\n",
    "            n_components=2, verbose=self.verbose, random_state=0, angle=0.99, init=\"pca\"\n",
    "        )\n",
    "        tsne_lda = tsne_model.fit_transform(X_topics)\n",
    "        print(\"done.\")\n",
    "\n",
    "        # get random colormap\n",
    "        colormap = U.get_random_colors(self.n_topics)\n",
    "\n",
    "        # generate inline visualization in Jupyter notebook\n",
    "        lda_keys = self._harden_topics(X_topics)\n",
    "        if colors is None:\n",
    "            colors = colormap[lda_keys]\n",
    "        topic_summaries = self.get_topics(n_words=5)\n",
    "        os.environ[\"BOKEH_RESOURCES\"] = \"inline\"\n",
    "        output_notebook()\n",
    "        dct = {\n",
    "            \"x\": tsne_lda[:, 0],\n",
    "            \"y\": tsne_lda[:, 1],\n",
    "            \"topic\": [topic_summaries[tid] for tid in lda_keys],\n",
    "            \"fill_color\": colors,\n",
    "        }\n",
    "        tool_tups = [(\"index\", \"$index\"), (\"(x,y)\", \"($x,$y)\"), (\"topic\", \"@topic\")]\n",
    "        for k in extra_info.keys():\n",
    "            dct[k] = extra_info[k]\n",
    "            tool_tups.append((k, \"@\" + k))\n",
    "\n",
    "        source = bp.ColumnDataSource(data=dct)\n",
    "        hover = HoverTool(tooltips=tool_tups)\n",
    "        p = bp.figure(\n",
    "            width=width,\n",
    "            height=height,\n",
    "            tools=[hover, \"save\", \"pan\", \"wheel_zoom\", \"box_zoom\", \"reset\"],\n",
    "            # tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "            title=title,\n",
    "        )\n",
    "        # plot_lda = bp.figure(plot_width=1400, plot_height=1100,\n",
    "        # title=title,\n",
    "        # tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "        # x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "        p.circle(\"x\", \"y\", size=point_size, source=source, fill_color=\"fill_color\")\n",
    "        bp.show(p)\n",
    "        if filepath is not None:\n",
    "            bp.output_file(filepath)\n",
    "            bp.save(p)\n",
    "        return\n",
    "\n",
    "    def train_recommender(self, n_neighbors:int=20, metric:str=\"minkowski\", p:int=2):\n",
    "        \"\"\"\n",
    "        Trains a recommender that, given a single document, will return\n",
    "        documents in the corpus that are semantically similar to it.\n",
    "\n",
    "       **Args**:\n",
    "\n",
    "         - n_neighbors: # of neighbors to use\n",
    "         - metric: metric to use\n",
    "         - p: paramter to default Minkowsi metric\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "        rec = NearestNeighbors(n_neighbors=n_neighbors, metric=metric, p=p)\n",
    "        probs = self.get_doctopics()\n",
    "        rec.fit(probs)\n",
    "        self.recommender = rec\n",
    "        return\n",
    "\n",
    "    def recommend(self, text:Optional[list]=None, doc_topic=None, n:int=5, n_neighbors:int=100):\n",
    "        \"\"\"\n",
    "        Given an example document, recommends documents similar to it\n",
    "        from the set of documents supplied to build().\n",
    "\n",
    "        **Args:**\n",
    "        \n",
    "          - texts(list of str): list of document texts.  Mutually-exclusive with <doc_topics>\n",
    "          - doc_topics(ndarray): pre-computed topic distribution for each document in texts.\n",
    "                                 Mutually-exclusive with <texts>.\n",
    "          - n (int): number of recommendations to return\n",
    "          \n",
    "        **Returns:**\n",
    "        \n",
    "            list of tuples: each tuple is of the form:\n",
    "                            (text, doc_id, topic_probability, topic_id)\n",
    "\n",
    "        \"\"\"\n",
    "        # error-checks\n",
    "        if text is not None and doc_topic is not None:\n",
    "            raise ValueError(\"text is mutually-exclusive with doc_topic\")\n",
    "        if text is None and doc_topic is None:\n",
    "            raise ValueError(\"One of text or doc_topic is required.\")\n",
    "        if text is not None and type(text) not in [str]:\n",
    "            raise ValueError(\"text must be a str \")\n",
    "        if doc_topic is not None and type(doc_topic) not in [np.ndarray]:\n",
    "            raise ValueError(\"doc_topic must be a np.ndarray\")\n",
    "\n",
    "        if n > n_neighbors:\n",
    "            n_neighbors = n\n",
    "\n",
    "        x_test = [doc_topic]\n",
    "        if text:\n",
    "            x_test = self.predict([text])\n",
    "        docs = self.get_docs()\n",
    "        indices = self.recommender.kneighbors(\n",
    "            x_test, return_distance=False, n_neighbors=n_neighbors\n",
    "        )\n",
    "        results = [doc for i, doc in enumerate(docs) if i in indices]\n",
    "        return results[:n]\n",
    "\n",
    "    def train_scorer(self, topic_ids:list=[], doc_ids=[], n_neighbors:int=20):\n",
    "        \"\"\"\n",
    "        Trains a scorer that can score documents based on similarity to a\n",
    "        seed set of documents represented by topic_ids and doc_ids.\n",
    "\n",
    "        NOTE: The score method currently employs the use of LocalOutLierFactor, which\n",
    "        means you should not try to score documents that were used in training. Only\n",
    "        new, unseen documents should be scored for similarity.\n",
    "        REFERENCE:\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor\n",
    "\n",
    "        **Args:**\n",
    "        \n",
    "          - topic_ids(list of ints): list of topid IDs where each id is in the range\n",
    "                                     of range(self.n_topics).  Documents associated\n",
    "                                     with these topic_ids will be used as seed set.\n",
    "          - doc_ids (list of ints): list of document IDs where each id is an index\n",
    "                                    into self.doctopics.  Documents associated\n",
    "                                    with these doc_ids will be used as seed set.\n",
    "        **Returns:**\n",
    "            None\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "        clf = LocalOutlierFactor(\n",
    "            n_neighbors=n_neighbors, novelty=True, contamination=0.1\n",
    "        )\n",
    "        probs = self.get_doctopics(topic_ids=topic_ids, doc_ids=doc_ids)\n",
    "        clf.fit(probs)\n",
    "        self.scorer = clf\n",
    "        return\n",
    "\n",
    "    def score(self, texts:Optional[list]=None, doc_topics=None):\n",
    "        \"\"\"\n",
    "        Given a new set of documents (supplied as texts or doc_topics), the score method\n",
    "        uses a One-Class classifier to score documents based on similarity to a\n",
    "        seed set of documents (where seed set is computed by train_scorer() method).\n",
    "\n",
    "        Higher scores indicate a higher degree of similarity.\n",
    "        Positive values represent a binary decision of similar.\n",
    "        Negative values represent a binary decision of dissimlar.\n",
    "        In practice, negative scores closer to zer will also be simlar as One-Class\n",
    "        classifiers are more strict than traditional binary classifiers.\n",
    "        Documents with negative scores closer to zero are good candidates for\n",
    "        inclusion in a training set for binary classification (e.g., active labeling).\n",
    "\n",
    "        NOTE: The score method currently employs the use of LocalOutLierFactor, which\n",
    "        means you should not try to score documents that were used in training. Only\n",
    "        new, unseen documents should be scored for similarity.\n",
    "\n",
    "        **Args:**\n",
    "            \n",
    "          - texts(list of str): list of document texts.  Mutually-exclusive with <doc_topics>\n",
    "          - doc_topics(ndarray): pre-computed topic distribution for each document in texts.\n",
    "                                 Mutually-exclusive with <texts>.\n",
    "        **Returns:**\n",
    "        \n",
    "            list of floats:  larger values indicate higher degree of similarity\n",
    "                             positive values indicate a binary decision of similar\n",
    "                             negative values indicate binary decision of dissimilar\n",
    "                             In practice, negative scores closer to zero will also\n",
    "                             be similar as One-class classifiers are more strict\n",
    "                             than traditional binary classifiers.\n",
    "\n",
    "        \"\"\"\n",
    "        # error-checks\n",
    "        if texts is not None and doc_topics is not None:\n",
    "            raise ValueError(\"texts is mutually-exclusive with doc_topics\")\n",
    "        if texts is None and doc_topics is None:\n",
    "            raise ValueError(\"One of texts or doc_topics is required.\")\n",
    "        if texts is not None and type(texts) not in [list, np.ndarray]:\n",
    "            raise ValueError(\"texts must be either a list or numpy ndarray\")\n",
    "        if doc_topics is not None and type(doc_topics) not in [np.ndarray]:\n",
    "            raise ValueError(\"doc_topics must be a np.ndarray\")\n",
    "\n",
    "        x_test = doc_topics\n",
    "        if texts:\n",
    "            x_test = self.predict(texts)\n",
    "        return self.scorer.decision_function(x_test)\n",
    "\n",
    "    def search(self, query, topic_ids:list=[], doc_ids:list=[], case_sensitive:bool=False):\n",
    "        \"\"\"\n",
    "        search documents for query string.\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "         - query(str):  the word or phrase to search\n",
    "         - topic_ids(list of ints): list of topid IDs where each id is in the range\n",
    "                                     of range(self.n_topics).\n",
    "          - doc_ids (list of ints): list of document IDs where each id is an index\n",
    "                                    into self.doctopics\n",
    "          - case_sensitive(bool):  If True, case sensitive search\n",
    "        \"\"\"\n",
    "\n",
    "        # setup pattern\n",
    "        if not case_sensitive:\n",
    "            query = query.lower()\n",
    "        pattern = re.compile(r\"\\b%s\\b\" % query)\n",
    "\n",
    "        # retrive docs\n",
    "        docs = self.get_docs(topic_ids=topic_ids, doc_ids=doc_ids)\n",
    "\n",
    "        # search\n",
    "        mb = master_bar(range(1))\n",
    "        results = []\n",
    "        for i in mb:\n",
    "            for doc in progress_bar(docs, parent=mb):\n",
    "                text = doc[\"text\"]\n",
    "                if not case_sensitive:\n",
    "                    text = text.lower()\n",
    "                matches = pattern.findall(text)\n",
    "                if matches:\n",
    "                    results.append(doc)\n",
    "            if self.verbose:\n",
    "                mb.write(\"done.\")\n",
    "        return results\n",
    "\n",
    "    def _rank_documents(self, texts:list, doc_topics=None):\n",
    "        \"\"\"\n",
    "        Rank documents by topic score.\n",
    "        If topic_index is supplied, rank documents based on relevance to supplied topic.\n",
    "        Otherwise, rank all texts by their highest topic score (for any topic).\n",
    "        \n",
    "        **Args:**\n",
    "          - texts(list of str): list of document texts.\n",
    "          - doc_topics(ndarray): pre-computed topic distribution for each document\n",
    "                                 If None, re-computed from texts.\n",
    "\n",
    "        **Returns:**\n",
    "        \n",
    "            dict of lists: each element in list is a tuple of (doc_index, topic_index, score)\n",
    "            ... where doc_index is an index into either texts\n",
    "        \"\"\"\n",
    "        if doc_topics is not None:\n",
    "            X_topics = doc_topics\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"transforming texts to topic space...\")\n",
    "            X_topics = self.predict(texts)\n",
    "        topics = np.argmax(X_topics, axis=1)\n",
    "        scores = np.amax(X_topics, axis=1)\n",
    "        doc_ids = np.array([i for i, x in enumerate(texts)])\n",
    "        result = list(zip(texts, doc_ids, topics, scores))\n",
    "        if self.verbose:\n",
    "            print(\"done.\")\n",
    "        result = sorted(result, key=lambda x: x[-1], reverse=True)\n",
    "        result_dict = {}\n",
    "        for r in result:\n",
    "            text = r[0]\n",
    "            doc_id = r[1]\n",
    "            topic_id = r[2]\n",
    "            score = r[3]\n",
    "            lst = result_dict.get(topic_id, [])\n",
    "            lst.append((text, doc_id, score))\n",
    "            result_dict[topic_id] = lst\n",
    "        return result_dict\n",
    "\n",
    "    def _harden_topics(self, X_topics):\n",
    "        \"\"\"\n",
    "        Transforms soft-clustering to hard-clustering\n",
    "        \"\"\"\n",
    "        max_topics = []\n",
    "        for i in range(X_topics.shape[0]):\n",
    "            max_topics.append(X_topics[i].argmax())\n",
    "        X_topics = np.array(max_topics)\n",
    "        return X_topics\n",
    "\n",
    "    def _check_build(self):\n",
    "        self._check_model()\n",
    "        if self.topic_dict is None:\n",
    "            raise Exception(\"Must call build() method.\")\n",
    "\n",
    "    def _check_scorer(self):\n",
    "        if self.scorer is None:\n",
    "            raise Exception(\"Must call train_scorer()\")\n",
    "\n",
    "    def _check_recommender(self):\n",
    "        if self.recommender is None:\n",
    "            raise Exception(\"Must call train_recommender()\")\n",
    "\n",
    "    def _check_model(self):\n",
    "        if self.model is None or self.vectorizer is None:\n",
    "            raise Exception(\"Must call train()\")\n",
    "\n",
    "    def save(self, fname:str):\n",
    "        \"\"\"\n",
    "        save TopicModel object\n",
    "        \"\"\"\n",
    "\n",
    "        with open(fname + \".tm_vect\", \"wb\") as f:\n",
    "            pickle.dump(self.vectorizer, f)\n",
    "        with open(fname + \".tm_model\", \"wb\") as f:\n",
    "            pickle.dump(self.model, f)\n",
    "        params = {\n",
    "            \"n_topics\": self.n_topics,\n",
    "            \"n_features\": self.n_features,\n",
    "            \"verbose\": self.verbose,\n",
    "        }\n",
    "        with open(fname + \".tm_params\", \"wb\") as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "get_topic_model = TopicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
