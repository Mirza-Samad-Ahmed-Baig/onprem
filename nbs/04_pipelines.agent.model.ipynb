{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelines.agent.model\n",
    "\n",
    "> Agent-friendly wrapper around and LLM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines.agent.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import re\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from smolagents import ChatMessage, Model, get_clean_message_list, tool_role_conversions\n",
    "from smolagents.models import get_tool_call_from_text, remove_stop_sequences\n",
    "import onprem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class AgentModel(Model):\n",
    "    \"\"\"\n",
    "    A smolagents Model implementation that wraps an onprem LLM instance.\n",
    "    \n",
    "    This adapter allows onprem LLM instances to be used with smolagents Agents.\n",
    "    \n",
    "    Parameters:\n",
    "        llm (LLM): An instance of onprem.llm.base.LLM\n",
    "        **kwargs: Additional keyword arguments to pass to the parent Model class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: onprem.LLM,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.maxlength=8192\n",
    "        self.model_id = self.llm.model_name\n",
    "        super().__init__(\n",
    "            flatten_messages_as_text=True,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "    def generate(\n",
    "        self,\n",
    "        messages,\n",
    "        stop_sequences=None,\n",
    "        response_format=None, \n",
    "        tools_to_call_from=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Process the input messages and return the model's response.\n",
    "        \n",
    "        Parameters:\n",
    "            messages: A list of message dictionaries to be processed.\n",
    "            stop_sequences: A list of strings that will stop generation if encountered.\n",
    "            response_format: The response format to use in the model's response.\n",
    "            tools_to_call_from: A list of tools that the model can use.\n",
    "            **kwargs: Additional keyword arguments to pass to the LLM.\n",
    "            \n",
    "        Returns:\n",
    "            ChatMessage: A chat message object containing the model's response.\n",
    "        \"\"\"\n",
    "        # Convert smolagents messages to a format that onprem LLM can use\n",
    "        messages = self.clean(messages)\n",
    "        \n",
    "        # Call the LLM with the processed messages\n",
    "        #import litellm as api\n",
    "        #result = api.completion(\n",
    "                #model='ollama_chat/llama3.1:8b',\n",
    "                #messages=messages,\n",
    "                #max_tokens=self.maxlength,\n",
    "                #stream=False,\n",
    "                #stop=stop_sequences,\n",
    "            #)\n",
    "        #data = result[\"choices\"][0]\n",
    "        #text = data.get(\"text\", data.get(\"message\", data.get(\"delta\")))\n",
    "        #text = text if isinstance(text, str) else text.get(\"content\")\n",
    "        #response = text\n",
    "\n",
    "        response = self.llm.prompt(\n",
    "            messages,\n",
    "            stop=stop_sequences or [],\n",
    "            max_tokens = self.maxlength,\n",
    "            **kwargs\n",
    "        )\n",
    "        #print(f'RESPONSE: {response}')\n",
    "\n",
    "        # Remove stop sequences from LLM output\n",
    "        if stop_sequences is not None:\n",
    "            response = remove_stop_sequences(response, stop_sequences)\n",
    "        \n",
    "        # Create and return a ChatMessage with the response\n",
    "        message =  ChatMessage(role=\"assistant\", content=response)\n",
    "\n",
    "        # Extract first tool action\n",
    "        if tools_to_call_from:\n",
    "            message.tool_calls = [\n",
    "                get_tool_call_from_text(\n",
    "                    re.sub(r\".*?Action:(.*?\\n\\}).*\", r\"\\1\", response, flags=re.DOTALL), self.tool_name_key, self.tool_arguments_key\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        return message\n",
    "\n",
    "\n",
    "    def parameters(self, maxlength):\n",
    "        \"\"\"\n",
    "        Set LLM inference parameters.\n",
    "\n",
    "        Args:\n",
    "            maxlength: maximum sequence length\n",
    "        \"\"\"\n",
    "\n",
    "        self.maxlength = maxlength\n",
    "\n",
    "\n",
    "    def clean(self, messages):\n",
    "        \"\"\"\n",
    "        Gets a clean message list.\n",
    "\n",
    "        Args:\n",
    "            messages: input messages\n",
    "\n",
    "        Returns:\n",
    "            clean messages\n",
    "        \"\"\"\n",
    "\n",
    "        # Get clean message list\n",
    "        messages = get_clean_message_list(messages, role_conversions=tool_role_conversions, flatten_messages_as_text=self.flatten_messages_as_text)\n",
    "\n",
    "        # Ensure all roles are strings and not enums for compability across LLM frameworks\n",
    "        for message in messages:\n",
    "            if \"role\" in message:\n",
    "                message[\"role\"] = message[\"role\"].value if isinstance(message[\"role\"], Enum) else message[\"role\"]\n",
    "\n",
    "        return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
