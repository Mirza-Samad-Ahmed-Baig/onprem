{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelines.agent.model\n",
    "\n",
    "> Agent-friendly wrapper around and LLM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines.agent.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "import json\n",
    "import re\n",
    "from smolagents.models import Model, ChatMessage, MessageRole,  ChatMessageToolCall, ChatMessageToolCallDefinition\n",
    "from smolagents.models import get_tool_call_from_text, remove_stop_sequences\n",
    "from smolagents import get_clean_message_list, tool_role_conversions\n",
    "from enum import Enum\n",
    "import onprem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class AgentModel(Model):\n",
    "    \"\"\"\n",
    "    A smolagents Model implementation that wraps an onprem LLM instance.\n",
    "    \n",
    "    This adapter allows onprem LLM instances to be used with smolagents Agents.\n",
    "    \n",
    "    Parameters:\n",
    "        llm (LLM): An instance of onprem.llm.base.LLM\n",
    "        model_id (str, optional): An identifier for the model\n",
    "        **kwargs: Additional keyword arguments to pass to the parent Model class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: onprem.LLM,\n",
    "        model_id: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Initialize the parent Model class\n",
    "        super().__init__(\n",
    "            model_id=model_id or f\"onprem-{llm.model_name}\",\n",
    "            **kwargs\n",
    "        )\n",
    "        # Store the LLM instance\n",
    "        self.llm = llm\n",
    "        # Set default keys for tool call extraction\n",
    "        self.tool_name_key = \"name\"\n",
    "        self.tool_arguments_key = \"arguments\"\n",
    "        \n",
    "    def generate(\n",
    "        self,\n",
    "        messages: List[Dict[str, Any] | ChatMessage],\n",
    "        stop_sequences: Optional[List[str]] = None,\n",
    "        response_format: Optional[Dict[str, str]] = None,\n",
    "        tools_to_call_from: Optional[List[Any]] = None,\n",
    "        **kwargs\n",
    "    ) -> ChatMessage:\n",
    "        \"\"\"\n",
    "        Process the input messages and return the model's response.\n",
    "        \n",
    "        Parameters:\n",
    "            messages: A list of message dictionaries to be processed.\n",
    "            stop_sequences: A list of strings that will stop generation if encountered.\n",
    "            response_format: The response format to use in the model's response.\n",
    "            tools_to_call_from: A list of tools that the model can use.\n",
    "            **kwargs: Additional keyword arguments to pass to the LLM.\n",
    "            \n",
    "        Returns:\n",
    "            ChatMessage: A chat message object containing the model's response.\n",
    "        \"\"\"\n",
    "        # Convert smolagents messages to a format that onprem LLM can use\n",
    "        messages = self.clean(messages)\n",
    "\n",
    "        \n",
    "        # Call the LLM with the processed messages\n",
    "        response = self.llm.prompt(\n",
    "            messages,\n",
    "            stop=stop_sequences or [],\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "        #print(f'RESPONSE: {response}')\n",
    "\n",
    "        # Remove stop sequences from LLM output\n",
    "        if stop_sequences is not None:\n",
    "            response = remove_stop_sequences(response, stop_sequences)\n",
    "        \n",
    "        # Create and return a ChatMessage with the response\n",
    "        message =  ChatMessage(\n",
    "            role=MessageRole.ASSISTANT,\n",
    "            content=response,\n",
    "            raw={\"response\": response},\n",
    "            token_usage=None\n",
    "        )\n",
    "\n",
    "        if tools_to_call_from:\n",
    "            try:\n",
    "                # First try to extract tool call using Action: format\n",
    "                tool_data = self.extract_json_after_action(response)\n",
    "                # Preprocess tool arguments to handle malformed schema-like structures\n",
    "                tool_data = self.preprocess_tool_arguments(tool_data)\n",
    "                tool_call = get_tool_call_from_text(\n",
    "                    tool_data, self.tool_name_key, self.tool_arguments_key\n",
    "                )\n",
    "                message.tool_calls = [tool_call]\n",
    "            except Exception as e:\n",
    "                # If Action: format fails, try to extract tool call from \"Called Tool:\" format\n",
    "                try:\n",
    "                    tool_data = self.extract_called_tool_format(response)\n",
    "                    tool_call = get_tool_call_from_text(\n",
    "                        tool_data, self.tool_name_key, self.tool_arguments_key\n",
    "                    )\n",
    "                    message.tool_calls = [tool_call]\n",
    "                except Exception as e2:\n",
    "                    # Last resort: try direct extraction\n",
    "                    try:\n",
    "                        tool_call = get_tool_call_from_text(\n",
    "                            response, self.tool_name_key, self.tool_arguments_key\n",
    "                        )\n",
    "                        message.tool_calls = [tool_call]\n",
    "                    except Exception as e3:\n",
    "                        print(\"[!] Failed to extract tool call:\", e)\n",
    "                        print(\"Error while parsing tool call from model output:\", e)\n",
    "                        print(\"JSON blob was:\", response)\n",
    "                        \n",
    "                        # Fallback: if no Action found, try to generate final_answer from content\n",
    "                        if \"No 'Action:' keyword found\" in str(e) and response.strip():\n",
    "                            try:\n",
    "                                # Create a final answer tool call with the response content\n",
    "                                final_answer_data = json.dumps({\n",
    "                                    \"name\": \"final_answer\",\n",
    "                                    \"arguments\": {\"answer\": response.strip()}\n",
    "                                })\n",
    "                                tool_call = get_tool_call_from_text(\n",
    "                                    final_answer_data, self.tool_name_key, self.tool_arguments_key\n",
    "                                )\n",
    "                                message.tool_calls = [tool_call]\n",
    "                                print(\"[!] Fallback: Generated final_answer from response content\")\n",
    "                            except Exception as fallback_e:\n",
    "                                print(\"[!] Fallback failed:\", fallback_e)\n",
    "\n",
    "        return message\n",
    "\n",
    "\n",
    "\n",
    "    def extract_json_after_action(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the first JSON object that appears after 'Action:' and return it as a string.\n",
    "        \n",
    "        This method handles the format:\n",
    "        Action: \n",
    "        {\n",
    "          \"name\": \"tool_name\",\n",
    "          \"arguments\": {}\n",
    "        }\n",
    "        \n",
    "        Observation: ...\n",
    "        \"\"\"\n",
    "        import json\n",
    "        import ast\n",
    "        \n",
    "        # Ensure we're working with a string\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            \n",
    "        if \"Action:\" not in text:\n",
    "            raise ValueError(\"No 'Action:' keyword found.\")\n",
    "\n",
    "        after_action = text.split(\"Action:\", 1)[1].strip()\n",
    "        \n",
    "        # Handle the case where there might be an \"Observation:\" section\n",
    "        # Split on \"Observation:\" to get just the JSON part\n",
    "        if \"Observation:\" in after_action:\n",
    "            after_action = after_action.split(\"Observation:\", 1)[0].strip()\n",
    "        \n",
    "        # Try to find a complete JSON block from here\n",
    "        start_idx = after_action.find(\"{\")\n",
    "        if start_idx == -1:\n",
    "            raise ValueError(\"No JSON object found after 'Action:'.\")\n",
    "\n",
    "        # Bracket matching to find the full JSON object\n",
    "        stack = []\n",
    "        for i, c in enumerate(after_action[start_idx:], start=start_idx):\n",
    "            if c == '{':\n",
    "                stack.append('{')\n",
    "            elif c == '}':\n",
    "                if not stack:\n",
    "                    raise ValueError(\"Unmatched closing brace in tool call JSON.\")\n",
    "                stack.pop()\n",
    "                if not stack:\n",
    "                    json_str = after_action[start_idx:i + 1]\n",
    "                    try:\n",
    "                        # Validate JSON and return as string\n",
    "                        parsed = json.loads(json_str)\n",
    "                        return json_str\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Fallback to ast.literal_eval for single quotes\n",
    "                        try:\n",
    "                            parsed = ast.literal_eval(json_str)\n",
    "                            return json.dumps(parsed)\n",
    "                        except (ValueError, SyntaxError):\n",
    "                            raise ValueError(f\"Invalid JSON format: {json_str}\")\n",
    "\n",
    "        raise ValueError(\"Unmatched braces in tool call JSON.\")\n",
    "\n",
    "    def extract_called_tool_format(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract tool call from \"Called Tool:\" format.\n",
    "        \n",
    "        Handles format like:\n",
    "        Called Tool: 'tool_name' with arguments: {'arg1': 'value1'}\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import json\n",
    "        \n",
    "        # Pattern to match \"Called Tool: 'tool_name' with arguments: {arguments}\"\n",
    "        pattern = r\"Called Tool:\\s*['\\\"]([^'\\\"]+)['\\\"] with arguments:\\s*(\\{.*?\\})\"\n",
    "        match = re.search(pattern, text)\n",
    "        \n",
    "        if not match:\n",
    "            raise ValueError(\"No 'Called Tool:' format found.\")\n",
    "        \n",
    "        tool_name = match.group(1)\n",
    "        arguments_str = match.group(2)\n",
    "        \n",
    "        try:\n",
    "            # Parse the arguments as JSON (handling single quotes)\n",
    "            arguments_str = arguments_str.replace(\"'\", '\"')\n",
    "            arguments = json.loads(arguments_str)\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON parsing fails, try ast.literal_eval\n",
    "            import ast\n",
    "            try:\n",
    "                arguments = ast.literal_eval(match.group(2))\n",
    "            except (ValueError, SyntaxError):\n",
    "                raise ValueError(f\"Invalid arguments format: {match.group(2)}\")\n",
    "        \n",
    "        # Create the expected JSON format\n",
    "        tool_data = {\n",
    "            \"name\": tool_name,\n",
    "            \"arguments\": arguments\n",
    "        }\n",
    "        \n",
    "        return json.dumps(tool_data)\n",
    "\n",
    "    def preprocess_tool_arguments(self, tool_data: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess tool arguments to handle malformed schema-like structures and incorrect formats.\n",
    "        \n",
    "        Handles multiple cases:\n",
    "        1. Schema-like arguments with type/description/value structure\n",
    "        2. final_answer with string arguments instead of object with 'answer' field\n",
    "        3. Other malformed argument structures\n",
    "        \"\"\"\n",
    "        import json\n",
    "        \n",
    "        try:\n",
    "            # Parse the JSON\n",
    "            data = json.loads(tool_data)\n",
    "            \n",
    "            # Special handling for final_answer tool\n",
    "            if data.get(\"name\") == \"final_answer\":\n",
    "                if \"arguments\" in data:\n",
    "                    args = data[\"arguments\"]\n",
    "                    # If arguments is a string, wrap it in the expected format\n",
    "                    if isinstance(args, str):\n",
    "                        data[\"arguments\"] = {\"answer\": args}\n",
    "                    # If arguments is a dict but missing 'answer' key, try to fix it\n",
    "                    elif isinstance(args, dict) and \"answer\" not in args:\n",
    "                        # If there's only one value, use it as the answer\n",
    "                        if len(args) == 1:\n",
    "                            data[\"arguments\"] = {\"answer\": list(args.values())[0]}\n",
    "                        else:\n",
    "                            # Convert the whole dict to a string as the answer\n",
    "                            data[\"arguments\"] = {\"answer\": json.dumps(args)}\n",
    "            \n",
    "            # Check if arguments exist and need preprocessing for schema-like structures\n",
    "            elif \"arguments\" in data and isinstance(data[\"arguments\"], dict):\n",
    "                processed_args = {}\n",
    "                \n",
    "                for key, value in data[\"arguments\"].items():\n",
    "                    # Check if this argument has schema-like structure with 'value' field\n",
    "                    if isinstance(value, dict) and \"value\" in value:\n",
    "                        # Extract just the value\n",
    "                        processed_args[key] = value[\"value\"]\n",
    "                    else:\n",
    "                        # Keep as-is\n",
    "                        processed_args[key] = value\n",
    "                \n",
    "                # Update the arguments\n",
    "                data[\"arguments\"] = processed_args\n",
    "                \n",
    "            # Return the processed JSON as string\n",
    "            return json.dumps(data)\n",
    "            \n",
    "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "            # If preprocessing fails, return original data\n",
    "            print(f\"[!] Warning: Failed to preprocess tool arguments: {e}\")\n",
    "            return tool_data\n",
    "\n",
    "    def clean(self, messages):\n",
    "        \"\"\"\n",
    "        Gets a clean message list.\n",
    "\n",
    "        Args:\n",
    "            messages: input messages\n",
    "\n",
    "        Returns:\n",
    "            clean messages\n",
    "        \"\"\"\n",
    "\n",
    "        # Get clean message list\n",
    "        messages = get_clean_message_list(messages, role_conversions=tool_role_conversions, flatten_messages_as_text=self.flatten_messages_as_text)\n",
    "\n",
    "        # Ensure all roles are strings and not enums for compability across LLM frameworks\n",
    "        for message in messages:\n",
    "            if \"role\" in message:\n",
    "                message[\"role\"] = message[\"role\"].value if isinstance(message[\"role\"], Enum) else message[\"role\"]\n",
    "\n",
    "        return messages\n",
    "\n",
    "\n",
    "    def _process_messages(self, messages: List[Dict[str, Any] | ChatMessage]) -> str:\n",
    "        \"\"\"\n",
    "        Convert smolagents messages to a format suitable for onprem LLM.\n",
    "        \n",
    "        For now, this concatenates all messages into a single string prompt.\n",
    "        \n",
    "        Parameters:\n",
    "            messages: A list of message dictionaries or ChatMessage objects.\n",
    "            \n",
    "        Returns:\n",
    "            str: A formatted prompt string for the LLM.\n",
    "        \"\"\"\n",
    "        # Process each message and combine them\n",
    "        processed_parts = []\n",
    "        \n",
    "        for msg in messages:\n",
    "            # Handle ChatMessage objects\n",
    "            if isinstance(msg, ChatMessage):\n",
    "                role = msg.role\n",
    "                content = msg.content or \"\"\n",
    "                \n",
    "                # Handle tool calls if present\n",
    "                if msg.tool_calls:\n",
    "                    tool_calls_str = json.dumps([tc.dict() for tc in msg.tool_calls], indent=2)\n",
    "                    content = f\"{content}\\nTool Calls: {tool_calls_str}\"\n",
    "            else:\n",
    "                # Handle dictionary format\n",
    "                role = msg[\"role\"]\n",
    "                content = msg.get(\"content\", \"\")\n",
    "                \n",
    "                # Handle tool calls if present in dictionary format\n",
    "                if \"tool_calls\" in msg and msg[\"tool_calls\"]:\n",
    "                    tool_calls_str = json.dumps(msg[\"tool_calls\"], indent=2)\n",
    "                    content = f\"{content}\\nTool Calls: {tool_calls_str}\"\n",
    "            \n",
    "            # Format based on role\n",
    "            if role == MessageRole.USER:\n",
    "                processed_parts.append(f\"User: {content}\")\n",
    "            elif role == MessageRole.ASSISTANT:\n",
    "                processed_parts.append(f\"Assistant: {content}\")\n",
    "            elif role == MessageRole.SYSTEM:\n",
    "                processed_parts.append(f\"System: {content}\")\n",
    "            elif role == MessageRole.TOOL_CALL:\n",
    "                processed_parts.append(f\"Tool Call: {content}\")\n",
    "            elif role == MessageRole.TOOL_RESPONSE:\n",
    "                processed_parts.append(f\"Tool Response: {content}\")\n",
    "        \n",
    "        # Combine all parts with newlines\n",
    "        return \"\\n\\n\".join(processed_parts)\n",
    "\n",
    "\n",
    "    def _build_final_answer_json(self, answer: str) -> str:\n",
    "        data = {\n",
    "            \"name\": \"final_answer\",\n",
    "            \"arguments\": {\n",
    "            \"answer\": answer\n",
    "            }\n",
    "        }\n",
    "        import json\n",
    "        return json.dumps(data, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
