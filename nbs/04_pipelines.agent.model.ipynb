{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelines.agent.model\n",
    "\n",
    "> Agent-friendly wrapper around and LLM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines.agent.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "import json\n",
    "import re\n",
    "from smolagents.models import Model, ChatMessage, MessageRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "import json\n",
    "from smolagents.models import Model, ChatMessage, MessageRole\n",
    "from smolagents.models import get_tool_call_from_text, remove_stop_sequences\n",
    "from onprem import LLM\n",
    "\n",
    "class AgentModel(Model):\n",
    "    \"\"\"\n",
    "    A smolagents Model implementation that wraps an onprem LLM instance.\n",
    "    \n",
    "    This adapter allows onprem LLM instances to be used with smolagents Agents.\n",
    "    \n",
    "    Parameters:\n",
    "        llm (LLM): An instance of onprem.llm.base.LLM\n",
    "        model_id (str, optional): An identifier for the model\n",
    "        **kwargs: Additional keyword arguments to pass to the parent Model class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: LLM,\n",
    "        model_id: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Initialize the parent Model class\n",
    "        super().__init__(\n",
    "            model_id=model_id or f\"onprem-{llm.model_name}\",\n",
    "            **kwargs\n",
    "        )\n",
    "        # Store the LLM instance\n",
    "        self.llm = llm\n",
    "        \n",
    "    def generate(\n",
    "        self,\n",
    "        messages: List[Dict[str, Any] | ChatMessage],\n",
    "        stop_sequences: Optional[List[str]] = None,\n",
    "        response_format: Optional[Dict[str, str]] = None,\n",
    "        tools_to_call_from: Optional[List[Any]] = None,\n",
    "        **kwargs\n",
    "    ) -> ChatMessage:\n",
    "        \"\"\"\n",
    "        Process the input messages and return the model's response.\n",
    "        \n",
    "        Parameters:\n",
    "            messages: A list of message dictionaries to be processed.\n",
    "            stop_sequences: A list of strings that will stop generation if encountered.\n",
    "            response_format: The response format to use in the model's response.\n",
    "            tools_to_call_from: A list of tools that the model can use.\n",
    "            **kwargs: Additional keyword arguments to pass to the LLM.\n",
    "            \n",
    "        Returns:\n",
    "            ChatMessage: A chat message object containing the model's response.\n",
    "        \"\"\"\n",
    "        # Convert smolagents messages to a format that onprem LLM can use\n",
    "        processed_messages = self._process_messages(messages)\n",
    "        \n",
    "        # Call the LLM with the processed messages\n",
    "        response = self.llm.prompt(\n",
    "            processed_messages,\n",
    "            stop=stop_sequences or [],\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Remove stop sequences from LLM output\n",
    "        if stop_sequences is not None:\n",
    "            response = remove_stop_sequences(response, stop_sequences)\n",
    "        \n",
    "        # Create and return a ChatMessage with the response\n",
    "        message =  ChatMessage(\n",
    "            role=MessageRole.ASSISTANT,\n",
    "            content=response,\n",
    "            raw={\"response\": response},\n",
    "            token_usage=None  # onprem LLM doesn't track tokens in a way we can use here\n",
    "        )\n",
    "        if tools_to_call_from:\n",
    "            message.tool_calls = [\n",
    "                get_tool_call_from_text(\n",
    "                    re.sub(r\".*?Action:(.*?\\n\\}).*\", r\"\\1\", response, flags=re.DOTALL), self.tool_name_key, self.tool_arguments_key\n",
    "                )\n",
    "            ]\n",
    "        return message\n",
    "    \n",
    "    def _process_messages(self, messages: List[Dict[str, Any] | ChatMessage]) -> str:\n",
    "        \"\"\"\n",
    "        Convert smolagents messages to a format suitable for onprem LLM.\n",
    "        \n",
    "        For now, this concatenates all messages into a single string prompt.\n",
    "        \n",
    "        Parameters:\n",
    "            messages: A list of message dictionaries or ChatMessage objects.\n",
    "            \n",
    "        Returns:\n",
    "            str: A formatted prompt string for the LLM.\n",
    "        \"\"\"\n",
    "        # Process each message and combine them\n",
    "        processed_parts = []\n",
    "        \n",
    "        for msg in messages:\n",
    "            # Handle ChatMessage objects\n",
    "            if isinstance(msg, ChatMessage):\n",
    "                role = msg.role\n",
    "                content = msg.content or \"\"\n",
    "                \n",
    "                # Handle tool calls if present\n",
    "                if msg.tool_calls:\n",
    "                    tool_calls_str = json.dumps([tc.dict() for tc in msg.tool_calls], indent=2)\n",
    "                    content = f\"{content}\\nTool Calls: {tool_calls_str}\"\n",
    "            else:\n",
    "                # Handle dictionary format\n",
    "                role = msg[\"role\"]\n",
    "                content = msg.get(\"content\", \"\")\n",
    "                \n",
    "                # Handle tool calls if present in dictionary format\n",
    "                if \"tool_calls\" in msg and msg[\"tool_calls\"]:\n",
    "                    tool_calls_str = json.dumps(msg[\"tool_calls\"], indent=2)\n",
    "                    content = f\"{content}\\nTool Calls: {tool_calls_str}\"\n",
    "            \n",
    "            # Format based on role\n",
    "            if role == MessageRole.USER:\n",
    "                processed_parts.append(f\"User: {content}\")\n",
    "            elif role == MessageRole.ASSISTANT:\n",
    "                processed_parts.append(f\"Assistant: {content}\")\n",
    "            elif role == MessageRole.SYSTEM:\n",
    "                processed_parts.append(f\"System: {content}\")\n",
    "            elif role == MessageRole.TOOL_CALL:\n",
    "                processed_parts.append(f\"Tool Call: {content}\")\n",
    "            elif role == MessageRole.TOOL_RESPONSE:\n",
    "                processed_parts.append(f\"Tool Response: {content}\")\n",
    "        \n",
    "        # Combine all parts with newlines\n",
    "        return \"\\n\\n\".join(processed_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
