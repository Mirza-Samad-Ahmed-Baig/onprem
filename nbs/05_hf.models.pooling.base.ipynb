{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hf.models.pooling.base\n",
    "\n",
    "> Pooling module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp hf.models.pooling.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Pooling module\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from onprem.hf.models.models import Models\n",
    "\n",
    "\n",
    "class Pooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds pooled vectors usings outputs from a transformers model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, device, tokenizer=None, maxlength=None, modelargs=None):\n",
    "        \"\"\"\n",
    "        Creates a new Pooling model.\n",
    "\n",
    "        Args:\n",
    "            path: path to model, accepts Hugging Face model hub id or local path\n",
    "            device: tensor device id\n",
    "            tokenizer: optional path to tokenizer\n",
    "            maxlength: max sequence length\n",
    "            modelargs: additional model arguments\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = Models.load(path, modelargs=modelargs)\n",
    "        self.tokenizer = Models.tokenizer(tokenizer if tokenizer else path)\n",
    "        self.device = Models.device(device)\n",
    "\n",
    "        # Detect unbounded tokenizer typically found in older models\n",
    "        Models.checklength(self.model, self.tokenizer)\n",
    "\n",
    "        # Set max length\n",
    "        self.maxlength = maxlength if maxlength else self.tokenizer.model_max_length if self.tokenizer.model_max_length != int(1e30) else None\n",
    "\n",
    "        # Move to device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def encode(self, documents, batch=32):\n",
    "        \"\"\"\n",
    "        Builds an array of pooled embeddings for documents.\n",
    "\n",
    "        Args:\n",
    "            documents: list of documents used to build embeddings\n",
    "            batch: model batch size\n",
    "\n",
    "        Returns:\n",
    "            pooled embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        # Split documents into batches and process\n",
    "        results = []\n",
    "\n",
    "        # Sort document indices from largest to smallest to enable efficient batching\n",
    "        # This performance tweak matches logic in sentence-transformers\n",
    "        lengths = np.argsort([-len(x) if x else 0 for x in documents])\n",
    "        documents = [documents[x] for x in lengths]\n",
    "\n",
    "        for chunk in self.chunk(documents, batch):\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(chunk, padding=True, truncation=\"longest_first\", return_tensors=\"pt\", max_length=self.maxlength)\n",
    "\n",
    "            # Move inputs to device\n",
    "            inputs = inputs.to(self.device)\n",
    "\n",
    "            # Run inputs through model\n",
    "            with torch.no_grad():\n",
    "                outputs = self.forward(**inputs)\n",
    "\n",
    "            # Add batch result\n",
    "            results.extend(outputs.cpu().numpy())\n",
    "\n",
    "        # Restore original order and return array\n",
    "        return np.asarray([results[x] for x in np.argsort(lengths)])\n",
    "\n",
    "    def chunk(self, texts, size):\n",
    "        \"\"\"\n",
    "        Splits texts into separate batch sizes specified by size.\n",
    "\n",
    "        Args:\n",
    "            texts: text elements\n",
    "            size: batch size\n",
    "\n",
    "        Returns:\n",
    "            list of evenly sized batches with the last batch having the remaining elements\n",
    "        \"\"\"\n",
    "\n",
    "        return [texts[x : x + size] for x in range(0, len(texts), size)]\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        \"\"\"\n",
    "        Runs inputs through transformers model and returns outputs.\n",
    "\n",
    "        Args:\n",
    "            inputs: model inputs\n",
    "\n",
    "        Returns:\n",
    "            model outputs\n",
    "        \"\"\"\n",
    "\n",
    "        return self.model(**inputs)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
