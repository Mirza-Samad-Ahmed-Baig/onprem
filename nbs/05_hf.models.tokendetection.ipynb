{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hf.models.tokendetection\n",
    "\n",
    "> Token Detection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp hf.models.tokendetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Token Detection module\n",
    "\"\"\"\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "class TokenDetection(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    Runs the replaced token detection training objective. This method was first proposed by the ELECTRA model.\n",
    "    The method consists of a masked language model generator feeding data to a discriminator that determines\n",
    "    which of the tokens are incorrect. More on this training objective can be found in the ELECTRA paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, generator, discriminator, tokenizer, weight=50.0):\n",
    "        \"\"\"\n",
    "        Creates a new TokenDetection class.\n",
    "\n",
    "        Args:\n",
    "            generator: Generator model, must be a masked language model\n",
    "            discriminator: Discriminator model, must be a model that can detect replaced tokens. Any model can\n",
    "                           can be customized for this task. See ElectraForPretraining for more.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize model with discriminator config\n",
    "        super().__init__(discriminator.config)\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "        # Tokenizer to save with generator and discriminator\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Discriminator weight\n",
    "        self.weight = weight\n",
    "\n",
    "        # Share embeddings if both models are the same type\n",
    "        # Embeddings must be same size\n",
    "        if self.generator.config.model_type == self.discriminator.config.model_type:\n",
    "            self.discriminator.set_input_embeddings(self.generator.get_input_embeddings())\n",
    "\n",
    "        # Set attention mask present flags\n",
    "        self.gattention = \"attention_mask\" in inspect.signature(self.generator.forward).parameters\n",
    "        self.dattention = \"attention_mask\" in inspect.signature(self.discriminator.forward).parameters\n",
    "\n",
    "    # pylint: disable=E1101\n",
    "    def forward(self, input_ids=None, labels=None, attention_mask=None, token_type_ids=None):\n",
    "        \"\"\"\n",
    "        Runs a forward pass through the model. This method runs the masked language model then randomly samples\n",
    "        the generated tokens and builds a binary classification problem for the discriminator (detecting if each token is correct).\n",
    "\n",
    "        Args:\n",
    "            input_ids: token ids\n",
    "            labels: token labels\n",
    "            attention_mask: attention mask\n",
    "            token_type_ids: segment token indices\n",
    "\n",
    "        Returns:\n",
    "            (loss, generator outputs, discriminator outputs, discriminator labels)\n",
    "        \"\"\"\n",
    "\n",
    "        # Copy input ids\n",
    "        dinputs = input_ids.clone()\n",
    "\n",
    "        # Run inputs through masked language model\n",
    "        inputs = {\"attention_mask\": attention_mask} if self.gattention else {}\n",
    "        goutputs = self.generator(input_ids, labels=labels, token_type_ids=token_type_ids, **inputs)\n",
    "\n",
    "        # Get predictions\n",
    "        preds = torch.softmax(goutputs[1], dim=-1)\n",
    "        preds = preds.view(-1, self.config.vocab_size)\n",
    "\n",
    "        tokens = torch.multinomial(preds, 1).view(-1)\n",
    "        tokens = tokens.view(dinputs.shape[0], -1)\n",
    "\n",
    "        # Labels have a -100 value to ignore loss from unchanged tokens\n",
    "        mask = labels.ne(-100)\n",
    "\n",
    "        # Replace the masked out tokens of the input with the generator predictions\n",
    "        dinputs[mask] = tokens[mask]\n",
    "\n",
    "        # Turn mask into new target labels - 1 (True) for corrupted, 0 otherwise.\n",
    "        # If the prediction was correct, mark it as uncorrupted.\n",
    "        correct = tokens == labels\n",
    "        dlabels = mask.long()\n",
    "        dlabels[correct] = 0\n",
    "\n",
    "        # Run token classification, predict whether each token was corrupted\n",
    "        inputs = {\"attention_mask\": attention_mask} if self.dattention else {}\n",
    "        doutputs = self.discriminator(dinputs, labels=dlabels, token_type_ids=token_type_ids, **inputs)\n",
    "\n",
    "        # Compute combined loss\n",
    "        loss = goutputs[0] + self.weight * doutputs[0]\n",
    "        return loss, goutputs[1], doutputs[1], dlabels\n",
    "\n",
    "    def save_pretrained(self, output, state_dict=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Saves current model to output directory.\n",
    "\n",
    "        Args:\n",
    "            output: output directory\n",
    "            state_dict: model state\n",
    "            kwargs: additional keyword arguments\n",
    "        \"\"\"\n",
    "\n",
    "        # Save combined model to support training from checkpoints\n",
    "        super().save_pretrained(output, state_dict, **kwargs)\n",
    "\n",
    "        # Save generator tokenizer and model\n",
    "        gpath = os.path.join(output, \"generator\")\n",
    "        self.tokenizer.save_pretrained(gpath)\n",
    "        self.generator.save_pretrained(gpath)\n",
    "\n",
    "        # Save discriminator tokenizer and model\n",
    "        dpath = os.path.join(output, \"discriminator\")\n",
    "        self.tokenizer.save_pretrained(dpath)\n",
    "        self.discriminator.save_pretrained(dpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
