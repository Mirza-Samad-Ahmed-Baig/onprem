{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hf.hfmodel\n",
    "\n",
    "> HF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp hf.hfmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from onprem.hf.models import Models\n",
    "from onprem.hf.tensors import Tensors\n",
    "\n",
    "\n",
    "class HFModel(Tensors):\n",
    "    \"\"\"\n",
    "    Pipeline backed by a Hugging Face Transformers model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path=None, quantize=False, gpu=False, batch=64):\n",
    "        \"\"\"\n",
    "        Creates a new HFModel.\n",
    "\n",
    "        Args:\n",
    "            path: optional path to model, accepts Hugging Face model hub id or local path,\n",
    "                  uses default model for task if not provided\n",
    "            quantize: if model should be quantized, defaults to False\n",
    "            gpu: True/False if GPU should be enabled, also supports a GPU device id\n",
    "            batch: batch size used to incrementally process content\n",
    "        \"\"\"\n",
    "\n",
    "        # Default model path\n",
    "        self.path = path\n",
    "\n",
    "        # Quantization flag\n",
    "        self.quantization = quantize\n",
    "\n",
    "        # Get tensor device reference\n",
    "        self.deviceid = Models.deviceid(gpu)\n",
    "        self.device = Models.device(self.deviceid)\n",
    "\n",
    "        # Process batch size\n",
    "        self.batchsize = batch\n",
    "\n",
    "    def prepare(self, model):\n",
    "        \"\"\"\n",
    "        Prepares a model for processing. Applies dynamic quantization if necessary.\n",
    "\n",
    "        Args:\n",
    "            model: input model\n",
    "\n",
    "        Returns:\n",
    "            model\n",
    "        \"\"\"\n",
    "\n",
    "        if self.deviceid == -1 and self.quantization:\n",
    "            model = self.quantize(model)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def tokenize(self, tokenizer, texts):\n",
    "        \"\"\"\n",
    "        Tokenizes text using tokenizer. This method handles overflowing tokens and automatically splits\n",
    "        them into separate elements. Indices of each element is returned to allow reconstructing the\n",
    "        transformed elements after running through the model.\n",
    "\n",
    "        Args:\n",
    "            tokenizer: Tokenizer\n",
    "            texts: list of text\n",
    "\n",
    "        Returns:\n",
    "            (tokenization result, indices)\n",
    "        \"\"\"\n",
    "\n",
    "        # Pre-process and split on newlines\n",
    "        batch, positions = [], []\n",
    "        for x, text in enumerate(texts):\n",
    "            elements = [t + \" \" for t in text.split(\"\\n\") if t]\n",
    "            batch.extend(elements)\n",
    "            positions.extend([x] * len(elements))\n",
    "\n",
    "        # Run tokenizer\n",
    "        tokens = tokenizer(batch, padding=True)\n",
    "\n",
    "        inputids, attention, indices = [], [], []\n",
    "        for x, ids in enumerate(tokens[\"input_ids\"]):\n",
    "            if len(ids) > tokenizer.model_max_length:\n",
    "                # Remove padding characters, if any\n",
    "                ids = [i for i in ids if i != tokenizer.pad_token_id]\n",
    "\n",
    "                # Split into model_max_length chunks\n",
    "                for chunk in self.batch(ids, tokenizer.model_max_length - 1):\n",
    "                    # Append EOS token if necessary\n",
    "                    if chunk[-1] != tokenizer.eos_token_id:\n",
    "                        chunk.append(tokenizer.eos_token_id)\n",
    "\n",
    "                    # Set attention mask\n",
    "                    mask = [1] * len(chunk)\n",
    "\n",
    "                    # Append padding if necessary\n",
    "                    if len(chunk) < tokenizer.model_max_length:\n",
    "                        pad = tokenizer.model_max_length - len(chunk)\n",
    "                        chunk.extend([tokenizer.pad_token_id] * pad)\n",
    "                        mask.extend([0] * pad)\n",
    "\n",
    "                    inputids.append(chunk)\n",
    "                    attention.append(mask)\n",
    "                    indices.append(positions[x])\n",
    "            else:\n",
    "                inputids.append(ids)\n",
    "                attention.append(tokens[\"attention_mask\"][x])\n",
    "                indices.append(positions[x])\n",
    "\n",
    "        tokens = {\"input_ids\": inputids, \"attention_mask\": attention}\n",
    "\n",
    "        # pylint: disable=E1102\n",
    "        return ({name: self.tensor(tensor).to(self.device) for name, tensor in tokens.items()}, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
