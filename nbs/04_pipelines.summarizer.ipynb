{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelines.summarizer\n",
    "\n",
    "> Pipelines for specific tasks like summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines.summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union, Callable\n",
    "import numpy as np\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from onprem.ingest import load_single_document, load_documents\n",
    "from onprem.utils import segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "DEFAULT_MAP_PROMPT = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please write a concise summary. \n",
    "CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "DEFAULT_REDUCE_PROMPT = \"\"\"The following is set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary. \n",
    "SUMMARY:\"\"\"\n",
    "\n",
    "DEFAULT_BASE_REFINE_PROMPT = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "DEFAULT_REFINE_PROMPT = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary.\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "\n",
    "TARGET_PROMPT= \"\"\"What does the following context say with respect \"{concept_description}\"? \\n\\nCONTEXT:\\n{text}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class Summarizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        prompt_template: Optional[str] = None,              \n",
    "        map_prompt: Optional[str] = None,\n",
    "        reduce_prompt: Optional[str] = None,\n",
    "        refine_prompt: Optional[str] = None, \n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `Summarizer` summarizes one or more documents\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *llm*: An `onprem.LLM` object\n",
    "        - *prompt_template*: A model specific prompt_template with a single placeholder named \"{prompt}\".\n",
    "                             All prompts (e.g., Map-Reduce prompts) are wrapped within this prompt.\n",
    "                             If supplied, overrides the `prompt_template` supplied to the `LLM` constructor.\n",
    "        - *map_prompt*: Map prompt for Map-Reduce summarization. If None, default is used.\n",
    "        - *reduce_prompt*: Reduce prompt for Map-Reduce summarization. If None, default is used.\n",
    "        - *refine_prompt*: Refine prompt for Refine-based summarization. If None, default is used.\n",
    "\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.prompt_template = prompt_template if prompt_template is not None else llm.prompt_template\n",
    "        self.map_prompt = map_prompt if map_prompt else DEFAULT_MAP_PROMPT\n",
    "        self.reduce_prompt = reduce_prompt if reduce_prompt else DEFAULT_REDUCE_PROMPT\n",
    "        self.refine_prompt = refine_prompt if refine_prompt else DEFAULT_REFINE_PROMPT\n",
    "\n",
    "\n",
    "    def summarize(self, \n",
    "                  fpath:str, #  path to either a folder of documents or a single file\n",
    "                  strategy:str='map_reduce', # One of {'map_reduce', 'refine'}\n",
    "                  chunk_size:int=1000, # Number of characters of each chunk to summarize\n",
    "                  chunk_overlap:int=0, # Number of characters that overlap between chunks\n",
    "                  token_max:int=2000, # Maximum number of tokens to group documents into\n",
    "                  max_chunks_to_use: Optional[int] = None, # Maximum number of chunks (starting from beginning) to use\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Summarize one or more documents (e.g., PDFs, MS Word, MS Powerpoint, plain text)\n",
    "        using either Langchain's Map-Reduce strategy or Refine strategy.\n",
    "        The `max_chunks` parameter may be useful for documents that have abstracts or informative introductions. \n",
    "        If `max_chunks=None`, all chunks are considered for summarizer.\n",
    "        \"\"\"\n",
    "          \n",
    "        if os.path.isfile(fpath):\n",
    "            docs = load_single_document(fpath)\n",
    "        else:\n",
    "            docs = load_documents(fpath)\n",
    "\n",
    "        if strategy == 'map_reduce':\n",
    "            summary = self._map_reduce(docs, \n",
    "                                      chunk_size=chunk_size, \n",
    "                                      chunk_overlap=chunk_overlap, \n",
    "                                      token_max=token_max,\n",
    "                                      max_chunks_to_use=max_chunks_to_use)\n",
    "        elif strategy == 'refine':\n",
    "            summary = self._refine(docs, \n",
    "                                   chunk_size=chunk_size, \n",
    "                                   chunk_overlap=chunk_overlap, \n",
    "                                   token_max=token_max,\n",
    "                                   max_chunks_to_use=max_chunks_to_use)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f'Unknown strategy: {self.strategy}')\n",
    "        return summary\n",
    "\n",
    "    \n",
    "    def _map_reduce(self, docs, chunk_size=1000, chunk_overlap=0, token_max=1000, \n",
    "                    max_chunks_to_use = None, **kwargs):\n",
    "        \"\"\" Map-Reduce summarization\"\"\"\n",
    "        langchain_llm = self.llm.llm\n",
    "\n",
    "        # Map\n",
    "        # map_template = \"\"\"The following is a set of documents\n",
    "        # {docs}\n",
    "        # Based on this list of docs, please identify the main themes \n",
    "        # Helpful Answer:\"\"\"\n",
    "        # map_template = \"\"\"The following is a set of documents\n",
    "        # {docs}\n",
    "        # Based on this list of docs, please write a concise summary.\n",
    "        # CONCISE SUMMARY:\"\"\"\n",
    "        map_template = self.map_prompt\n",
    "        if self.prompt_template: \n",
    "            map_template = self.prompt_template.format(**{'prompt':map_template})\n",
    "        map_prompt = PromptTemplate.from_template(map_template)\n",
    "        map_chain = LLMChain(llm=langchain_llm, prompt=map_prompt)\n",
    "\n",
    "        # Reduce\n",
    "        # reduce_template = \"\"\"The following is set of summaries:\n",
    "        # {docs}\n",
    "        # Take these and distill it into a final, consolidated summary. \n",
    "        # SUMMARY:\"\"\"\n",
    "        reduce_template = self.reduce_prompt\n",
    "        if self.prompt_template:\n",
    "            reduce_template = self.prompt_template.format(**{'prompt':reduce_template})\n",
    "        reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "        # Run chain\n",
    "        reduce_chain = LLMChain(llm=langchain_llm, prompt=reduce_prompt)\n",
    "        \n",
    "        # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "        combine_documents_chain = StuffDocumentsChain(\n",
    "            llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    "        )\n",
    "        \n",
    "        # Combines and iteravely reduces the mapped documents\n",
    "        reduce_documents_chain = ReduceDocumentsChain(\n",
    "            # This is final chain that is called.\n",
    "            combine_documents_chain=combine_documents_chain,\n",
    "            # If documents exceed context for `StuffDocumentsChain`\n",
    "            collapse_documents_chain=combine_documents_chain,\n",
    "            # The maximum number of tokens to group documents into.\n",
    "            token_max=token_max)\n",
    "\n",
    "        # Combining documents by mapping a chain over them, then combining results\n",
    "        map_reduce_chain = MapReduceDocumentsChain(\n",
    "            # Map chain\n",
    "            llm_chain=map_chain,\n",
    "            # Reduce chain\n",
    "            reduce_documents_chain=reduce_documents_chain,\n",
    "            # The variable name in the llm_chain to put the documents in\n",
    "            document_variable_name=\"docs\",\n",
    "            # Return the results of the map steps in the output\n",
    "            return_intermediate_steps=False,\n",
    "        )\n",
    "        \n",
    "        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        split_docs = split_docs[:max_chunks_to_use] if max_chunks_to_use else split_docs\n",
    "\n",
    "        return map_reduce_chain.invoke(split_docs)\n",
    "\n",
    "    def _refine(self, docs, chunk_size=1000, chunk_overlap=0, \n",
    "                max_chunks_to_use = None, **kwargs):\n",
    "        \"\"\" Refine summarization\"\"\"\n",
    "\n",
    "        # initial_template = \"\"\"Write a concise summary of the following:\n",
    "        # {text}\n",
    "        # CONCISE SUMMARY:\"\"\"\n",
    "        initial_template = self.refine_base_prompt\n",
    "        if self.prompt_template:\n",
    "            initial_template = self.prompt_template.format(**{'prompt':initial_template})\n",
    "        prompt = PromptTemplate.from_template(initial_template)\n",
    "        \n",
    "        # refine_template = (\n",
    "        #     \"Your job is to produce a final summary\\n\"\n",
    "        #     \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "        #     \"We have the opportunity to refine the existing summary\"\n",
    "        #     \"(only if needed) with some more context below.\\n\"\n",
    "        #     \"------------\\n\"\n",
    "        #     \"{text}\\n\"\n",
    "        #     \"------------\\n\"\n",
    "        #     \"Given the new context, refine the original summary.\"\n",
    "        #     \"If the context isn't useful, return the original summary.\"\n",
    "        # )\n",
    "        refine_template = self.refine_prompt\n",
    "        if self.prompt_template:\n",
    "            refine_template = self.prompt_template.format(**{'prompt':refine_template})\n",
    "        refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "        chain = load_summarize_chain(\n",
    "            llm=self.llm.llm,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=prompt,\n",
    "            refine_prompt=refine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            input_key=\"input_documents\",\n",
    "            output_key=\"output_text\",\n",
    "        )\n",
    "        \n",
    "        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        split_docs = split_docs[:max_chunks_to_use] if max_chunks_to_use else split_docs\n",
    "        result = chain({\"input_documents\": split_docs}, return_only_outputs=True)\n",
    "        return result['output_text']\n",
    "        \n",
    "    def summarize_by_concept(self,\n",
    "                            fpath:str, # path to file\n",
    "                            concept_description:str, # Summaries are generated with respect to the described concept.\n",
    "                            similarity_threshold:float=0.0, # Minimum similarity for consideration. Tip: Increase this when using similarity_method=\"senttransform\" to mitigate hallucination. A value of 0.0 is sufficient for TF-IDF or should be kept near-zero.\n",
    "                            max_chunks:int=4, # Only this many snippets above similarity_threshold are considered.\n",
    "                            similarity_method:str=\"tfidf\", # One of \"senttransform\" (sentence-transformer embeddings) or \"tfidf\" (TF-IDF)\n",
    "                            summary_prompt:str = TARGET_PROMPT, # The prompt used for summarization. Should have exactly two variables, {concept_description} and {text}.\n",
    "                            ):\n",
    "        \"\"\"\n",
    "        Summarize document with respect to concept described by `concept_description`. Returns a tuple of the form (summary, sources).\n",
    "        \"\"\"\n",
    "        include_surrounding=False # not used\n",
    "        if similarity_method not in ['tfidf', 'senttransform']:\n",
    "            raise ValueError('similarity_method must be one of {\"tifidf\", \"senttransform\"}')\n",
    "        \n",
    "        # Read in text\n",
    "        if not os.path.isfile(fpath):\n",
    "            raise ValueError(f\"{fpath} is not a file.\")\n",
    "        docs = load_single_document(fpath)\n",
    "        ext = \".\" + fpath.rsplit(\".\", 1)[-1].lower()\n",
    "        content = '\\n\\n'.join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Chunk text\n",
    "        paragraphs = segment(content, maxchars=2000, unit=\"paragraph\")\n",
    "        # Combine paragraphs if very short (i.e. a header)\n",
    "        chunks = []\n",
    "        text = \"\"\n",
    "        count = 0 \n",
    "        max_combine = 2\n",
    "        for p in paragraphs:\n",
    "            text += f\"\\n\\n{p}\" \n",
    "            count += 1\n",
    "            if count == max_combine+1 or len(text)>100:\n",
    "                chunks.append(text)\n",
    "                count = 0 \n",
    "                text = \"\"\n",
    "        \n",
    "        # Remove duplicate chunks\n",
    "        chunks = list(set(chunks))\n",
    "            \n",
    "        # TF-IDF method to find relevant sections of the documents\n",
    "        if similarity_method==\"tfidf\":\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                            ngram_range=(1,5),\n",
    "                            stop_words=\"english\",\n",
    "                            max_features=10000,\n",
    "                            min_df=0.,\n",
    "                            max_df=0.95,\n",
    "                        )\n",
    "        \n",
    "            result = vectorizer.fit_transform([concept_description] + chunks)\n",
    "            cos = cosine_similarity(result[0:1], result).flatten()\n",
    "\n",
    "            direct_word_chunks = {}\n",
    "            for index, c in enumerate(chunks):\n",
    "                if cos[index+1] <= similarity_threshold: continue\n",
    "                update_c = {\n",
    "                    \"chunk\": c, \n",
    "                    \"index\": index, \n",
    "                    \"cosSim\": cos[index+1],\n",
    "                }\n",
    "                direct_word_chunks[index] = update_c\n",
    "                \n",
    "                \n",
    "        # Sentence Transformer method to find relevant sections of the documents\n",
    "        elif similarity_method==\"senttransform\":\n",
    "            \n",
    "            from sentence_transformers import SentenceTransformer, util\n",
    "            # Sentence transformer model\n",
    "            k = None # Optional parameter to restrict number\n",
    "            st_model = SentenceTransformer(self.llm.embedding_model_name)\n",
    "            chunk_embedding = st_model.encode(chunks, convert_to_tensor=True)\n",
    "            phrase_embedding = st_model.encode(concept_description, convert_to_tensor=True)\n",
    "            top_k = k if k is not None else chunk_embedding.shape[0]\n",
    "            cos_scores = util.pytorch_cos_sim(phrase_embedding, chunk_embedding)[0]\n",
    "            # sort cosine similarity scores\n",
    "            try:\n",
    "                top_results = np.argpartition(-cos_scores.cpu(), range(top_k))[0:top_k]\n",
    "            except:\n",
    "                top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "        \n",
    "            # Get the best chunks of text \n",
    "            direct_word_chunks = {}\n",
    "            for index, c in enumerate(chunks):\n",
    "                score = float(cos_scores[index].cpu())\n",
    "                if score <= similarity_threshold: continue\n",
    "                update_c = {\n",
    "                    \"chunk\": c,\n",
    "                    \"index\": index,\n",
    "                    \"cosSim\": score\n",
    "                }\n",
    "                direct_word_chunks[index] = update_c\n",
    "            \n",
    "        # Sort the list of chunks by the cosine similarity\n",
    "        sorted_chunks = sorted(\n",
    "            direct_word_chunks.items(), \n",
    "            key=lambda x:x[1][\"cosSim\"],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        # Select the chunks (with relevant surrounding chunks)    \n",
    "        selected_ids = [s[0] for s in sorted_chunks[0:max_chunks]]\n",
    "        if include_surrounding:\n",
    "            ids_in_context = get_surrounding_chunks(\n",
    "                selected_ids, \n",
    "                chunks, \n",
    "                context_size=1, \n",
    "                check_energy=False\n",
    "            )\n",
    "        else:\n",
    "            ids_in_context = selected_ids\n",
    "        \n",
    "        # Get the text to use\n",
    "        target_text = \"\"\n",
    "        for ids in ids_in_context: \n",
    "            target_text += f\"{chunks[ids].strip()}\\n\"\n",
    "            \n",
    "        # prompt the LLM to summarize energy parts \n",
    "        response = \"\"\n",
    "        if target_text: \n",
    "            response = self.llm.prompt(summary_prompt.format(**{\"text\": target_text, \n",
    "                                                                \"concept_description\":concept_description}))\n",
    "        else:\n",
    "            response = f'No text relevant to \"{concept_description}\" in document.'\n",
    "        return response, sorted_chunks[:max_chunks]\n",
    "\n",
    "\n",
    "def get_surrounding_chunks(selected_ids, chunks, context_size=1, check_energy=False): \n",
    "    ids_to_use = []\n",
    "    for ids in selected_ids:\n",
    "        id_range = list(np.arange(\n",
    "            max(ids-context_size,0), \n",
    "            min(ids+context_size+1,len(chunks))\n",
    "        ))\n",
    "        if check_energy:\n",
    "            updated_ids = [i for i in id_range if \"energy\" in chunks[i].lower() or i==ids]\n",
    "        else:\n",
    "            updated_ids = id_range\n",
    "        ids_to_use.extend(updated_ids)\n",
    "    return sorted(list(set(ids_to_use)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/summarizer.py#L85){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Summarizer.summarize\n",
       "\n",
       ">      Summarizer.summarize (fpath:str, strategy:str='map_reduce',\n",
       ">                            chunk_size:int=1000, chunk_overlap:int=0,\n",
       ">                            token_max:int=2000,\n",
       ">                            max_chunks_to_use:Optional[int]=None)\n",
       "\n",
       "*Summarize one or more documents (e.g., PDFs, MS Word, MS Powerpoint, plain text)\n",
       "using either Langchain's Map-Reduce strategy or Refine strategy.\n",
       "The `max_chunks` parameter may be useful for documents that have abstracts or informative introductions. \n",
       "If `max_chunks=None`, all chunks are considered for summarizer.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fpath | str |  | path to either a folder of documents or a single file |\n",
       "| strategy | str | map_reduce | One of {'map_reduce', 'refine'} |\n",
       "| chunk_size | int | 1000 | Number of characters of each chunk to summarize |\n",
       "| chunk_overlap | int | 0 | Number of characters that overlap between chunks |\n",
       "| token_max | int | 2000 | Maximum number of tokens to group documents into |\n",
       "| max_chunks_to_use | Optional | None | Maximum number of chunks (starting from beginning) to use |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/summarizer.py#L85){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Summarizer.summarize\n",
       "\n",
       ">      Summarizer.summarize (fpath:str, strategy:str='map_reduce',\n",
       ">                            chunk_size:int=1000, chunk_overlap:int=0,\n",
       ">                            token_max:int=2000,\n",
       ">                            max_chunks_to_use:Optional[int]=None)\n",
       "\n",
       "*Summarize one or more documents (e.g., PDFs, MS Word, MS Powerpoint, plain text)\n",
       "using either Langchain's Map-Reduce strategy or Refine strategy.\n",
       "The `max_chunks` parameter may be useful for documents that have abstracts or informative introductions. \n",
       "If `max_chunks=None`, all chunks are considered for summarizer.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fpath | str |  | path to either a folder of documents or a single file |\n",
       "| strategy | str | map_reduce | One of {'map_reduce', 'refine'} |\n",
       "| chunk_size | int | 1000 | Number of characters of each chunk to summarize |\n",
       "| chunk_overlap | int | 0 | Number of characters that overlap between chunks |\n",
       "| token_max | int | 2000 | Maximum number of tokens to group documents into |\n",
       "| max_chunks_to_use | Optional | None | Maximum number of chunks (starting from beginning) to use |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Summarizer.summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/summarizer.py#L248){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Summarizer.summarize_by_concept\n",
       "\n",
       ">      Summarizer.summarize_by_concept (fpath:str, concept_description:str,\n",
       ">                                       similarity_threshold:float=0.0,\n",
       ">                                       max_chunks:int=4,\n",
       ">                                       similarity_method:str='tfidf',\n",
       ">                                       include_surrounding:bool=False,\n",
       ">                                       summary_prompt:str='What does the\n",
       ">                                       context say with respect\n",
       ">                                       \"{concept_description}\"?\n",
       ">                                       \\n\\nCONTEXT:\\n{text}')\n",
       "\n",
       "*Summarize document with respect to concept described by `concept_description`. Returns a tuple of the form (summary, sources).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fpath | str |  | path to file |\n",
       "| concept_description | str |  | Summaries are generated with respect to the described concept. |\n",
       "| similarity_threshold | float | 0.0 | Minimum similarity for consideration. Tip: Increase this when using similarity_method=\"senttransform\" to mitigate hallucination. A value of 0.0 is sufficient for TF-IDF or should be kept near-zero. |\n",
       "| max_chunks | int | 4 | Only this many snippets above similarity_threshold are considered. |\n",
       "| similarity_method | str | tfidf | One of \"senttransform\" (sentence-transformer embeddings) or \"tfidf\" (TF-IDF) |\n",
       "| include_surrounding | bool | False | If True, consider surrounding text. |\n",
       "| summary_prompt | str | What does the context say with respect \"{concept_description}\"? <br><br>CONTEXT:<br>{text} | The prompt used for summarization. Should have exactly two variables, {concept_description} and {text}. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/summarizer.py#L248){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Summarizer.summarize_by_concept\n",
       "\n",
       ">      Summarizer.summarize_by_concept (fpath:str, concept_description:str,\n",
       ">                                       similarity_threshold:float=0.0,\n",
       ">                                       max_chunks:int=4,\n",
       ">                                       similarity_method:str='tfidf',\n",
       ">                                       include_surrounding:bool=False,\n",
       ">                                       summary_prompt:str='What does the\n",
       ">                                       context say with respect\n",
       ">                                       \"{concept_description}\"?\n",
       ">                                       \\n\\nCONTEXT:\\n{text}')\n",
       "\n",
       "*Summarize document with respect to concept described by `concept_description`. Returns a tuple of the form (summary, sources).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fpath | str |  | path to file |\n",
       "| concept_description | str |  | Summaries are generated with respect to the described concept. |\n",
       "| similarity_threshold | float | 0.0 | Minimum similarity for consideration. Tip: Increase this when using similarity_method=\"senttransform\" to mitigate hallucination. A value of 0.0 is sufficient for TF-IDF or should be kept near-zero. |\n",
       "| max_chunks | int | 4 | Only this many snippets above similarity_threshold are considered. |\n",
       "| similarity_method | str | tfidf | One of \"senttransform\" (sentence-transformer embeddings) or \"tfidf\" (TF-IDF) |\n",
       "| include_surrounding | bool | False | If True, consider surrounding text. |\n",
       "| summary_prompt | str | What does the context say with respect \"{concept_description}\"? <br><br>CONTEXT:<br>{text} | The prompt used for summarization. Should have exactly two variables, {concept_description} and {text}. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Summarizer.summarize_by_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
