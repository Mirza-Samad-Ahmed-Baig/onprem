{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hf.train.hftrainer\n",
    "\n",
    "> Hugging Face Transformers trainer wrapper module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp hf.train.hftrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Hugging Face Transformers trainer wrapper module\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForPreTraining,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq, Trainer, set_seed\n",
    "from transformers import TrainingArguments as HFTrainingArguments\n",
    "\n",
    "# Conditional import\n",
    "try:\n",
    "    from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "    # pylint: disable=C0412\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    PEFT = True\n",
    "except ImportError:\n",
    "    PEFT = False\n",
    "\n",
    "from onprem.hf.data import Labels, Questions, Sequences, Texts\n",
    "from onprem.hf.models.models import Models\n",
    "from onprem.hf.models.tokendetection import TokenDetection\n",
    "from onprem.hf.tensors import Tensors\n",
    "\n",
    "\n",
    "class HFTrainer(Tensors):\n",
    "    \"\"\"\n",
    "    Trains a new Hugging Face Transformer model using the Trainer framework.\n",
    "    \"\"\"\n",
    "\n",
    "    # pylint: disable=R0913\n",
    "    def __call__(\n",
    "        self,\n",
    "        base,\n",
    "        train,\n",
    "        validation=None,\n",
    "        columns=None,\n",
    "        maxlength=None,\n",
    "        stride=128,\n",
    "        task=\"text-classification\",\n",
    "        prefix=None,\n",
    "        metrics=None,\n",
    "        tokenizers=None,\n",
    "        checkpoint=None,\n",
    "        quantize=None,\n",
    "        lora=None,\n",
    "        **args\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Builds a new Hugging Face transformers model using arguments.\n",
    "\n",
    "        Args:\n",
    "            base: path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple\n",
    "            train: training data\n",
    "            validation: validation data\n",
    "            columns: tuple of columns to use for text/label, defaults to (text, None, label)\n",
    "            maxlength: maximum sequence length, defaults to tokenizer.model_max_length\n",
    "            stride: chunk size for splitting data for QA tasks\n",
    "            task: optional model task or category, determines the model type, defaults to \"text-classification\"\n",
    "            prefix: optional source prefix\n",
    "            metrics: optional function that computes and returns a dict of evaluation metrics\n",
    "            tokenizers: optional number of concurrent tokenizers, defaults to None\n",
    "            checkpoint: optional resume from checkpoint flag or path to checkpoint directory, defaults to None\n",
    "            quantize: quantization configuration to pass to base model\n",
    "            lora: lora configuration to pass to PEFT model\n",
    "            args: training arguments\n",
    "\n",
    "        Returns:\n",
    "            (model, tokenizer)\n",
    "        \"\"\"\n",
    "\n",
    "        # Quantization / LoRA support\n",
    "        if (quantize or lora) and not PEFT:\n",
    "            raise ImportError('PEFT is not available: pip install peft')\n",
    "\n",
    "        # Parse TrainingArguments\n",
    "        args = self.parse(args)\n",
    "\n",
    "        # Set seed for model reproducibility\n",
    "        set_seed(args.seed)\n",
    "\n",
    "        # Load model configuration, tokenizer and max sequence length\n",
    "        config, tokenizer, maxlength = self.load(base, maxlength)\n",
    "\n",
    "        # Default tokenizer pad token if it's not set\n",
    "        tokenizer.pad_token = tokenizer.pad_token if tokenizer.pad_token is not None else tokenizer.eos_token\n",
    "\n",
    "        # Prepare parameters\n",
    "        process, collator, labels = self.prepare(task, train, tokenizer, columns, maxlength, stride, prefix, args)\n",
    "\n",
    "        # Tokenize training and validation data\n",
    "        train, validation = process(train, validation, os.cpu_count() if tokenizers and isinstance(tokenizers, bool) else tokenizers)\n",
    "\n",
    "        # Create model to train\n",
    "        model = self.model(task, base, config, labels, tokenizer, quantize)\n",
    "\n",
    "        # Default config pad token if it's not set\n",
    "        model.config.pad_token_id = model.config.pad_token_id if model.config.pad_token_id is not None else model.config.eos_token_id\n",
    "\n",
    "        # Load as PEFT model, if necessary\n",
    "        model = self.peft(task, lora, model)\n",
    "\n",
    "        # Add model to collator\n",
    "        if collator:\n",
    "            collator.model = model\n",
    "\n",
    "        # Build trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=collator,\n",
    "            args=args,\n",
    "            train_dataset=train,\n",
    "            eval_dataset=validation if validation else None,\n",
    "            compute_metrics=metrics,\n",
    "        )\n",
    "\n",
    "        # Run training\n",
    "        trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "        # Run evaluation\n",
    "        if validation:\n",
    "            trainer.evaluate()\n",
    "\n",
    "        # Save model outputs\n",
    "        if args.should_save:\n",
    "            trainer.save_model()\n",
    "            trainer.save_state()\n",
    "\n",
    "        # Put model in eval mode to disable weight updates and return (model, tokenizer)\n",
    "        return (model.eval(), tokenizer)\n",
    "\n",
    "    def parse(self, updates):\n",
    "        \"\"\"\n",
    "        Parses and merges custom arguments with defaults.\n",
    "\n",
    "        Args:\n",
    "            updates: custom arguments\n",
    "\n",
    "        Returns:\n",
    "            TrainingArguments\n",
    "        \"\"\"\n",
    "\n",
    "        # Default training arguments\n",
    "        args = {\"output_dir\": \"\", \"save_strategy\": \"no\", \"report_to\": \"none\", \"log_level\": \"warning\", \"use_cpu\": not Models.hasaccelerator()}\n",
    "\n",
    "        # Apply custom arguments\n",
    "        args.update(updates)\n",
    "\n",
    "        return TrainingArguments(**args)\n",
    "\n",
    "    def load(self, base, maxlength):\n",
    "        \"\"\"\n",
    "        Loads the base config and tokenizer.\n",
    "\n",
    "        Args:\n",
    "            base: base model - supports a file path or (model, tokenizer) tuple\n",
    "            maxlength: maximum sequence length\n",
    "\n",
    "        Returns:\n",
    "            (config, tokenizer, maxlength)\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(base, (list, tuple)):\n",
    "            # Unpack existing config and tokenizer\n",
    "            model, tokenizer = base\n",
    "            config = model.config\n",
    "        else:\n",
    "            # Load config\n",
    "            config = AutoConfig.from_pretrained(base)\n",
    "\n",
    "            # Load tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base)\n",
    "\n",
    "        # Detect unbounded tokenizer\n",
    "        Models.checklength(config, tokenizer)\n",
    "\n",
    "        # Derive max sequence length\n",
    "        maxlength = min(maxlength if maxlength else sys.maxsize, tokenizer.model_max_length)\n",
    "\n",
    "        return (config, tokenizer, maxlength)\n",
    "\n",
    "    def prepare(self, task, train, tokenizer, columns, maxlength, stride, prefix, args):\n",
    "        \"\"\"\n",
    "        Prepares data for model training.\n",
    "\n",
    "        Args:\n",
    "            task: optional model task or category, determines the model type, defaults to \"text-classification\"\n",
    "            train: training data\n",
    "            tokenizer: model tokenizer\n",
    "            columns: tuple of columns to use for text/label, defaults to (text, None, label)\n",
    "            maxlength: maximum sequence length, defaults to tokenizer.model_max_length\n",
    "            stride: chunk size for splitting data for QA tasks\n",
    "            prefix: optional source prefix\n",
    "            args: training arguments\n",
    "        \"\"\"\n",
    "\n",
    "        process, collator, labels = None, None, None\n",
    "\n",
    "        if task == \"language-generation\":\n",
    "            process = Texts(tokenizer, columns, maxlength)\n",
    "            collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, pad_to_multiple_of=8 if args.fp16 else None)\n",
    "        elif task in (\"language-modeling\", \"token-detection\"):\n",
    "            process = Texts(tokenizer, columns, maxlength)\n",
    "            collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n",
    "        elif task == \"question-answering\":\n",
    "            process = Questions(tokenizer, columns, maxlength, stride)\n",
    "        elif task == \"sequence-sequence\":\n",
    "            process = Sequences(tokenizer, columns, maxlength, prefix)\n",
    "            collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n",
    "        else:\n",
    "            process = Labels(tokenizer, columns, maxlength)\n",
    "            labels = process.labels(train)\n",
    "\n",
    "        return process, collator, labels\n",
    "\n",
    "    def model(self, task, base, config, labels, tokenizer, quantize):\n",
    "        \"\"\"\n",
    "        Loads the base model to train.\n",
    "\n",
    "        Args:\n",
    "            task: optional model task or category, determines the model type, defaults to \"text-classification\"\n",
    "            base: base model - supports a file path or (model, tokenizer) tuple\n",
    "            config: model configuration\n",
    "            labels: number of labels\n",
    "            tokenizer: model tokenizer\n",
    "            quantize: quantization config\n",
    "\n",
    "        Returns:\n",
    "            model\n",
    "        \"\"\"\n",
    "\n",
    "        if labels is not None:\n",
    "            # Add number of labels to config\n",
    "            config.update({\"num_labels\": labels})\n",
    "\n",
    "        # Format quantization configuration\n",
    "        quantization = self.quantization(quantize)\n",
    "\n",
    "        # Clear quantization configuration if GPU is not available\n",
    "        quantization = quantization if torch.cuda.is_available() else None\n",
    "\n",
    "        # pylint: disable=E1120\n",
    "        # Unpack existing model or create new model from config\n",
    "        if isinstance(base, (list, tuple)) and not isinstance(base[0], str):\n",
    "            return base[0]\n",
    "        if task == \"language-generation\":\n",
    "            return AutoModelForCausalLM.from_pretrained(base, config=config, quantization_config=quantization)\n",
    "        if task == \"language-modeling\":\n",
    "            return AutoModelForMaskedLM.from_pretrained(base, config=config, quantization_config=quantization)\n",
    "        if task == \"question-answering\":\n",
    "            return AutoModelForQuestionAnswering.from_pretrained(base, config=config, quantization_config=quantization)\n",
    "        if task == \"sequence-sequence\":\n",
    "            return AutoModelForSeq2SeqLM.from_pretrained(base, config=config, quantization_config=quantization)\n",
    "        if task == \"token-detection\":\n",
    "            return TokenDetection(\n",
    "                AutoModelForMaskedLM.from_pretrained(base, config=config, quantization_config=quantization),\n",
    "                AutoModelForPreTraining.from_pretrained(base, config=config, quantization_config=quantization),\n",
    "                tokenizer,\n",
    "            )\n",
    "\n",
    "        # Default task\n",
    "        return AutoModelForSequenceClassification.from_pretrained(base, config=config, quantization_config=quantization)\n",
    "\n",
    "    def quantization(self, quantize):\n",
    "        \"\"\"\n",
    "        Formats and returns quantization configuration.\n",
    "\n",
    "        Args:\n",
    "            quantize: input quantization configuration\n",
    "\n",
    "        Returns:\n",
    "            formatted quantization configuration\n",
    "        \"\"\"\n",
    "\n",
    "        if quantize:\n",
    "            # Default quantization settings when set to True\n",
    "            if isinstance(quantize, bool):\n",
    "                quantize = {\n",
    "                    \"load_in_4bit\": True,\n",
    "                    \"bnb_4bit_use_double_quant\": True,\n",
    "                    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "                    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "                }\n",
    "\n",
    "            # Load dictionary configuration\n",
    "            if isinstance(quantize, dict):\n",
    "                quantize = BitsAndBytesConfig(**quantize)\n",
    "\n",
    "        return quantize if quantize else None\n",
    "\n",
    "    def peft(self, task, lora, model):\n",
    "        \"\"\"\n",
    "        Wraps the input model as a PEFT model if lora configuration is set.\n",
    "\n",
    "        Args:\n",
    "            task: optional model task or category, determines the model type, defaults to \"text-classification\"\n",
    "            lora: lora configuration\n",
    "            model: transformers model\n",
    "\n",
    "        Returns:\n",
    "            wrapped model if lora configuration set, otherwise input model is returned\n",
    "        \"\"\"\n",
    "\n",
    "        if lora:\n",
    "            # Format LoRA configuration\n",
    "            config = self.lora(task, lora)\n",
    "\n",
    "            # Wrap as PeftModel\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "            model = get_peft_model(model, config)\n",
    "            model.print_trainable_parameters()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def lora(self, task, lora):\n",
    "        \"\"\"\n",
    "        Formats and returns LoRA configuration.\n",
    "\n",
    "        Args:\n",
    "            task: optional model task or category, determines the model type, defaults to \"text-classification\"\n",
    "            lora: lora configuration\n",
    "\n",
    "        Returns:\n",
    "            formatted lora configuration\n",
    "        \"\"\"\n",
    "\n",
    "        if lora:\n",
    "            # Default lora settings when set to True\n",
    "            if isinstance(lora, bool):\n",
    "                lora = {\"r\": 16, \"lora_alpha\": 8, \"target_modules\": \"all-linear\", \"lora_dropout\": 0.05, \"bias\": \"none\"}\n",
    "\n",
    "            # Load dictionary configuration\n",
    "            if isinstance(lora, dict):\n",
    "                # Set task type if missing\n",
    "                if \"task_type\" not in lora:\n",
    "                    lora[\"task_type\"] = self.loratask(task)\n",
    "\n",
    "                lora = LoraConfig(**lora)\n",
    "\n",
    "        return lora\n",
    "\n",
    "    def loratask(self, task):\n",
    "        \"\"\"\n",
    "        Looks up the corresponding LoRA task for input task.\n",
    "\n",
    "        Args:\n",
    "            task: optional model task or category, determines the model type, defaults to \"text-classification\"\n",
    "\n",
    "        Returns:\n",
    "            lora task\n",
    "        \"\"\"\n",
    "\n",
    "        # Task mapping\n",
    "        tasks = {\n",
    "            \"language-generation\": TaskType.CAUSAL_LM,\n",
    "            \"language-modeling\": TaskType.FEATURE_EXTRACTION,\n",
    "            \"question-answering\": TaskType.QUESTION_ANS,\n",
    "            \"sequence-sequence\": TaskType.SEQ_2_SEQ_LM,\n",
    "            \"text-classification\": TaskType.SEQ_CLS,\n",
    "            \"token-detection\": TaskType.FEATURE_EXTRACTION,\n",
    "        }\n",
    "\n",
    "        # Default task\n",
    "        task = task if task in tasks else \"text-classification\"\n",
    "\n",
    "        # Lookup and return task\n",
    "        return tasks[task]\n",
    "\n",
    "\n",
    "class TrainingArguments(HFTrainingArguments):\n",
    "    \"\"\"\n",
    "    Extends standard TrainingArguments to make the output directory optional for transient models.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def should_save(self):\n",
    "        \"\"\"\n",
    "        Override should_save to disable model saving when output directory is None.\n",
    "\n",
    "        Returns:\n",
    "            If model should be saved\n",
    "        \"\"\"\n",
    "\n",
    "        return super().should_save if self.output_dir else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
