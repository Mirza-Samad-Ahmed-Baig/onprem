{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.base\n",
    "\n",
    "> functionality for text extraction and document ingestion into a vector database for question-answering and other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "from typing import List, Optional, Callable\n",
    "import multiprocessing\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters.base import Language\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    TextLoader,\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredPDFLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from onprem import utils as U\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logger = logging.getLogger('OnPrem.LLM-ingest')\n",
    "\n",
    "DEFAULT_CHUNK_SIZE = 500\n",
    "DEFAULT_CHUNK_OVERLAP = 50\n",
    "COLLECTION_NAME = \"onprem_chroma\"\n",
    "CHROMA_MAX = 41000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MyElmLoader(UnstructuredEmailLoader):\n",
    "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                doc = UnstructuredEmailLoader.load(self)\n",
    "            except ValueError as e:\n",
    "                if \"text/html content not found in email\" in str(e):\n",
    "                    # Try plain text\n",
    "                    self.unstructured_kwargs[\"content_source\"] = \"text/plain\"\n",
    "                    doc = UnstructuredEmailLoader.load(self)\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "        return doc\n",
    "\n",
    "\n",
    "class MyUnstructuredPDFLoader(UnstructuredPDFLoader):\n",
    "    \"\"\"Custom PDF Loader\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper UnstructuredPDFLoader\"\"\"\n",
    "        try:\n",
    "            docs = UnstructuredPDFLoader.load(self)\n",
    "            if not docs:\n",
    "                raise Exception('Document had no content. ')\n",
    "            page_content = '\\n'.join([d.metadata.get('text_as_html', d.page_content)\n",
    "                           if d.metadata.get('category', None) == 'Table' else d.page_content for d in docs])\n",
    "            source = docs[0].metadata['source']\n",
    "            doc = Document(page_content=page_content, metadata={'source':source})\n",
    "            return [doc]\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n",
    "class _PyMuPDFLoader(PyMuPDFLoader):\n",
    "    \"\"\"Custom PyMUPDF Loader with optional support for inferring table structure\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            # PyMuPDFLoader complains when you add custom flags to text_kwargs,\n",
    "            # so delete before loading\n",
    "            infer_table_structure = self.text_kwargs.get('infer_table_structure', False)\n",
    "            if 'infer_table_structure' in self.text_kwargs:\n",
    "                del self.text_kwargs['infer_table_structure']\n",
    "            docs = PyMuPDFLoader.load(self)\n",
    "            if infer_table_structure:\n",
    "                docs = self.extract_tables(docs)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n",
    "    def extract_tables(self, docs:List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Extract tables from PDF and append to end of Document list\n",
    "        \"\"\"\n",
    "        from onprem.ingest.pdftables import PDFTables\n",
    "        filepath = None if not docs else docs[0].metadata['source']\n",
    "        if not filepath: return docs\n",
    "        pdftab = PDFTables.from_file(filepath, verbose=False)\n",
    "        md_tables = pdftab.get_markdown_tables()\n",
    "        tabledocs = []\n",
    "        for md_table in md_tables:\n",
    "            tabledoc = Document(page_content=md_table, metadata={'source':self.file_path})\n",
    "            tabledocs.append(tabledoc)\n",
    "        docs.extend(tabledocs)\n",
    "        return docs\n",
    "\n",
    "\n",
    "class PDF2MarkdownLoader(_PyMuPDFLoader):\n",
    "    \"\"\"Custom PDF to Markdown Loader\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        import pymupdf4llm\n",
    "        try:\n",
    "            md_text = pymupdf4llm.to_markdown(self.file_path)\n",
    "            if not md_text.strip():\n",
    "                raise Exception('Document had no content. ')\n",
    "            doc = Document(page_content=md_text, metadata={'source':self.file_path, 'markdown':True})\n",
    "            docs = [doc]\n",
    "            if self.text_kwargs.get('infer_table_structure', False):\n",
    "                docs = self.extract_tables(docs)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Map file extensions to document loaders and their arguments\n",
    "PDFOCR = '.pdfOCR'\n",
    "PDFMD = '.pdfMD'\n",
    "PDF = '.pdf'\n",
    "PDF_EXTS = [PDF, PDFOCR, PDFMD]\n",
    "OCR_CHAR_THRESH = 32\n",
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".eml\": (MyElmLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "    PDF   : (_PyMuPDFLoader, {}),\n",
    "    PDFMD: (PDF2MarkdownLoader, {}),\n",
    "    PDFOCR: (MyUnstructuredPDFLoader, {\"infer_table_structure\":False, \"mode\":\"elements\", \"strategy\":\"hi_res\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n",
    "\n",
    "\n",
    "def extract_files(source_dir:str):\n",
    "    \"\"\"\n",
    "    Extract files of supported file types from folder.\n",
    "    \"\"\"\n",
    "    source_dir = os.path.abspath(source_dir)\n",
    "    all_files = []\n",
    "    for ext in LOADER_MAPPING:\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext.lower()}\"), recursive=True)\n",
    "        )\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext.upper()}\"), recursive=True)\n",
    "        )\n",
    "    return all_files\n",
    "\n",
    "\n",
    "def load_single_document(file_path: str, # path to file\n",
    "                         pdf_unstructured:bool=False, # use unstructured for PDF extraction if True (will also OCR if necessary)\n",
    "                         pdf_markdown:bool = False, # Convert PDFs to Markdown instead of plain text if True.\n",
    "                         **kwargs,\n",
    "                         ) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a single document. Will attempt to OCR PDFs, if necessary.\n",
    "\n",
    "\n",
    "    Note that extra kwargs can be supplied to configure the behavior of PDF loaders.\n",
    "    For instance, supplying `infer_table_structure` will cause `load_single_document` to try and\n",
    "    infer and extract tables from PDFs. When `pdf_unstructured=True` and `infer_table_structure=True`,\n",
    "    tables are represented as HTML within the main body of extracted text. In all other cases, inferred tables\n",
    "    are represented as Markdown and appended to the end of the extracted text when `infer_table_structure=True`.\n",
    "    \"\"\"\n",
    "    if pdf_unstructured and pdf_markdown:\n",
    "        raise ValueError('pdf_unstructured and pdf_markdown cannot both be True.')\n",
    "    file_path = os.path.abspath(file_path)\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1].lower()\n",
    "    if ext in LOADER_MAPPING:\n",
    "        try:\n",
    "            if ext == PDF:\n",
    "                if pdf_unstructured:\n",
    "                    ext = PDFOCR\n",
    "                elif pdf_markdown:\n",
    "                    ext = PDFMD\n",
    "            loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "            loader_args = loader_args.copy() # copy so any supplied kwargs do not persist across calls\n",
    "            if ext in PDF_EXTS:\n",
    "                loader_args.update(kwargs)\n",
    "            loader = loader_class(file_path, **loader_args)\n",
    "            if ext in PDF_EXTS and ext != PDFOCR:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "                    docs = loader.load()\n",
    "                if not docs or len('\\n'.join([d.page_content.strip() for d in docs]).strip()) < OCR_CHAR_THRESH:\n",
    "                    loader_class, loader_args = LOADER_MAPPING[PDFOCR]\n",
    "                    loader = loader_class(file_path, **loader_args)\n",
    "                    docs = loader.load()\n",
    "                    for doc in docs:\n",
    "                        doc.metadata = dict(doc.metadata, ocr=True)\n",
    "                return docs\n",
    "            else:\n",
    "                return loader.load()\n",
    "        except Exception as e:\n",
    "            logger.warning(f'\\nSkipping {file_path} due to error: {str(e)}')\n",
    "    else:\n",
    "        logger.warning(f\"\\nSkipping {file_path} due to unsupported file extension: '{ext}'\")\n",
    "\n",
    "\n",
    "def load_documents(source_dir: str, # path to folder containing documents\n",
    "                   ignored_files: List[str] = [], # list of filepaths to ignore\n",
    "                   ignore_fn:Optional[Callable] = None, # callable that accepts file path and returns True for ignored files\n",
    "                   pdf_unstructured:bool=False, # If True, use unstructured for PDF extraction\n",
    "                   **kwargs\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source documents directory, ignoring specified files.\n",
    "    Extra kwargs fed to `ingest.load_single_document`.\n",
    "    \"\"\"\n",
    "    all_files = extract_files(source_dir)\n",
    "\n",
    "    filtered_files = [\n",
    "        file_path for file_path in all_files if file_path not in ignored_files and not os.path.basename(file_path).startswith('~$')\n",
    "         and (ignore_fn is None or not ignore_fn(file_path))\n",
    "    ]\n",
    "\n",
    "    # Use \"spawn\" if using TableTransformers\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/40403\n",
    "    if kwargs.get('infer_table_structure', False):\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "    with multiprocessing.Pool(processes=os.cpu_count()) as pool:\n",
    "        results = []\n",
    "        with tqdm(\n",
    "            total=len(filtered_files), desc=\"Loading new documents\", ncols=80\n",
    "        ) as pbar:\n",
    "            for i, docs in enumerate(\n",
    "                pool.imap_unordered(functools.partial(load_single_document,\n",
    "                                                      pdf_unstructured=pdf_unstructured, **kwargs),\n",
    "                                    filtered_files)\n",
    "            ):\n",
    "                if docs is not None:\n",
    "                    results.extend(docs)\n",
    "                pbar.update()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_documents(\n",
    "    source_directory: str, # path to folder containing document store\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    ignored_files: List[str] = [], # list of files to ignore\n",
    "    ignore_fn:Optional[Callable] = None, # Callable that accepts the file path (including file name) as input and ignores if returns True\n",
    "    pdf_unstructured:bool=False, # If True, use unstructured for PDF extraction\n",
    "    **kwargs\n",
    "\n",
    "\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents and split in chunks.\n",
    "    Extra kwargs fed to `ingest.load_single_document`.\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from {source_directory}\")\n",
    "    documents = load_documents(source_directory,\n",
    "                              ignored_files, ignore_fn=ignore_fn,\n",
    "                              pdf_unstructured=pdf_unstructured, **kwargs)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        return\n",
    "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
    "\n",
    "    contains_markdown = False\n",
    "    if kwargs.get('pdf_markdown', False) or\\\n",
    "       (kwargs.get('infer_table_structure', False) and not kwargs.get('pdf_unstructured', False)):\n",
    "        contains_markdown = True\n",
    "    if contains_markdown:\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "            language=Language.MARKDOWN,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap)\n",
    "    else:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} chars each)\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "def does_vectorstore_exist(db) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    if not db.get()[\"documents\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def batchify_chunks(texts):\n",
    "    \"\"\"\n",
    "    split texts into batches specifically for Chroma\n",
    "    \"\"\"\n",
    "    split_docs_chunked = U.split_list(texts, CHROMA_MAX)\n",
    "    total_chunks = sum(1 for _ in U.split_list(texts, CHROMA_MAX))\n",
    "    return split_docs_chunked, total_chunks\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "DEFAULT_DB = \"vectordb\"\n",
    "\n",
    "\n",
    "class Ingester:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        embedding_model_kwargs: dict = {\"device\": \"cpu\"},\n",
    "        embedding_encode_kwargs: dict = {\"normalize_embeddings\": False},\n",
    "        persist_directory: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_folder` (previously-ingested documents are ignored)\n",
    "\n",
    "        **Args**:\n",
    "\n",
    "          - *embedding_model*: name of sentence-transformers model\n",
    "          - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`)\n",
    "          - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                       embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "          - *persist_directory*: Path to vector database (created if it doesn't exist).\n",
    "                                 Default is `onprem_data/vectordb` in user's home directory.\n",
    "\n",
    "\n",
    "        **Returns**: `None`\n",
    "        \"\"\"\n",
    "        self.persist_directory = persist_directory or os.path.join(\n",
    "            U.get_datadir(), DEFAULT_DB\n",
    "        )\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model_name,\n",
    "            model_kwargs=embedding_model_kwargs,\n",
    "            encode_kwargs=embedding_encode_kwargs,\n",
    "        )\n",
    "        self.chroma_settings = Settings(\n",
    "            persist_directory=self.persist_directory, anonymized_telemetry=False\n",
    "        )\n",
    "        self.chroma_client = chromadb.PersistentClient(\n",
    "            settings=self.chroma_settings, path=self.persist_directory\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Returns an instance to the `langchain_chroma.Chroma` instance\n",
    "        \"\"\"\n",
    "        db = Chroma(\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings,\n",
    "            client_settings=self.chroma_settings,\n",
    "            client=self.chroma_client,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            collection_name=COLLECTION_NAME,\n",
    "        )\n",
    "        return db if does_vectorstore_exist(db) else None\n",
    "\n",
    "    def get_embedding_model(self):\n",
    "        \"\"\"\n",
    "        Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance\n",
    "        \"\"\"\n",
    "        return self.embeddings\n",
    "\n",
    "\n",
    "    def get_ingested_files(self):\n",
    "        \"\"\"\n",
    "        Returns a list of files previously added to vector database (typically via `LLM.ingest`)\n",
    "        \"\"\"\n",
    "        return set([d['source'] for d in self.get_db().get()['metadatas']])\n",
    "\n",
    "\n",
    "    def store_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Stores instances of `langchain_core.documents.base.Document` in vectordb\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return\n",
    "        db = self.get_db()\n",
    "        if db:\n",
    "            print(\"Creating embeddings. May take some minutes...\")\n",
    "            chunk_batches, total_chunks = batchify_chunks(documents)\n",
    "            for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                db.add_documents(lst)\n",
    "        else:\n",
    "            chunk_batches, total_chunks = batchify_chunks(documents)\n",
    "            print(\"Creating embeddings. May take some minutes...\")\n",
    "            db = None\n",
    "\n",
    "            for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                if not db:\n",
    "                    db = Chroma.from_documents(\n",
    "                        lst,\n",
    "                        self.embeddings,\n",
    "                        persist_directory=self.persist_directory,\n",
    "                        client_settings=self.chroma_settings,\n",
    "                        client=self.chroma_client,\n",
    "                        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                        collection_name=COLLECTION_NAME,\n",
    "                    )\n",
    "                else:\n",
    "                    db.add_documents(lst)\n",
    "        return\n",
    "\n",
    "\n",
    "    def ingest(\n",
    "        self,\n",
    "        source_directory: str, # path to folder containing document store\n",
    "        chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n",
    "        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "        ignore_fn:Optional[Callable] = None, # Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested.\n",
    "        pdf_unstructured:bool=False, # If True, use unstructured for PDF extraction\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_directory` (previously-ingested documents are\n",
    "        ignored). When retrieved, the\n",
    "        [Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
    "        objects will each have a `metadata` dict with the absolute path to the file\n",
    "        in `metadata[\"source\"]`.\n",
    "        Extra kwargs fed to `ingest.load_single_document`.\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(source_directory):\n",
    "            raise ValueError(\"The source_directory does not exist.\")\n",
    "        elif os.path.isfile(source_directory):\n",
    "            raise ValueError(\n",
    "                \"The source_directory argument must be a folder, not a file.\"\n",
    "            )\n",
    "        texts = None\n",
    "        db = self.get_db()\n",
    "        if db:\n",
    "            # Update and store locally vectorstore\n",
    "            print(f\"Appending to existing vectorstore at {self.persist_directory}\")\n",
    "            collection = db.get()\n",
    "            ignored_files=[ metadata[\"source\"] for metadata in collection[\"metadatas\"]]\n",
    "        else:\n",
    "            print(f\"Creating new vectorstore at {self.persist_directory}\")\n",
    "            ignored_files = []\n",
    "\n",
    "        texts = process_documents(\n",
    "            source_directory,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            ignored_files=ignored_files,\n",
    "            ignore_fn=ignore_fn,\n",
    "            pdf_unstructured=pdf_unstructured,\n",
    "            **kwargs\n",
    "\n",
    "        )\n",
    "        self.store_documents(texts)\n",
    "\n",
    "        if texts:\n",
    "            print(\n",
    "                \"Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\"\n",
    "            )\n",
    "        db = None\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L273){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_embedding_model\n",
       "\n",
       ">      Ingester.get_embedding_model ()\n",
       "\n",
       "*Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L273){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_embedding_model\n",
       "\n",
       ">      Ingester.get_embedding_model ()\n",
       "\n",
       "*Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.get_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L259){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_db\n",
       "\n",
       ">      Ingester.get_db ()\n",
       "\n",
       "*Returns an instance to the `langchain_chroma.Chroma` instance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L259){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_db\n",
       "\n",
       ">      Ingester.get_db ()\n",
       "\n",
       "*Returns an instance to the `langchain_chroma.Chroma` instance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.get_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L280){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_ingested_files\n",
       "\n",
       ">      Ingester.get_ingested_files ()\n",
       "\n",
       "*Returns a list of files previously added to vector database (typically via `LLM.ingest`)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L280){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_ingested_files\n",
       "\n",
       ">      Ingester.get_ingested_files ()\n",
       "\n",
       "*Returns a list of files previously added to vector database (typically via `LLM.ingest`)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.get_ingested_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L319){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.ingest\n",
       "\n",
       ">      Ingester.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                       chunk_overlap:int=50, ignore_fn:Optional[Callable]=None)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L319){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.ingest\n",
       "\n",
       ">      Ingester.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                       chunk_overlap:int=50, ignore_fn:Optional[Callable]=None)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L287){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.store_documents\n",
       "\n",
       ">      Ingester.store_documents (documents)\n",
       "\n",
       "*Stores instances of `langchain_core.documents.base.Document` in vectordb*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L287){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.store_documents\n",
       "\n",
       ">      Ingester.store_documents (documents)\n",
       "\n",
       "*Stores instances of `langchain_core.documents.base.Document` in vectordb*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.store_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 11:35:20.660565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from sample_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 2/2 [00:00<00:00, 16.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 new documents from sample_data\n",
      "Split into 62 chunks of text (max. 500 chars each)\n",
      "Creating embeddings. May take some minutes...\n",
      "Ingestion complete! You can now query your documents using the LLM.ask method\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "ingester = Ingester()\n",
    "ingester.ingest(\"sample_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
