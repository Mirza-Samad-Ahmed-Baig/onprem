{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest\n",
    "\n",
    "> functionality for text extraction and document ingestion into a vector database for question-answering and other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from onprem.llm import helpers\n",
    "from onprem.utils import split_list, get_datadir\n",
    "from onprem.ingest.helpers import extract_files, extract_extension, extract_tables, includes_caption\n",
    "from onprem.ingest.helpers import md5sum, extract_mimetype, extract_file_dates\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters.base import Language\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    TextLoader,\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredPDFLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "from typing import List, Optional, Callable\n",
    "import multiprocessing\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logger = logging.getLogger('OnPrem.LLM-ingest')\n",
    "\n",
    "DEFAULT_CHUNK_SIZE = 500\n",
    "DEFAULT_CHUNK_OVERLAP = 50\n",
    "TABLE_CHUNK_SIZE = 2000\n",
    "COLLECTION_NAME = \"onprem_chroma\"\n",
    "CHROMA_MAX = 41000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MyElmLoader(UnstructuredEmailLoader):\n",
    "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                doc = UnstructuredEmailLoader.load(self)\n",
    "            except ValueError as e:\n",
    "                if \"text/html content not found in email\" in str(e):\n",
    "                    # Try plain text\n",
    "                    self.unstructured_kwargs[\"content_source\"] = \"text/plain\"\n",
    "                    doc = UnstructuredEmailLoader.load(self)\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "        return doc\n",
    "\n",
    "\n",
    "class MyUnstructuredPDFLoader(UnstructuredPDFLoader):\n",
    "    \"\"\"Custom PDF Loader\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper UnstructuredPDFLoader\"\"\"\n",
    "        try:\n",
    "            docs = UnstructuredPDFLoader.load(self)\n",
    "            if not docs:\n",
    "                raise Exception('Document had no content. ')\n",
    "            tables = [d.metadata['text_as_html'] for d in docs if d.metadata.get('text_as_html', None) is not None]\n",
    "            texts = [d.page_content for d in docs if d.metadata.get('text_as_html', None) is None]\n",
    "\n",
    "            page_content = '\\n'.join(texts)\n",
    "            source = docs[0].metadata['source']\n",
    "            docs = [Document(page_content=page_content, metadata={'source':source})]\n",
    "            table_docs = [Document(page_content=t,\n",
    "                                   metadata={'source':source, 'table':True}) for t in tables]\n",
    "            docs.extend(table_docs)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n",
    "class _PyMuPDFLoader(PyMuPDFLoader):\n",
    "    \"\"\"Custom PyMUPDF Loader with optional support for inferring table structure\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            # PyMuPDFLoader complains when you add custom flags to text_kwargs,\n",
    "            # so delete before loading\n",
    "            infer_table_structure = self.parser.text_kwargs.get('infer_table_structure', False)\n",
    "            if 'infer_table_structure' in self.parser.text_kwargs:\n",
    "                del self.parser.text_kwargs['infer_table_structure']\n",
    "            docs = PyMuPDFLoader.load(self)\n",
    "            if infer_table_structure:\n",
    "                docs = extract_tables(docs=docs)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n",
    "class PDF2MarkdownLoader(_PyMuPDFLoader):\n",
    "    \"\"\"Custom PDF to Markdown Loader\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        import pymupdf4llm\n",
    "        try:\n",
    "            md_text = pymupdf4llm.to_markdown(self.file_path)\n",
    "            if not md_text.strip():\n",
    "                raise Exception('Document had no content. ')\n",
    "            doc = Document(page_content=md_text, metadata={'source':self.file_path, 'markdown':True})\n",
    "            docs = [doc]\n",
    "            if self.parser.text_kwargs.get('infer_table_structure', False):\n",
    "                docs = extract_tables(docs=docs)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Map file extensions to document loaders and their arguments\n",
    "PDFOCR = 'pdfOCR'\n",
    "PDFMD = 'pdfMD'\n",
    "PDF = 'pdf'\n",
    "PDF_EXTS = [PDF, PDFOCR, PDFMD]\n",
    "OCR_CHAR_THRESH = 32\n",
    "LOADER_MAPPING = {\n",
    "    \"csv\": (CSVLoader, {}),\n",
    "    \"doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \"docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \"enex\": (EverNoteLoader, {}),\n",
    "    \"eml\": (MyElmLoader, {}),\n",
    "    \"epub\": (UnstructuredEPubLoader, {}),\n",
    "    \"html\": (UnstructuredHTMLLoader, {}),\n",
    "    \"md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \"odt\": (UnstructuredODTLoader, {}),\n",
    "    \"ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \"pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \"txt\": (TextLoader, {\"autodetect_encoding\": True}),\n",
    "    PDF   : (_PyMuPDFLoader, {}),\n",
    "    PDFMD: (PDF2MarkdownLoader, {}),\n",
    "    PDFOCR: (MyUnstructuredPDFLoader, {\"infer_table_structure\":False, \"mode\":\"elements\", \"strategy\":\"hi_res\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n",
    "\n",
    "def _update_metadata(docs:List[Document], metadata:dict):\n",
    "    \"\"\"\n",
    "    Update metadata in docs with supplied metadata dictionary\n",
    "    \"\"\"\n",
    "    for doc in docs:\n",
    "        doc.metadata.update(metadata)\n",
    "    return docs\n",
    "\n",
    "def _apply_text_callables(docs:List[Document], text_callables:dict):\n",
    "    \"\"\"\n",
    "    Invokes text_callables on entire text of document.\n",
    "\n",
    "    Returns a dictionary with values containing results from callables for each key\n",
    "    \"\"\"\n",
    "    if not text_callables: return {}\n",
    "        \n",
    "    text = '\\n\\n'.join([d.page_content for d in docs])\n",
    "    results = {}\n",
    "    for k,v in text_callables.items():\n",
    "        results[k] = v(text)\n",
    "    return results\n",
    "\n",
    "def _apply_file_callables(file_path:str, file_callables:dict):\n",
    "    \"\"\"\n",
    "    Invokes file_callables on file path.\n",
    "\n",
    "    Returns a dictionary with values containing results from callables for each key\n",
    "    \"\"\"\n",
    "    if not file_callables: return {}\n",
    "        \n",
    "    if not os.path.exists(file_path):\n",
    "        raise ValueError('file_path does not exist: {file_path}')\n",
    "    \n",
    "    results = {}\n",
    "    for k,v in file_callables.items():\n",
    "        results[k] = v(file_path)\n",
    "    return results\n",
    "\n",
    "    \n",
    "def load_single_document(file_path: str, # path to file\n",
    "                         pdf_unstructured:bool=False, # use unstructured for PDF extraction if True (will also OCR if necessary)\n",
    "                         pdf_markdown:bool = False, # Convert PDFs to Markdown instead of plain text if True.\n",
    "                         store_md5:bool=False, # Extract and store MD5 of document in metadata\n",
    "                         store_mimetype:bool=False, # Guess and store mime type of document in metadata\n",
    "                         store_file_dates:bool=False, # Extract snd store file dates in metadata\n",
    "                         file_callables:Optional[dict]=None, # optional dict with  keys and functions called with filepath as argument. Results stored as metadata.\n",
    "                         text_callables:Optional[dict]=None, # optional dict with  keys and functions called with file text as argument. Results stored as metadata.\n",
    "                         **kwargs,\n",
    "                         ) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Extract text from a single document. Will attempt to OCR PDFs, if necessary.\n",
    "\n",
    "\n",
    "    Note that extra kwargs can be supplied to configure the behavior of PDF loaders.\n",
    "    For instance, supplying `infer_table_structure` will cause `load_single_document` to try and\n",
    "    infer and extract tables from PDFs. When `pdf_unstructured=True` and `infer_table_structure=True`,\n",
    "    tables are represented as HTML within the main body of extracted text. In all other cases, inferred tables\n",
    "    are represented as Markdown and appended to the end of the extracted text when `infer_table_structure=True`.\n",
    "    \"\"\"\n",
    "    if pdf_unstructured and pdf_markdown:\n",
    "        raise ValueError('pdf_unstructured and pdf_markdown cannot both be True.')\n",
    "    file_path = os.path.abspath(file_path)\n",
    "\n",
    "\n",
    "    # extract metadata\n",
    "    md5, mimetype, cdate, mdate = None, None, None, None\n",
    "    file_metadata = {}\n",
    "    if store_md5:\n",
    "        file_metadata['md5'] = md5sum(file_path)\n",
    "    if store_mimetype:\n",
    "        file_metadata['mimetype'], _, _ = extract_mimetype(file_path)\n",
    "    if store_file_dates:\n",
    "        file_metadata['cdate'], file_metadata['mdate'] = extract_file_dates(file_path)\n",
    "    ext = extract_extension(file_path)\n",
    "    file_metadata['extension'] = ext\n",
    "    file_metadata.update(_apply_file_callables(file_path, file_callables))\n",
    "        \n",
    "    # load file\n",
    "    if ext in LOADER_MAPPING:\n",
    "        try:\n",
    "            if ext == PDF:\n",
    "                if pdf_unstructured:\n",
    "                    ext = PDFOCR\n",
    "                elif pdf_markdown:\n",
    "                    ext = PDFMD\n",
    "            loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "            loader_args = loader_args.copy() # copy so any supplied kwargs do not persist across calls\n",
    "            if ext in PDF_EXTS:\n",
    "                loader_args.update(kwargs)\n",
    "            loader = loader_class(file_path, **loader_args)\n",
    "            if ext in PDF_EXTS and ext != PDFOCR:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "                    docs = loader.load()\n",
    "                    file_metadata.update(_apply_text_callables(docs, text_callables))\n",
    "                    docs = _update_metadata(docs, file_metadata)\n",
    "                if not docs or len('\\n'.join([d.page_content.strip() for d in docs]).strip()) < OCR_CHAR_THRESH:\n",
    "                    loader_class, loader_args = LOADER_MAPPING[PDFOCR]\n",
    "                    loader = loader_class(file_path, **loader_args)\n",
    "                    file_metadta['ocr'] = True\n",
    "                    docs = loader.load()\n",
    "                    file_metadata.update(_apply_text_callables(docs, text_callables))\n",
    "                    docs = _update_metadata(docs, file_metadata)\n",
    "            else:\n",
    "                docs = loader.load()\n",
    "                file_metadata.update(_apply_text_callables(docs, text_callables))                \n",
    "                docs = _update_metadata(docs, file_metadata)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            logger.warning(f'\\nSkipping {file_path} due to error: {str(e)}')\n",
    "    else:\n",
    "        logger.warning(f\"\\nSkipping {file_path} due to unsupported file extension: '{ext}'\")\n",
    "\n",
    "\n",
    "def load_documents(source_dir: str, # path to folder containing documents\n",
    "                   ignored_files: List[str] = [], # list of filepaths to ignore\n",
    "                   ignore_fn:Optional[Callable] = None, # callable that accepts file path and returns True for ignored files\n",
    "                   caption_tables:bool=False,# If True, agument table text with summaries of tables if infer_table_structure is True.\n",
    "                   extract_document_titles:bool=False, # If True, infer document title and attach to individual chunks\n",
    "                   llm=None, # a reference to the LLM (used by `caption_tables` and `extract_document_titles`\n",
    "                   n_proc:Optional[int]=None, # number of CPU cores to use for text extraction. If None, use maximum for system.\n",
    "                   **kwargs\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source documents directory, ignoring specified files.\n",
    "    Extra kwargs fed to `ingest.load_single_document`.\n",
    "    \"\"\"\n",
    "    all_files = extract_files(source_dir, LOADER_MAPPING)\n",
    "\n",
    "    filtered_files = [\n",
    "        file_path for file_path in all_files if file_path not in ignored_files and not os.path.basename(file_path).startswith('~$')\n",
    "         and (ignore_fn is None or not ignore_fn(file_path))\n",
    "    ]\n",
    "\n",
    "    load_args = kwargs.copy()\n",
    "    if kwargs.get('infer_table_structure', False):\n",
    "        # Use \"spawn\" if using TableTransformers\n",
    "        # Reference: https://github.com/pytorch/pytorch/issues/40403\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "        # call extract_tables sequentially below instead of in load_single_document if n_proc>1\n",
    "        # because extract_tables is not well-suited to multiprocessing even with line above\n",
    "        if not n_proc or n_proc>1:\n",
    "            load_args = {k:load_args[k] for k in load_args if k!='infer_table_structure'}\n",
    "    with multiprocessing.Pool(processes=n_proc if n_proc else os.cpu_count()) as pool:\n",
    "        results = []\n",
    "        with tqdm(\n",
    "            total=len(filtered_files), desc=\"Loading new documents\", ncols=80\n",
    "        ) as pbar:\n",
    "            for i, docs in enumerate(\n",
    "                pool.imap_unordered(functools.partial(load_single_document, **load_args),\n",
    "                                                      filtered_files)\n",
    "            ):\n",
    "                if docs is not None:\n",
    "                    if kwargs.get('infer_table_structure', False):\n",
    "                        docs = extract_tables(docs=docs)\n",
    "                    if llm and caption_tables:\n",
    "                        helpers.caption_tables(docs, llm=llm, **kwargs)\n",
    "                    if llm and extract_document_titles:\n",
    "                        title = helpers.extract_title(docs, llm=llm, **kwargs)\n",
    "                        for doc in docs:\n",
    "                            if title:\n",
    "                                doc.metadata['document_title'] = title\n",
    "                    results.extend(docs)\n",
    "                pbar.update()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_folder(\n",
    "    source_directory: str, # path to folder containing document store\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    ignored_files: List[str] = [], # list of files to ignore\n",
    "    ignore_fn:Optional[Callable] = None, # Callable that accepts the file path (including file name) as input and ignores if returns True\n",
    "    **kwargs\n",
    "\n",
    "\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents from folder, extract text from them, split texts into chunks.\n",
    "    Extra kwargs fed to `ingest.load_documents` and `ingest.load_single_document`.\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from {source_directory}\")\n",
    "    documents = load_documents(source_directory,\n",
    "                              ignored_files, ignore_fn=ignore_fn,\n",
    "                              **kwargs)\n",
    "\n",
    "    return chunk_documents(documents,\n",
    "                           chunk_size = chunk_size,\n",
    "                           chunk_overlap = chunk_overlap,\n",
    "                           **kwargs)\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "    documents: list, # list of LangChain Documents\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    **kwargs\n",
    "\n",
    "\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Process list of Documents by splitting into chunks.\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        print(\"No new documents to process\")\n",
    "        return\n",
    "    print(f\"Processing {len(documents)} new documents\")\n",
    "\n",
    "    # remove tables before chunking\n",
    "    if kwargs.get('infer_table_structure', False) and not kwargs.get('pdf_unstructured', False):\n",
    "        tables = [d for d in documents if d.metadata.get('table', False)]\n",
    "        docs = [d for d in documents if not d.metadata.get('table', False)]\n",
    "    else:\n",
    "        tables = []\n",
    "        docs = documents\n",
    "\n",
    "    # initialize the splitter\n",
    "    contains_markdown = kwargs.get('pdf_markdown', False)\n",
    "    if contains_markdown:\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "            language=Language.MARKDOWN,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap)\n",
    "    else:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "    # split non-table texts\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    # attempt to remove text chunks containing mangled tables\n",
    "    texts = [d for d in texts if not includes_caption(d)]\n",
    "\n",
    "    # split table texts\n",
    "    if tables:\n",
    "        table_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "            language=Language.MARKDOWN,\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=0)\n",
    "        table_texts = table_splitter.split_documents(tables)\n",
    "        texts.extend(table_texts)\n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} chars each for text; max. {TABLE_CHUNK_SIZE} chars for tables)\")\n",
    "\n",
    "    # attach document title to each chunk (where title was extracted earlier by `load_documents`)\n",
    "    if kwargs.get('extract_document_titles', False):\n",
    "        for text in texts:\n",
    "            if text.metadata.get('document_title', ''):\n",
    "                text.page_content = f'The content below is from a document titled, \\\"{text.metadata[\"document_title\"]}\\\"\\n\\n{text.page_content}'\n",
    "    return texts\n",
    "\n",
    "\n",
    "def does_vectorstore_exist(db) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    if not db.get()[\"documents\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def batchify_chunks(texts, batch_size=CHROMA_MAX):\n",
    "    \"\"\"\n",
    "    split texts into batches specifically for Chroma\n",
    "    \"\"\"\n",
    "    split_docs_chunked = split_list(texts, batch_size)\n",
    "    total_chunks = sum(1 for _ in split_list(texts, batch_size))\n",
    "    return split_docs_chunked, total_chunks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "DEFAULT_DB = \"vectordb\"\n",
    "\n",
    "\n",
    "class Ingester:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        embedding_model_kwargs: dict = {\"device\": \"cpu\"},\n",
    "        embedding_encode_kwargs: dict = {\"normalize_embeddings\": False},\n",
    "        persist_directory: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_folder` (previously-ingested documents are ignored)\n",
    "\n",
    "        **Args**:\n",
    "\n",
    "          - *embedding_model*: name of sentence-transformers model\n",
    "          - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`)\n",
    "          - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                       embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "          - *persist_directory*: Path to vector database (created if it doesn't exist).\n",
    "                                 Default is `onprem_data/vectordb` in user's home directory.\n",
    "\n",
    "\n",
    "        **Returns**: `None`\n",
    "        \"\"\"\n",
    "        self.persist_directory = persist_directory or os.path.join(\n",
    "            get_datadir(), DEFAULT_DB\n",
    "        )\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model_name,\n",
    "            model_kwargs=embedding_model_kwargs,\n",
    "            encode_kwargs=embedding_encode_kwargs,\n",
    "        )\n",
    "        self.chroma_settings = Settings(\n",
    "            persist_directory=self.persist_directory, anonymized_telemetry=False\n",
    "        )\n",
    "        self.chroma_client = chromadb.PersistentClient(\n",
    "            settings=self.chroma_settings, path=self.persist_directory\n",
    "        )\n",
    "        return\n",
    "\n",
    "\n",
    "    def _get_chroma_max(self):\n",
    "        return CHROMA_MAX\n",
    "\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Returns an instance to the `langchain_chroma.Chroma` instance\n",
    "        \"\"\"\n",
    "        db = Chroma(\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings,\n",
    "            client_settings=self.chroma_settings,\n",
    "            client=self.chroma_client,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            collection_name=COLLECTION_NAME,\n",
    "        )\n",
    "        return db if does_vectorstore_exist(db) else None\n",
    "\n",
    "    def get_embedding_model(self):\n",
    "        \"\"\"\n",
    "        Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance\n",
    "        \"\"\"\n",
    "        return self.embeddings\n",
    "\n",
    "\n",
    "    def get_ingested_files(self):\n",
    "        \"\"\"\n",
    "        Returns a list of files previously added to vector database (typically via `LLM.ingest`)\n",
    "        \"\"\"\n",
    "        return set([d['source'] for d in self.get_db().get()['metadatas']])\n",
    "\n",
    "\n",
    "    def store_documents(self, documents, batch_size:int=CHROMA_MAX):\n",
    "        \"\"\"\n",
    "        Stores instances of `langchain_core.documents.base.Document` in vectordb\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return\n",
    "        db = self.get_db()\n",
    "        if db:\n",
    "            print(\"Creating embeddings. May take some minutes...\")\n",
    "            chunk_batches, total_chunks = batchify_chunks(documents, batch_size=batch_size)\n",
    "            for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                db.add_documents(lst)\n",
    "        else:\n",
    "            chunk_batches, total_chunks = batchify_chunks(documents, batch_size)\n",
    "            print(\"Creating embeddings. May take some minutes...\")\n",
    "            db = None\n",
    "\n",
    "            for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                if not db:\n",
    "                    db = Chroma.from_documents(\n",
    "                        lst,\n",
    "                        self.embeddings,\n",
    "                        persist_directory=self.persist_directory,\n",
    "                        client_settings=self.chroma_settings,\n",
    "                        client=self.chroma_client,\n",
    "                        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                        collection_name=COLLECTION_NAME,\n",
    "                    )\n",
    "                else:\n",
    "                    db.add_documents(lst)\n",
    "        return\n",
    "\n",
    "\n",
    "    def ingest(\n",
    "        self,\n",
    "        source_directory: str, # path to folder containing document store\n",
    "        chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n",
    "        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "        ignore_fn:Optional[Callable] = None, # Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested.\n",
    "        batch_size:int=CHROMA_MAX, # batch size used when creating embeddings and storing documents.\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_directory` (previously-ingested documents are\n",
    "        ignored). When retrieved, the\n",
    "        [Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
    "        objects will each have a `metadata` dict with the absolute path to the file\n",
    "        in `metadata[\"source\"]`.\n",
    "        Extra kwargs fed to `ingest.load_single_document`.\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(source_directory):\n",
    "            raise ValueError(\"The source_directory does not exist.\")\n",
    "        elif os.path.isfile(source_directory):\n",
    "            raise ValueError(\n",
    "                \"The source_directory argument must be a folder, not a file.\"\n",
    "            )\n",
    "        texts = None\n",
    "        db = self.get_db()\n",
    "        if db:\n",
    "            # Update and store locally vectorstore\n",
    "            print(f\"Appending to existing vectorstore at {self.persist_directory}\")\n",
    "            collection = db.get()\n",
    "            ignored_files=[ metadata[\"source\"] for metadata in collection[\"metadatas\"]]\n",
    "        else:\n",
    "            print(f\"Creating new vectorstore at {self.persist_directory}\")\n",
    "            ignored_files = []\n",
    "\n",
    "        texts = process_folder(\n",
    "            source_directory,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            ignored_files=ignored_files,\n",
    "            ignore_fn=ignore_fn,\n",
    "            **kwargs\n",
    "\n",
    "        )\n",
    "        self.store_documents(texts, batch_size=batch_size)\n",
    "\n",
    "        if texts:\n",
    "            print(\n",
    "                \"Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\"\n",
    "            )\n",
    "        db = None\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#L449){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_embedding_model\n",
       "\n",
       ">      Ingester.get_embedding_model ()\n",
       "\n",
       "*Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#L449){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_embedding_model\n",
       "\n",
       ">      Ingester.get_embedding_model ()\n",
       "\n",
       "*Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.get_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#L435){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_db\n",
       "\n",
       ">      Ingester.get_db ()\n",
       "\n",
       "*Returns an instance to the `langchain_chroma.Chroma` instance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#L435){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_db\n",
       "\n",
       ">      Ingester.get_db ()\n",
       "\n",
       "*Returns an instance to the `langchain_chroma.Chroma` instance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.get_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#L456){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_ingested_files\n",
       "\n",
       ">      Ingester.get_ingested_files ()\n",
       "\n",
       "*Returns a list of files previously added to vector database (typically via `LLM.ingest`)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#L456){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_ingested_files\n",
       "\n",
       ">      Ingester.get_ingested_files ()\n",
       "\n",
       "*Returns a list of files previously added to vector database (typically via `LLM.ingest`)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.get_ingested_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#L496){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.ingest\n",
       "\n",
       ">      Ingester.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                       chunk_overlap:int=50, ignore_fn:Optional[Callable]=None,\n",
       ">                       **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#L496){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.ingest\n",
       "\n",
       ">      Ingester.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                       chunk_overlap:int=50, ignore_fn:Optional[Callable]=None,\n",
       ">                       **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#L463){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.store_documents\n",
       "\n",
       ">      Ingester.store_documents (documents)\n",
       "\n",
       "*Stores instances of `langchain_core.documents.base.Document` in vectordb*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#L463){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.store_documents\n",
       "\n",
       ">      Ingester.store_documents (documents)\n",
       "\n",
       "*Stores instances of `langchain_core.documents.base.Document` in vectordb*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.store_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 11:35:20.660565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from sample_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 2/2 [00:00<00:00, 16.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 new documents from sample_data\n",
      "Split into 62 chunks of text (max. 500 chars each)\n",
      "Creating embeddings. May take some minutes...\n",
      "Ingestion complete! You can now query your documents using the LLM.ask method\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "ingester = Ingester()\n",
    "ingester.ingest(\"tests/sample_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
