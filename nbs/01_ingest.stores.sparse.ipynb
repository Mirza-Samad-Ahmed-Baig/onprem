{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.stores.sparse\n",
    "\n",
    "> full-text search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.stores.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from whoosh import index\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "from whoosh.fields import *\n",
    "from whoosh.filedb.filestore import RamStorage\n",
    "from whoosh.qparser import MultifieldParser, OrGroup\n",
    "from whoosh.query import Term, And\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from onprem.ingest.base import VectorStore\n",
    "from onprem.ingest.helpers import doc_from_dict\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# IMPORTANT: Metadata fields in langchain_core.documents.Document objects\n",
    "#            (i.e., the input to WSearch.index_documents) should\n",
    "#            ideally match schema fields below, but this is not strictly required.\n",
    "#\n",
    "#            The page_content field is the only truly required field in supplied\n",
    "#            Document objects. All other fields, including dynamic fields, are optional. \n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "DEFAULT_SCHEMA = Schema(\n",
    "    page_content=TEXT(stored=True), # REQUIRED\n",
    "    id=ID(stored=True, unique=True),\n",
    "    source=KEYWORD(stored=True, commas=True), \n",
    "    source_search=TEXT(stored=True),\n",
    "    filepath=KEYWORD(stored=True, commas=True),\n",
    "    filepath_search=TEXT(stored=True),\n",
    "    filename=KEYWORD(stored=True),\n",
    "    ocr=BOOLEAN(stored=True),\n",
    "    table=BOOLEAN(stored=True),\n",
    "    markdown=BOOLEAN(stored=True),\n",
    "    page=NUMERIC(stored=True),\n",
    "    document_title=TEXT(stored=True),\n",
    "    md5=KEYWORD(stored=True),\n",
    "    mimetype=KEYWORD(stored=True),\n",
    "    extension=KEYWORD(stored=True),\n",
    "    filesize=NUMERIC(stored=True),\n",
    "    createdate=DATETIME(stored=True),\n",
    "    modifydate=DATETIME(stored=True),\n",
    "    tags=KEYWORD(stored=True, commas=True),\n",
    "    notes=TEXT(stored=True),\n",
    "    msg=TEXT(stored=True),\n",
    "    )\n",
    "DEFAULT_SCHEMA.add(\"*_t\", TEXT(stored=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_k\", KEYWORD(stored=True, commas=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_b\", BOOLEAN(stored=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_n\", NUMERIC(stored=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_d\", DATETIME(stored=True), glob=True)\n",
    "\n",
    "\n",
    "def default_schema():\n",
    "    schema = DEFAULT_SCHEMA\n",
    "    #if \"raw\" not in schema.stored_names():\n",
    "        #schema.add(\"raw\", TEXT(stored=True))\n",
    "    return schema\n",
    "\n",
    "\n",
    "class SparseStore(VectorStore):\n",
    "    def __init__(self,\n",
    "                persist_directory: Optional[str]=None, \n",
    "                index_name:str = 'myindex',\n",
    "                **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializes full-text search engine.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *persist_directory*: path to folder where search index is stored\n",
    "        - *index_name*: name of index\n",
    "        - *embedding_model*: name of sentence-transformers model\n",
    "        - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.\n",
    "        - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                     embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "        \"\"\"\n",
    "\n",
    "        self.persist_directory = persist_directory # alias for consistency with DenseStore\n",
    "        self.index_name = index_name\n",
    "        if self.persist_directory and not self.index_name:\n",
    "            raise ValueError('index_name is required if persist_directory is supplied')\n",
    "        if self.persist_directory:\n",
    "            if not index.exists_in(self.persist_directory, indexname=self.index_name):\n",
    "                self.ix = __class__.initialize_index(self.persist_directory, self.index_name)\n",
    "            else:\n",
    "                self.ix = index.open_dir(self.persist_directory, indexname=self.index_name)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"No persist_directory was supplied, so an in-memory only index\"\n",
    "                \"was created using DEFAULT_SCHEMA\"\n",
    "            )\n",
    "            self.ix = RamStorage().create_index(default_schema())\n",
    "        self.init_embedding_model(**kwargs) # stored as self.embeddings\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Get raw index\n",
    "        \"\"\"\n",
    "        return self.ix\n",
    "\n",
    "\n",
    "    def exists(self):\n",
    "        \"\"\"\n",
    "        Returns True if documents have been added to search index\n",
    "        \"\"\"\n",
    "        return self.get_size() > 0\n",
    "\n",
    "\n",
    "    def add_documents(self,\n",
    "                      docs: Sequence[Document], # list of LangChain Documents\n",
    "                      limitmb:int=1024, # maximum memory in  megabytes to use\n",
    "                      verbose:bool=True, # Set to False to disable progress bar\n",
    "                      **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Indexes documents. Extra kwargs supplied to `TextStore.ix.writer`.\n",
    "        \"\"\"\n",
    "        writer = self.ix.writer(limitmb=limitmb, **kwargs)\n",
    "        for doc in tqdm(docs, total=len(docs), disable=not verbose):\n",
    "            d = self.doc2dict(doc)\n",
    "            writer.update_document(**d)\n",
    "        writer.commit(optimize=True)\n",
    "\n",
    "\n",
    "    def remove_document(self, value:str, field:str='id'):\n",
    "        \"\"\"\n",
    "        Remove document with corresponding value and field.\n",
    "        Default field is the id field.\n",
    "        \"\"\"\n",
    "        writer = self.ix.writer()\n",
    "        writer.delete_by_term(field, value)\n",
    "        writer.commit(optimize=True)\n",
    "        return\n",
    "\n",
    "\n",
    "    def update_documents(self,\n",
    "                         doc_dicts:dict, # dictionary with keys 'page_content', 'source', 'id', etc.\n",
    "                         **kwargs):\n",
    "        \"\"\"\n",
    "        Update a set of documents (doc in index with same ID will be over-written)\n",
    "        \"\"\"\n",
    "        docs = [doc_from_dict(d) for d in doc_dicts]\n",
    "        self.add_documents(docs)\n",
    "\n",
    "\n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a generator to iterate through all indexed documents\n",
    "        \"\"\"\n",
    "        return self.ix.searcher().documents()\n",
    "       \n",
    "\n",
    "    def get_doc(self, id:str):\n",
    "        \"\"\"\n",
    "        Get an indexed record by ID\n",
    "        \"\"\"\n",
    "        r = self.query(f'id:{id}', return_dict=True)\n",
    "        return r['hits'][0] if len(r['hits']) > 0 else None\n",
    "\n",
    "\n",
    "    def get_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Gets size of index\n",
    "        \"\"\"\n",
    "        return self.ix.doc_count_all()\n",
    "\n",
    "        \n",
    "    def erase(self, confirm=True):\n",
    "        \"\"\"\n",
    "        Clears index\n",
    "        \"\"\"\n",
    "        shall = True\n",
    "        if confirm:\n",
    "            msg = (\n",
    "                f\"You are about to remove all documents from the search index.\"\n",
    "                + f\"(Original documents on file system will remain.) Are you sure?\"\n",
    "            )\n",
    "            shall = input(\"%s (Y/n) \" % msg) == \"Y\"\n",
    "        if shall and index.exists_in(\n",
    "            self.persist_directory, indexname=self.index_name\n",
    "        ):\n",
    "            ix = index.create_in(\n",
    "                self.persist_directory,\n",
    "                indexname=self.index_name,\n",
    "                schema=default_schema(),\n",
    "            )\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _analyze_query(self, q, field:str='page_content'):\n",
    "        \"\"\"\n",
    "        Analyze query\n",
    "        \"\"\"\n",
    "        analyzer = self.ix.schema[field].analyzer\n",
    "        return \" \".join([token.text for token in analyzer(q)])\n",
    "\n",
    "    \n",
    "    def query(\n",
    "            self,\n",
    "            q: str,\n",
    "            fields: Sequence = [\"page_content\"],\n",
    "            highlight: bool = True,\n",
    "            limit:int=10,\n",
    "            page:int=1,\n",
    "            return_dict:bool=False,\n",
    "            filters:Optional[Dict[str, str]] = None,\n",
    "            where_document:Optional[str]=None,\n",
    "\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Queries the index\n",
    "\n",
    "        **Args**\n",
    "\n",
    "        - *q*: the query string\n",
    "        - *fields*: a list of fields to search\n",
    "        - *highlight*: If True, highlight hits\n",
    "        - *limit*: results per page\n",
    "        - *page*: page of hits to return\n",
    "        - *return_dict*: If True, return list of dictionaries instead of LangChain Document objects\n",
    "        - *filters*: filter results by field values (e.g., {'extension':'pdf'})\n",
    "        - *where_document*: optional query to further filter results\n",
    "        \"\"\"\n",
    "\n",
    "        search_results = []\n",
    "\n",
    "        # Apply analyzer to query as long as it is not a boolean query\n",
    "        if \"AND\" not in q and \"OR\" not in q and \"NOT\" not in q and \":\" not in q:\n",
    "            q = self._analyze_query(q)\n",
    "        if where_document:\n",
    "            q = f'({q}) AND ({where_document})'\n",
    "\n",
    "        # process filters\n",
    "        combined_filter=None\n",
    "        if filters:\n",
    "            terms = []\n",
    "            for k in filters:\n",
    "                terms.append(Term(k, filters[k]))\n",
    "            combined_filter = And(terms)\n",
    "                   \n",
    "        # process search\n",
    "        with self.ix.searcher() as searcher:\n",
    "            if page == 1:\n",
    "                results = searcher.search(\n",
    "                    MultifieldParser(fields, schema=self.ix.schema, group=OrGroup.factory(0.9)).parse(q), limit=limit, filter=combined_filter)\n",
    "            else:\n",
    "                results = searcher.search_page(\n",
    "                    MultifieldParser(fields, schema=self.ix.schema, group=OrGroup.factory(0.9)).parse(q), page, limit, filter=combined_filter)\n",
    "            total_hits = results.scored_length()\n",
    "            if page > math.ceil(total_hits/limit):\n",
    "               results = []\n",
    "            for r in results:\n",
    "                #d = json.loads(r[\"raw\"])\n",
    "                d = dict(r)\n",
    "                if highlight:\n",
    "                    for f in fields:\n",
    "                        if r[f] and isinstance(r[f], str):\n",
    "                            d['hl_'+f] = r.highlights(f) or r[f]\n",
    "                d = d if return_dict else doc_from_dict(d)\n",
    "                search_results.append(d)\n",
    "   \n",
    "        return {'hits':search_results, 'total_hits':total_hits}\n",
    "\n",
    "    def semantic_search(self, \n",
    "                        query, \n",
    "                        k:int=4, # number of records to return based on highest semantic similarity scores.\n",
    "                        n_candidates=50, # Number of records to consider (for which we compute embeddings on-the-fly)\n",
    "                        filters:Optional[Dict[str, str]] = None, # filter sources by field values (e.g., {'table':True})\n",
    "                        where_document:Optional[str]=None, # a boolean query to filter results further (e.g., \"climate\" AND extension:pdf)\n",
    "                        **kwargs):\n",
    "        \"\"\"\n",
    "        Retrieves results based on semantic similarity to supplied `query`.\n",
    "        \"\"\"\n",
    "        from sentence_transformers import util\n",
    "        import torch\n",
    "\n",
    "        results = self.query(query, limit=n_candidates, return_dict=True, filters=filters, where_document=where_document)['hits']\n",
    "        if not results: return []\n",
    "        texts = [r['page_content'] for r in results]\n",
    "        embeddings = self.get_embedding_model()\n",
    "\n",
    "        # Compute embeddings\n",
    "        query_emb = torch.tensor(embeddings.embed_query(query)).unsqueeze(0)  # Shape (1, embedding_dim)\n",
    "        text_embs = torch.tensor(embeddings.embed_documents(texts))  # Shape (len(texts), embedding_dim)\n",
    "    \n",
    "        # Compute cosine similarity\n",
    "        cos_scores = util.pytorch_cos_sim(query_emb, text_embs).squeeze(0).tolist()  # Shape (len(texts),)\n",
    "\n",
    "        # Assign scores back to results\n",
    "        for i, score in enumerate(cos_scores):\n",
    "            results[i]['score'] = score\n",
    "\n",
    "        # Sort results by similarity in descending order\n",
    "        sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)[:k]\n",
    "        return [doc_from_dict(r) for r in sorted_results]\n",
    "              \n",
    "        \n",
    "    @classmethod\n",
    "    def index_exists_in(cls, index_path: str, index_name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Returns True if index exists with name, *indexname*, and path, *index_path*.\n",
    "        \"\"\"\n",
    "        return index.exists_in(index_path, indexname=index_name)\n",
    "\n",
    "    @classmethod\n",
    "    def initialize_index(\n",
    "        cls, index_path: str, index_name: str, schema: Optional[Schema] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize index\n",
    "\n",
    "        **Args**\n",
    "\n",
    "        - *index_path*: path to folder storing search index\n",
    "        - *index_name*: name of index\n",
    "        - *schema*: optional whoosh.fields.Schema object.\n",
    "                    If None, DEFAULT_SCHEMA is used\n",
    "        \"\"\"\n",
    "        schema = default_schema() if not schema else schema\n",
    "\n",
    "        if index.exists_in(index_path, indexname=index_name):\n",
    "            raise ValueError(\n",
    "                f\"There is already an existing index named {index_name}  with path {index_path} \\n\"\n",
    "                + f\"Delete {index_path} manually and try again.\"\n",
    "            )\n",
    "        if not os.path.exists(index_path):\n",
    "            os.makedirs(index_path)\n",
    "        ix = index.create_in(index_path, indexname=index_name, schema=schema)\n",
    "        return ix\n",
    "\n",
    "    def doc2dict(self, doc:Document):\n",
    "        \"\"\"\n",
    "        Convert LangChain Document to expected format\n",
    "        \"\"\"\n",
    "        stored_names = self.ix.schema.stored_names()\n",
    "        d = {}\n",
    "        for k,v in doc.metadata.items():\n",
    "            suffix = None\n",
    "            if k in stored_names:\n",
    "                suffix = ''\n",
    "            elif isinstance(v, bool):\n",
    "                suffix = '_b' if not k.endswith('_b') else ''\n",
    "            elif isinstance(v, str):\n",
    "                if k.endswith('_date'):\n",
    "                    suffix = '_d'\n",
    "                else:\n",
    "                    suffix = '_k'if not k.endswith('_k') else ''\n",
    "            elif isinstance(v, (int, float)):\n",
    "                suffix = '_n'if not k.endswith('_n') else ''\n",
    "            if suffix is not None:\n",
    "                d[k+suffix] = v\n",
    "        d['id'] = uuid.uuid4().hex if not doc.metadata.get('id', '') else doc.metadata['id']\n",
    "        d['page_content' ] = doc.page_content\n",
    "        #d['raw'] = json.dumps(d)\n",
    "        if 'source' in d:\n",
    "            d['source_search'] = d['source']\n",
    "        if 'filepath' in d:\n",
    "            d['filepath_search'] = d['filepath']\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
