{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Built-In Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**[OnPrem.LLM](https://github.com/amaiya/onprem)** includes a built-in web app to easily access and use LLMs. After [installing](https://github.com/amaiya/onprem#install) OnPrem.LLM, you can start it by running the following command at the command-line:\n",
    "\n",
    "```shell\n",
    "# run at command-line\n",
    "onprem --port 8000\n",
    "```\n",
    "Then, enter `localhost:8000` in your Web browser to access the application:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_welcome.png\" border=\"0\" alt=\"screenshot\" width=\"775\"/>\n",
    "\n",
    "The Web app is implemented with [streamlit](https://streamlit.io/): `pip install streamlit`.  If it is not already installed, the `onprem` command will ask you to install it.\n",
    "Here is more information on the `onprem` command:\n",
    "```sh\n",
    "$:~/projects/github/onprem$ onprem --help\n",
    "usage: onprem [-h] [-p PORT] [-a ADDRESS] [-v]\n",
    "\n",
    "Start the OnPrem.LLM web app\n",
    "Example: onprem --port 8000\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -p PORT, --port PORT  Port to use; default is 8501\n",
    "  -a ADDRESS, --address ADDRESS\n",
    "                        Address to bind; default is 0.0.0.0\n",
    "  -v, --version         Print a version\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app requires a file called `webapp.yml` exists in the `onprem_data` folder in the user's home directory. This file stores information used by the Web app such as the model to use. If one does not exist, then a default one will be created for you and is also shown below:\n",
    "\n",
    "```yaml\n",
    "# Default YAML configuration\n",
    "llm:\r",
    "  # model url (or model file name if previously downloaded)\r\n",
    "  # if changing, be sure to update the prompt_template variable below\r\n",
    "  model_url: https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.ggufuf\r\n",
    "  # number of layers offloaded to GPU\r\n",
    "  n_gpu_layers\n",
    "  # the vector store type to use (dual, dense, or sparse)\n",
    "  # dual: a vector store where both Chroma semantic searches and conventional keyword searches are supported\n",
    "  store_type: dual: 32\r\n",
    "  # path to vector db folder\r\n",
    "  vectordb_path: {datadir}/vectordb\r\n",
    "  # path to model download folder\r\n",
    "  model_download_path: {datadir}\r\n",
    "  # number of source documents used by LLM.ask and LLM.chat\r\n",
    "  rag_num_source_docs: 6\r\n",
    "  # minimum similarity score for source to be considered by LLM.ask/LLM.chat\r\n",
    "  rag_score_threshold: 0.0\r\n",
    "  # verbosity of Llama.cpp\r\n",
    "  # additional parameters added in the \"llm\" YAML section will be fed directly to LlamaCpp (e.g., temperature)\r\n",
    "  #temperature: 0.0\r\n",
    "prompt:\r\n",
    "  # The default prompt_template is specifically for Zephyr-7B.\r\n",
    "  # It will need to be changed if you change the model_url above.\r\n",
    "  prompt_template: <|system|>\\n</s>\\n<|user|>\\n{prompt}<  prompt_template:\r\n",
    "ui:\r\n",
    "  # title of application\r\n",
    "  title: OnPrem.LLM\r\n",
    "  # subtitle in \"Talk to Your Documents\" screen\r\n",
    "  rag_title:\r\n",
    "  # path to markdown file with contents that will be inserted below rag_title\r\n",
    "  rag_text_path:\r\n",
    "  # path to folder containing raw documents (i.e., absolute path of folder you supplied to LLM.ingest)\r\n",
    "  rag_source_path:\r\n",
    "  # base url (leave blank unless you're running your own separate web server to serve sour\n",
    "  # whether to show the Manage page in the sidebar (TRUE or FALSE)\n",
    "  show_manage: TRUE  ce dl\n",
    " in GGUF format can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can edit the file based on your requirements. Variables in the `llm` section are automatically passed to the `onprem.LLM` constructor, which, in turn, passes extra `**kwargs` to `llama-cpp-python` or the `transformers.pipeline`.  For instance, you can add a `temperature` variable in the `llm` section to adjust temperature of the model in the web app (e.g., lower values closer to 0.0 for more deterministic output and higher values for more creativity). \n",
    "\n",
    "The default model is a 7B-parameter model called [Zephyr-7B](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some models have particular prompt formats.  For instance, if using the default **Zephyr-7B** model above, as described on the [model's home page](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF#prompt-template-zephyr), the `prompt_template` in the YAML file must be set to:\n",
    "```yaml\n",
    "prompt:\n",
    "  prompt_template: <|system|>\\n</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\n",
    "```\n",
    "\n",
    "If changing models, don't forget to update the `prompt_template` variable with the prompt format approrpriate for your chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Prompts to Solve Problems\n",
    "\n",
    "The first app page is a UI for interactive chatting and prompting to solve problems various problems with local LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_prompting.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk To Your Documents\n",
    "The second screen in the app is a UI for [retrieval augmented generation](https://arxiv.org/abs/2005.11401) or RAG (i.e., chatting with documents). Sources considered by the LLM when generating answers are displayed and ranked by answer-to-source similarity. Hovering over the question marks in the sources will display the snippets of text from a document considered by the LLM when generating answers.  Documents you would like to consider as sources for question-answering can be uploaded through the Web UI and this is discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_rag.png\" border=\"0\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Search \n",
    "\n",
    "The third screen is a UI for searching documents you've uploaded either through keyword searches or semantic searches. Documents that you would like to search can be uploaded through the Web app and is discussed next.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_search.png\" border=\"0\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Documents\n",
    "\n",
    "The Web UI also includes a point-and-click interface to upload and index documents into the vector store(s). Documents can either be uploaded individually or as a zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_upload.png\" border=\"0\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
