{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hf.data.questions\n",
    "\n",
    "> Questions module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp hf.data.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Questions module\n",
    "\"\"\"\n",
    "\n",
    "from onprem.hf.data.base import Data\n",
    "\n",
    "\n",
    "class Questions(Data):\n",
    "    \"\"\"\n",
    "    Tokenizes question-answering datasets as input for training question-answering models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, columns, maxlength, stride):\n",
    "        \"\"\"\n",
    "        Creates a new instance for tokenizing Questions training data.\n",
    "\n",
    "        Args:\n",
    "            tokenizer: model tokenizer\n",
    "            columns: tuple of columns to use for question/context/answer\n",
    "            maxlength: maximum sequence length\n",
    "            stride: chunk size for splitting data for QA tasks\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(tokenizer, columns, maxlength)\n",
    "\n",
    "        if not self.columns:\n",
    "            self.columns = (\"question\", \"context\", \"answers\")\n",
    "\n",
    "        self.question, self.context, self.answer = self.columns\n",
    "        self.stride = stride\n",
    "        self.rpad = tokenizer.padding_side == \"right\"\n",
    "\n",
    "    def process(self, data):\n",
    "        # Tokenize data\n",
    "        tokenized = self.tokenize(data)\n",
    "\n",
    "        # Get mapping of overflowing tokens and answer offsets\n",
    "        samples = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "        offsets = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "        # Start/end positions\n",
    "        tokenized[\"start_positions\"] = []\n",
    "        tokenized[\"end_positions\"] = []\n",
    "\n",
    "        for x, offset in enumerate(offsets):\n",
    "            # Label NO ANSWER with CLS token\n",
    "            inputids = tokenized[\"input_ids\"][x]\n",
    "            clstoken = inputids.index(self.tokenizer.cls_token_id)\n",
    "\n",
    "            # Sequence ids\n",
    "            sequences = tokenized.sequence_ids(x)\n",
    "\n",
    "            # Get and format answer\n",
    "            answers = self.answers(data, samples[x])\n",
    "\n",
    "            # If no answers are given, set cls token as answer.\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized[\"start_positions\"].append(clstoken)\n",
    "                tokenized[\"end_positions\"].append(clstoken)\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                startchar = answers[\"answer_start\"][0]\n",
    "                endchar = startchar + len(answers[\"text\"][0])\n",
    "\n",
    "                # Start token index of the current span in the text.\n",
    "                start = 0\n",
    "                while sequences[start] != (1 if self.rpad else 0):\n",
    "                    start += 1\n",
    "\n",
    "                # End token index of the current span in the text.\n",
    "                end = len(inputids) - 1\n",
    "                while sequences[end] != (1 if self.rpad else 0):\n",
    "                    end -= 1\n",
    "\n",
    "                # Label with CLS token if out of span\n",
    "                if not (offset[start][0] <= startchar and offset[end][1] >= endchar):\n",
    "                    tokenized[\"start_positions\"].append(clstoken)\n",
    "                    tokenized[\"end_positions\"].append(clstoken)\n",
    "                else:\n",
    "                    # Map start character and end character to matching token index\n",
    "                    while start < len(offset) and offset[start][0] <= startchar:\n",
    "                        start += 1\n",
    "                    tokenized[\"start_positions\"].append(start - 1)\n",
    "\n",
    "                    while offset[end][1] >= endchar:\n",
    "                        end -= 1\n",
    "                    tokenized[\"end_positions\"].append(end + 1)\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    def tokenize(self, data):\n",
    "        \"\"\"\n",
    "        Tokenizes batch of data\n",
    "\n",
    "        Args:\n",
    "            data: input data batch\n",
    "\n",
    "        Returns:\n",
    "            tokenized data\n",
    "        \"\"\"\n",
    "\n",
    "        # Trim question whitespace\n",
    "        data[self.question] = [x.lstrip() for x in data[self.question]]\n",
    "\n",
    "        # Tokenize records\n",
    "        return self.tokenizer(\n",
    "            data[self.question if self.rpad else self.context],\n",
    "            data[self.context if self.rpad else self.question],\n",
    "            truncation=\"only_second\" if self.rpad else \"only_first\",\n",
    "            max_length=self.maxlength,\n",
    "            stride=self.stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "    def answers(self, data, index):\n",
    "        \"\"\"\n",
    "        Gets and formats an answer.\n",
    "\n",
    "        Args:\n",
    "            data: input examples\n",
    "            index: answer index to retrieve\n",
    "\n",
    "        Returns:\n",
    "            answers dict\n",
    "        \"\"\"\n",
    "\n",
    "        # Answer mappings\n",
    "        answers = data[self.answer][index]\n",
    "        context = data[self.context][index]\n",
    "\n",
    "        # Handle mapping string answers to dict\n",
    "        if not isinstance(answers, dict):\n",
    "            if not answers:\n",
    "                answers = {\"text\": [], \"answer_start\": []}\n",
    "            else:\n",
    "                answers = {\"text\": [answers], \"answer_start\": [context.index(answers)]}\n",
    "\n",
    "        return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
