{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hf.llm.huggingface\n",
    "\n",
    "> HF LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp hf.llm.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Hugging Face LLM module\n",
    "\"\"\"\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "from onprem.hf.models import Models\n",
    "\n",
    "from onprem.hf.hfpipeline import HFPipeline\n",
    "\n",
    "from onprem.hf.llm.generation import Generation\n",
    "\n",
    "\n",
    "class HFGeneration(Generation):\n",
    "    \"\"\"\n",
    "    Hugging Face Transformers generative model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, template=None, **kwargs):\n",
    "        # Call parent constructor\n",
    "        super().__init__(path, template, **kwargs)\n",
    "\n",
    "        # Create HuggingFace LLM pipeline\n",
    "        self.llm = HFLLM(path, **kwargs)\n",
    "\n",
    "    def stream(self, texts, maxlength, stream, stop, **kwargs):\n",
    "        yield from self.llm(texts, maxlength=maxlength, stream=stream, stop=stop, **kwargs)\n",
    "\n",
    "\n",
    "class HFLLM(HFPipeline):\n",
    "    \"\"\"\n",
    "    Hugging Face Transformers large language model (LLM) pipeline. This pipeline autodetects if the model path\n",
    "    is a text generation or sequence to sequence model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path=None, quantize=False, gpu=True, model=None, task=None, **kwargs):\n",
    "        super().__init__(self.task(path, task, **kwargs), path, quantize, gpu, model, **kwargs)\n",
    "\n",
    "        # Load tokenizer, if necessary\n",
    "        self.pipeline.tokenizer = self.pipeline.tokenizer if self.pipeline.tokenizer else Models.tokenizer(path, **kwargs)\n",
    "\n",
    "    def __call__(self, text, prefix=None, maxlength=512, workers=0, stream=False, stop=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Generates text. Supports the following input formats:\n",
    "\n",
    "          - String or list of strings (instruction-tuned models must follow chat templates)\n",
    "          - List of dictionaries with `role` and `content` key-values or lists of lists\n",
    "\n",
    "        Args:\n",
    "            text: text|list\n",
    "            prefix: optional prefix to prepend to text elements\n",
    "            maxlength: maximum sequence length\n",
    "            workers: number of concurrent workers to use for processing data, defaults to None\n",
    "            stream: stream response if True, defaults to False\n",
    "            stop: list of stop strings\n",
    "            kwargs: additional generation keyword arguments\n",
    "\n",
    "        Returns:\n",
    "            generated text\n",
    "        \"\"\"\n",
    "\n",
    "        # List of texts\n",
    "        texts = text if isinstance(text, list) else [text]\n",
    "\n",
    "        # Add prefix, if necessary\n",
    "        if prefix:\n",
    "            texts = [f\"{prefix}{x}\" for x in texts]\n",
    "\n",
    "        # Combine all keyword arguments\n",
    "        kwargs = self.parameters(maxlength, workers, stop, **kwargs)\n",
    "\n",
    "        # Stream response\n",
    "        if stream:\n",
    "            return StreamingResponse(self.pipeline, texts, stop, **kwargs)()\n",
    "\n",
    "        # Run pipeline\n",
    "        results = self.pipeline(texts, stop_strings=stop, **kwargs)\n",
    "\n",
    "        # Extract generated text\n",
    "        results = [self.extract(result) for result in results]\n",
    "\n",
    "        return results[0] if isinstance(text, str) else results\n",
    "\n",
    "    def parameters(self, maxlength, workers, stop, **kwargs):\n",
    "        \"\"\"\n",
    "        Builds a combined parameter dictionary.\n",
    "\n",
    "        Args:\n",
    "            maxlength: maximum sequence length\n",
    "            workers: number of concurrent workers to use for processing data, defaults to None\n",
    "            stop: list of stop strings\n",
    "            kwargs: additional generation keyword arguments\n",
    "\n",
    "        Returns:\n",
    "            dict of parameters\n",
    "        \"\"\"\n",
    "\n",
    "        # Default parameters\n",
    "        defaults = {\n",
    "            \"max_length\": maxlength,\n",
    "            \"num_workers\": workers,\n",
    "        }\n",
    "\n",
    "        # Add pad token if it's missing from model config\n",
    "        model = self.pipeline.model\n",
    "        if not model.config.pad_token_id:\n",
    "            tokenid = model.config.eos_token_id\n",
    "            tokenid = tokenid[0] if isinstance(tokenid, list) else tokenid\n",
    "\n",
    "            # Set pad_token_id parameter\n",
    "            defaults[\"pad_token_id\"] = tokenid\n",
    "\n",
    "            # Update tokenizer for batching\n",
    "            if \"batch_size\" in kwargs and self.pipeline.tokenizer.pad_token_id is None:\n",
    "                self.pipeline.tokenizer.pad_token_id = tokenid\n",
    "                self.pipeline.tokenizer.padding_side = \"left\"\n",
    "\n",
    "        # Set tokenizer when stop strings is set\n",
    "        if stop:\n",
    "            defaults[\"tokenizer\"] = self.pipeline.tokenizer\n",
    "\n",
    "        return {**defaults, **kwargs}\n",
    "\n",
    "    def extract(self, result):\n",
    "        \"\"\"\n",
    "        Extracts generated text from a pipeline result.\n",
    "\n",
    "        Args:\n",
    "            result: pipeline result\n",
    "\n",
    "        Returns:\n",
    "            generated text\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract output from list, if necessary\n",
    "        result = result[0] if isinstance(result, list) else result\n",
    "        text = result[\"generated_text\"]\n",
    "        return text[-1][\"content\"] if isinstance(text, list) else text\n",
    "\n",
    "    def task(self, path, task, **kwargs):\n",
    "        \"\"\"\n",
    "        Get the pipeline task name.\n",
    "\n",
    "        Args:\n",
    "            path: model path input\n",
    "            task: task name\n",
    "            kwargs: optional additional keyword arguments\n",
    "\n",
    "        Returns:\n",
    "            pipeline task name\n",
    "        \"\"\"\n",
    "\n",
    "        mapping = {\"language-generation\": \"text-generation\", \"sequence-sequence\": \"text2text-generation\"}\n",
    "\n",
    "        # Attempt to resolve task\n",
    "        if path and not task:\n",
    "            task = Models.task(path, **kwargs)\n",
    "\n",
    "        # Map to Hugging Face task. Default to text2text-generation pipeline when task not resolved.\n",
    "        return mapping.get(task, \"text2text-generation\")\n",
    "\n",
    "\n",
    "class Generator(HFLLM):\n",
    "    \"\"\"\n",
    "    Generate text with a causal language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):\n",
    "        super().__init__(path, quantize, gpu, model, \"language-generation\", **kwargs)\n",
    "\n",
    "\n",
    "class Sequences(HFLLM):\n",
    "    \"\"\"\n",
    "    Generate text with a sequence-sequence model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):\n",
    "        super().__init__(path, quantize, gpu, model, \"sequence-sequence\", **kwargs)\n",
    "\n",
    "\n",
    "class StreamingResponse:\n",
    "    \"\"\"\n",
    "    Generate text as a streaming response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pipeline, texts, stop, **kwargs):\n",
    "        # Create streamer\n",
    "        self.stream = TextIteratorStreamer(pipeline.tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=5)\n",
    "        kwargs[\"streamer\"] = self.stream\n",
    "        kwargs[\"stop_strings\"] = stop\n",
    "\n",
    "        # Create thread\n",
    "        self.thread = Thread(target=pipeline, args=[texts], kwargs=kwargs)\n",
    "\n",
    "        # Store number of inputs\n",
    "        self.length = len(texts)\n",
    "\n",
    "    def __call__(self):\n",
    "        # Start the process\n",
    "        self.thread.start()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.length):\n",
    "            yield from self.stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
